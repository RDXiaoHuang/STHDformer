PEMS04
Trainset:	x-(10181, 12, 307, 3)	y-(10181, 12, 307, 1)
Valset:  	x-(3394, 12, 307, 3)  	y-(3394, 12, 307, 1)
Testset:	x-(3394, 12, 307, 3)	y-(3394, 12, 307, 1)

Random seed = 1
--------- STHDformer ---------
{
    "num_nodes": 307,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0005,
    "milestones": [
        15,
        30,
        50
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 307,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "steps_per_week": 7,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers_t": 1,
        "num_layers_c": 1,
        "num_layers_s": 1,
        "num_layers_mlp": 2,
        "dropout": 0.1,
        "adaptive_embedding_dim": 80,
        "node_dim": 64,
        "use_temporal_heterogeneity": true,
        "use_spatial_heterogeneity": true,
        "use_temporal_cross": false,
        "use_spatial_cross": true,
        "use_mixed_proj": true
    }
}
==============================================================================================================
Layer (type:depth-idx)                                       Output Shape              Param #
==============================================================================================================
STHDformer                                                   [16, 12, 307, 1]          271,464
├─Linear: 1-1                                                [16, 12, 307, 24]         96
├─Embedding: 1-2                                             [16, 12, 307, 24]         6,912
├─Embedding: 1-3                                             [16, 12, 307, 24]         168
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-1                               [16, 12, 307, 152]        23,256
│    │    └─AttentionLayer: 3-1                              [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-2                                     [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-3                                   [16, 307, 12, 152]        304
│    │    └─Sequential: 3-4                                  [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-5                                     [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-6                                   [16, 307, 12, 152]        304
├─ModuleList: 1-5                                            --                        --
│    └─Temporal_Heterogeneity_SelfAttentionLayer: 2-2        [16, 12, 307, 152]        --
│    │    └─Temporal_Heterogeneity_AttentionLayer: 3-7       [16, 307, 12, 152]        93,072
│    │    └─Dropout: 3-8                                     [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-9                                   [16, 307, 12, 152]        304
│    │    └─Sequential: 3-10                                 [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-11                                    [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-12                                  [16, 307, 12, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-3                         [16, 12, 307, 152]        --
│    │    └─Cross_AttentionLayer: 3-13                       [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-14                                    [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-15                                  [16, 307, 12, 152]        304
│    │    └─Sequential: 3-16                                 [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-17                                    [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-18                                  [16, 307, 12, 152]        304
├─Dual_graph: 1-7                                            [16, 12, 307, 128]        --
│    └─Graph_projection: 2-4                                 [1, 307, 64]              --
│    │    └─Linear: 3-19                                     [1, 307, 64]              19,712
│    │    └─ReLU: 3-20                                       [1, 307, 64]              --
│    │    └─Dropout: 3-21                                    [1, 307, 64]              --
│    │    └─Linear: 3-22                                     [1, 307, 64]              4,160
│    └─Graph_projection: 2-5                                 [1, 307, 64]              --
│    │    └─Linear: 3-23                                     [1, 307, 64]              19,712
│    │    └─ReLU: 3-24                                       [1, 307, 64]              --
│    │    └─Dropout: 3-25                                    [1, 307, 64]              --
│    │    └─Linear: 3-26                                     [1, 307, 64]              4,160
├─Fusion_Model: 1-8                                          [16, 12, 307, 152]        --
│    └─Sequential: 2-6                                       [16, 12, 307, 80]         --
│    │    └─MLP: 3-27                                        [16, 12, 307, 208]        86,944
│    │    └─MLP: 3-28                                        [16, 12, 307, 208]        86,944
│    │    └─Linear: 3-29                                     [16, 12, 307, 80]         16,720
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-7                               [16, 12, 307, 152]        (recursive)
│    │    └─Sequential: 3-30                                 [16, 12, 307, 152]        23,256
│    │    └─AttentionLayer: 3-31                             [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-32                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-33                                  [16, 12, 307, 152]        (recursive)
│    │    └─Sequential: 3-34                                 [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-35                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-36                                  [16, 12, 307, 152]        (recursive)
├─ModuleList: 1-10                                           --                        --
│    └─Spatial_Heterogeneity_SelfAttentionLayer: 2-8         [16, 12, 307, 152]        --
│    │    └─Spatial_Heterogeneity_AttentionLayer: 3-37       [16, 12, 307, 152]        93,024
│    │    └─Dropout: 3-38                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-39                                  [16, 12, 307, 152]        304
│    │    └─Sequential: 3-40                                 [16, 12, 307, 152]        78,232
│    │    └─Dropout: 3-41                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-42                                  [16, 12, 307, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-9                         [16, 12, 307, 152]        (recursive)
│    │    └─Cross_AttentionLayer: 3-43                       [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-44                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-45                                  [16, 12, 307, 152]        (recursive)
│    │    └─Sequential: 3-46                                 [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-47                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-48                                  [16, 12, 307, 152]        (recursive)
├─Linear: 1-12                                               [16, 307, 12]             21,900
==============================================================================================================
Total params: 1,272,908
Trainable params: 1,272,908
Non-trainable params: 0
Total mult-adds (M): 20.43
==============================================================================================================
Input size (MB): 0.71
Forward/backward pass size (MB): 4271.48
Params size (MB): 3.91
Estimated Total Size (MB): 4276.10
==============================================================================================================

Loss: HuberLoss

2025-06-04 18:09:26.184360 Epoch 1  	Train Loss = 30.74742 Val Loss = 28.21900
2025-06-04 18:14:47.130667 Epoch 2  	Train Loss = 22.78409 Val Loss = 21.47303
2025-06-04 18:20:06.936893 Epoch 3  	Train Loss = 20.94535 Val Loss = 20.69364
2025-06-04 18:25:26.767871 Epoch 4  	Train Loss = 20.56829 Val Loss = 19.96936
2025-06-04 18:30:46.621164 Epoch 5  	Train Loss = 19.63565 Val Loss = 24.25359
2025-06-04 18:36:06.620105 Epoch 6  	Train Loss = 19.43943 Val Loss = 19.37351
2025-06-04 18:41:26.560478 Epoch 7  	Train Loss = 19.44350 Val Loss = 18.92406
2025-06-04 18:46:45.981483 Epoch 8  	Train Loss = 18.80335 Val Loss = 19.60429
2025-06-04 18:52:05.317171 Epoch 9  	Train Loss = 18.74973 Val Loss = 20.02336
2025-06-04 18:57:24.814882 Epoch 10  	Train Loss = 18.83903 Val Loss = 18.98283
2025-06-04 19:02:44.615945 Epoch 11  	Train Loss = 18.47380 Val Loss = 19.41372
2025-06-04 19:08:03.933449 Epoch 12  	Train Loss = 18.17423 Val Loss = 19.02456
2025-06-04 19:13:23.509561 Epoch 13  	Train Loss = 18.26382 Val Loss = 18.57566
2025-06-04 19:18:43.188376 Epoch 14  	Train Loss = 18.09521 Val Loss = 19.16943
2025-06-04 19:24:03.030190 Epoch 15  	Train Loss = 18.10206 Val Loss = 19.90353
2025-06-04 19:29:22.930064 Epoch 16  	Train Loss = 17.21642 Val Loss = 17.86014
2025-06-04 19:34:42.613429 Epoch 17  	Train Loss = 17.06073 Val Loss = 17.85367
2025-06-04 19:40:01.816592 Epoch 18  	Train Loss = 17.00923 Val Loss = 17.78372
2025-06-04 19:45:21.505156 Epoch 19  	Train Loss = 16.98673 Val Loss = 17.84565
2025-06-04 19:50:40.565391 Epoch 20  	Train Loss = 16.96130 Val Loss = 17.87438
2025-06-04 19:56:00.025982 Epoch 21  	Train Loss = 16.94114 Val Loss = 17.77985
2025-06-04 20:01:19.000605 Epoch 22  	Train Loss = 16.91632 Val Loss = 17.92380
2025-06-04 20:06:37.996112 Epoch 23  	Train Loss = 16.89098 Val Loss = 17.77755
2025-06-04 20:11:57.045546 Epoch 24  	Train Loss = 16.86187 Val Loss = 17.87852
2025-06-04 20:17:15.781158 Epoch 25  	Train Loss = 16.84353 Val Loss = 17.76556
2025-06-04 20:22:34.657511 Epoch 26  	Train Loss = 16.82505 Val Loss = 17.72667
2025-06-04 20:27:53.411739 Epoch 27  	Train Loss = 16.79826 Val Loss = 17.78080
2025-06-04 20:33:12.239511 Epoch 28  	Train Loss = 16.78468 Val Loss = 17.72087
2025-06-04 20:38:31.381534 Epoch 29  	Train Loss = 16.76298 Val Loss = 17.76602
2025-06-04 20:43:50.297557 Epoch 30  	Train Loss = 16.74814 Val Loss = 17.73591
2025-06-04 20:49:09.164354 Epoch 31  	Train Loss = 16.65014 Val Loss = 17.63518
2025-06-04 20:54:28.185135 Epoch 32  	Train Loss = 16.63820 Val Loss = 17.61980
2025-06-04 20:59:47.085526 Epoch 33  	Train Loss = 16.64221 Val Loss = 17.63828
2025-06-04 21:05:05.958999 Epoch 34  	Train Loss = 16.63900 Val Loss = 17.63680
2025-06-04 21:10:25.153644 Epoch 35  	Train Loss = 16.63514 Val Loss = 17.62048
2025-06-04 21:15:43.973171 Epoch 36  	Train Loss = 16.63349 Val Loss = 17.62688
2025-06-04 21:21:02.593146 Epoch 37  	Train Loss = 16.62583 Val Loss = 17.62823
2025-06-04 21:26:21.285507 Epoch 38  	Train Loss = 16.62748 Val Loss = 17.64689
2025-06-04 21:31:40.081364 Epoch 39  	Train Loss = 16.61864 Val Loss = 17.62678
2025-06-04 21:36:59.229187 Epoch 40  	Train Loss = 16.62092 Val Loss = 17.64025
2025-06-04 21:42:18.227801 Epoch 41  	Train Loss = 16.61990 Val Loss = 17.62876
2025-06-04 21:47:36.863523 Epoch 42  	Train Loss = 16.61698 Val Loss = 17.62680
2025-06-04 21:52:55.451483 Epoch 43  	Train Loss = 16.60856 Val Loss = 17.63483
2025-06-04 21:58:14.045197 Epoch 44  	Train Loss = 16.60929 Val Loss = 17.63277
2025-06-04 22:03:32.556612 Epoch 45  	Train Loss = 16.61115 Val Loss = 17.63073
2025-06-04 22:08:51.459941 Epoch 46  	Train Loss = 16.60020 Val Loss = 17.64521
2025-06-04 22:14:10.266707 Epoch 47  	Train Loss = 16.59270 Val Loss = 17.62770
2025-06-04 22:19:29.326414 Epoch 48  	Train Loss = 16.60216 Val Loss = 17.63119
2025-06-04 22:24:48.351952 Epoch 49  	Train Loss = 16.59180 Val Loss = 17.62515
2025-06-04 22:30:07.024839 Epoch 50  	Train Loss = 16.59363 Val Loss = 17.61686
2025-06-04 22:35:26.040571 Epoch 51  	Train Loss = 16.58449 Val Loss = 17.62272
2025-06-04 22:40:45.059578 Epoch 52  	Train Loss = 16.58547 Val Loss = 17.61900
2025-06-04 22:45:35.495267 Epoch 53  	Train Loss = 16.58526 Val Loss = 17.61800
2025-06-04 22:49:08.066430 Epoch 54  	Train Loss = 16.58154 Val Loss = 17.61716
2025-06-04 22:52:40.796956 Epoch 55  	Train Loss = 16.58584 Val Loss = 17.61581
2025-06-04 22:56:13.467512 Epoch 56  	Train Loss = 16.57516 Val Loss = 17.61580
2025-06-04 22:59:45.666461 Epoch 57  	Train Loss = 16.58059 Val Loss = 17.61633
2025-06-04 23:03:18.291097 Epoch 58  	Train Loss = 16.57727 Val Loss = 17.61310
2025-06-04 23:06:50.706334 Epoch 59  	Train Loss = 16.58079 Val Loss = 17.61664
2025-06-04 23:10:23.340416 Epoch 60  	Train Loss = 16.57458 Val Loss = 17.61519
2025-06-04 23:13:55.608106 Epoch 61  	Train Loss = 16.57988 Val Loss = 17.61388
2025-06-04 23:17:28.283339 Epoch 62  	Train Loss = 16.57703 Val Loss = 17.62169
2025-06-04 23:21:00.479720 Epoch 63  	Train Loss = 16.57688 Val Loss = 17.61465
2025-06-04 23:24:33.153931 Epoch 64  	Train Loss = 16.58100 Val Loss = 17.62297
2025-06-04 23:28:05.805695 Epoch 65  	Train Loss = 16.58005 Val Loss = 17.61523
2025-06-04 23:31:38.224963 Epoch 66  	Train Loss = 16.57989 Val Loss = 17.61920
2025-06-04 23:35:10.884032 Epoch 67  	Train Loss = 16.57557 Val Loss = 17.62021
2025-06-04 23:38:43.172874 Epoch 68  	Train Loss = 16.58060 Val Loss = 17.61782
2025-06-04 23:42:15.921438 Epoch 69  	Train Loss = 16.57827 Val Loss = 17.61483
2025-06-04 23:45:48.598096 Epoch 70  	Train Loss = 16.57193 Val Loss = 17.61439
2025-06-04 23:49:21.471412 Epoch 71  	Train Loss = 16.57172 Val Loss = 17.61833
2025-06-04 23:52:54.096398 Epoch 72  	Train Loss = 16.56954 Val Loss = 17.61399
2025-06-04 23:56:26.329197 Epoch 73  	Train Loss = 16.56880 Val Loss = 17.61923
2025-06-04 23:59:58.609120 Epoch 74  	Train Loss = 16.57660 Val Loss = 17.61844
2025-06-05 00:03:31.274493 Epoch 75  	Train Loss = 16.57669 Val Loss = 17.61778
2025-06-05 00:07:03.689823 Epoch 76  	Train Loss = 16.57422 Val Loss = 17.61752
2025-06-05 00:10:36.300726 Epoch 77  	Train Loss = 16.57522 Val Loss = 17.61464
2025-06-05 00:14:09.040063 Epoch 78  	Train Loss = 16.57118 Val Loss = 17.61490
Early stopping at epoch: 78
Best at epoch 58:
Train Loss = 16.57727
Train RMSE = 28.24232, MAE = 16.93164, MAPE = 12.08515
Val Loss = 17.61310
Val RMSE = 30.58414, MAE = 18.31352, MAPE = 11.77802
Saved Model: ../saved_models/STHDformer-PEMS04-2025-06-04-18-04-38.pt
--------- Test ---------
All Steps RMSE = 29.88318, MAE = 18.17484, MAPE = 11.90730
Step 1 RMSE = 26.97027, MAE = 16.64431, MAPE = 11.01642
Step 2 RMSE = 27.87943, MAE = 17.08287, MAPE = 11.32028
Step 3 RMSE = 28.56633, MAE = 17.44573, MAPE = 11.52486
Step 4 RMSE = 29.09247, MAE = 17.72601, MAPE = 11.66942
Step 5 RMSE = 29.53786, MAE = 17.96482, MAPE = 11.78007
Step 6 RMSE = 29.92098, MAE = 18.17174, MAPE = 11.88499
Step 7 RMSE = 30.27927, MAE = 18.37609, MAPE = 12.00816
Step 8 RMSE = 30.59375, MAE = 18.55645, MAPE = 12.10634
Step 9 RMSE = 30.89390, MAE = 18.73764, MAPE = 12.20423
Step 10 RMSE = 31.17411, MAE = 18.92219, MAPE = 12.32029
Step 11 RMSE = 31.47191, MAE = 19.12008, MAPE = 12.45302
Step 12 RMSE = 31.80674, MAE = 19.34989, MAPE = 12.59927
Inference time: 25.47 s
