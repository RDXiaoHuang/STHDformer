PEMS03
Trainset:	x-(15711, 12, 358, 3)	y-(15711, 12, 358, 1)
Valset:  	x-(5237, 12, 358, 3)  	y-(5237, 12, 358, 1)
Testset:	x-(5237, 12, 358, 3)	y-(5237, 12, 358, 1)

Random seed = 1
--------- STHDformer ---------
{
    "num_nodes": 358,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0005,
    "milestones": [
        15,
        30,
        40
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 358,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "steps_per_week": 7,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers_t": 1,
        "num_layers_c": 1,
        "num_layers_s": 1,
        "num_layers_mlp": 2,
        "dropout": 0.1,
        "adaptive_embedding_dim": 80,
        "node_dim": 64,
        "use_temporal_heterogeneity": true,
        "use_spatial_heterogeneity": false,
        "use_temporal_cross": true,
        "use_spatial_cross": true,
        "use_mixed_proj": true
    }
}
==============================================================================================================
Layer (type:depth-idx)                                       Output Shape              Param #
==============================================================================================================
STHDformer                                                   [16, 12, 358, 1]          320,424
├─Linear: 1-1                                                [16, 12, 358, 24]         96
├─Embedding: 1-2                                             [16, 12, 358, 24]         6,912
├─Embedding: 1-3                                             [16, 12, 358, 24]         168
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-1                               [16, 12, 358, 152]        23,256
│    │    └─AttentionLayer: 3-1                              [16, 358, 12, 152]        93,024
│    │    └─Dropout: 3-2                                     [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-3                                   [16, 358, 12, 152]        304
│    │    └─Sequential: 3-4                                  [16, 358, 12, 152]        78,232
│    │    └─Dropout: 3-5                                     [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-6                                   [16, 358, 12, 152]        304
├─ModuleList: 1-5                                            --                        --
│    └─Temporal_Heterogeneity_SelfAttentionLayer: 2-2        [16, 12, 358, 152]        --
│    │    └─Temporal_Heterogeneity_AttentionLayer: 3-7       [16, 358, 12, 152]        93,072
│    │    └─Dropout: 3-8                                     [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-9                                   [16, 358, 12, 152]        304
│    │    └─Sequential: 3-10                                 [16, 358, 12, 152]        78,232
│    │    └─Dropout: 3-11                                    [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-12                                  [16, 358, 12, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-3                         [16, 12, 358, 152]        --
│    │    └─Cross_AttentionLayer: 3-13                       [16, 358, 12, 152]        93,024
│    │    └─Dropout: 3-14                                    [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-15                                  [16, 358, 12, 152]        304
│    │    └─Sequential: 3-16                                 [16, 358, 12, 152]        78,232
│    │    └─Dropout: 3-17                                    [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-18                                  [16, 358, 12, 152]        304
├─Dual_graph: 1-7                                            [16, 12, 358, 128]        --
│    └─Graph_projection: 2-4                                 [1, 358, 64]              --
│    │    └─Linear: 3-19                                     [1, 358, 64]              22,976
│    │    └─ReLU: 3-20                                       [1, 358, 64]              --
│    │    └─Dropout: 3-21                                    [1, 358, 64]              --
│    │    └─Linear: 3-22                                     [1, 358, 64]              4,160
│    └─Graph_projection: 2-5                                 [1, 358, 64]              --
│    │    └─Linear: 3-23                                     [1, 358, 64]              22,976
│    │    └─ReLU: 3-24                                       [1, 358, 64]              --
│    │    └─Dropout: 3-25                                    [1, 358, 64]              --
│    │    └─Linear: 3-26                                     [1, 358, 64]              4,160
├─Fusion_Model: 1-8                                          [16, 12, 358, 152]        --
│    └─Sequential: 2-6                                       [16, 12, 358, 80]         --
│    │    └─MLP: 3-27                                        [16, 12, 358, 208]        86,944
│    │    └─MLP: 3-28                                        [16, 12, 358, 208]        86,944
│    │    └─Linear: 3-29                                     [16, 12, 358, 80]         16,720
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-7                               [16, 12, 358, 152]        (recursive)
│    │    └─Sequential: 3-30                                 [16, 12, 358, 152]        23,256
│    │    └─AttentionLayer: 3-31                             [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-32                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-33                                  [16, 12, 358, 152]        (recursive)
│    │    └─Sequential: 3-34                                 [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-35                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-36                                  [16, 12, 358, 152]        (recursive)
├─ModuleList: 1-10                                           --                        --
│    └─SpatialGATLayer: 2-8                                  [16, 12, 358, 152]        76
│    │    └─Linear: 3-37                                     [192, 358, 152]           23,256
│    │    └─LeakyReLU: 3-38                                  [192, 4, 358, 358]        --
│    │    └─Dropout: 3-39                                    [192, 4, 358, 358]        --
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-9                         [16, 12, 358, 152]        (recursive)
│    │    └─Cross_AttentionLayer: 3-40                       [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-41                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-42                                  [16, 12, 358, 152]        (recursive)
│    │    └─Sequential: 3-43                                 [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-44                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-45                                  [16, 12, 358, 152]        (recursive)
├─Linear: 1-12                                               [16, 358, 12]             21,900
==============================================================================================================
Total params: 1,179,864
Trainable params: 1,179,864
Non-trainable params: 0
Total mult-adds (M): 22.16
==============================================================================================================
Input size (MB): 0.82
Forward/backward pass size (MB): 4338.81
Params size (MB): 3.34
Estimated Total Size (MB): 4342.98
==============================================================================================================

Loss: HuberLoss

2025-07-01 23:33:32.196656 Epoch 1  	Train Loss = 22.63242 Val Loss = 16.00737
2025-07-01 23:39:53.010375 Epoch 2  	Train Loss = 16.35187 Val Loss = 20.86109
2025-07-01 23:46:13.172851 Epoch 3  	Train Loss = 15.16003 Val Loss = 14.68918
2025-07-01 23:52:33.964750 Epoch 4  	Train Loss = 14.64810 Val Loss = 15.22772
2025-07-01 23:58:54.726083 Epoch 5  	Train Loss = 14.28809 Val Loss = 14.07849
2025-07-02 00:05:14.897216 Epoch 6  	Train Loss = 14.04382 Val Loss = 14.38127
2025-07-02 00:11:35.496757 Epoch 7  	Train Loss = 13.93951 Val Loss = 14.53089
2025-07-02 00:17:56.199497 Epoch 8  	Train Loss = 13.72339 Val Loss = 13.74928
2025-07-02 00:24:16.928626 Epoch 9  	Train Loss = 13.59422 Val Loss = 14.10285
2025-07-02 00:30:37.124947 Epoch 10  	Train Loss = 13.42985 Val Loss = 13.55403
2025-07-02 00:36:57.741646 Epoch 11  	Train Loss = 13.30306 Val Loss = 13.63441
2025-07-02 00:43:18.366230 Epoch 12  	Train Loss = 13.26053 Val Loss = 13.61446
2025-07-02 00:49:38.567627 Epoch 13  	Train Loss = 13.20145 Val Loss = 13.72665
2025-07-02 00:55:59.261107 Epoch 14  	Train Loss = 13.08752 Val Loss = 13.82006
2025-07-02 01:02:19.897097 Epoch 15  	Train Loss = 13.07683 Val Loss = 13.38302
2025-07-02 01:08:40.002279 Epoch 16  	Train Loss = 12.38970 Val Loss = 13.04302
2025-07-02 01:15:00.659354 Epoch 17  	Train Loss = 12.31934 Val Loss = 13.00466
2025-07-02 01:21:21.317440 Epoch 18  	Train Loss = 12.28996 Val Loss = 13.00770
2025-07-02 01:27:41.828910 Epoch 19  	Train Loss = 12.27442 Val Loss = 12.97034
2025-07-02 01:34:02.244211 Epoch 20  	Train Loss = 12.24822 Val Loss = 13.01752
2025-07-02 01:40:22.965288 Epoch 21  	Train Loss = 12.23459 Val Loss = 13.00268
2025-07-02 01:46:43.638650 Epoch 22  	Train Loss = 12.21935 Val Loss = 12.97500
2025-07-02 01:53:03.771743 Epoch 23  	Train Loss = 12.20361 Val Loss = 12.96965
2025-07-02 01:59:24.344984 Epoch 24  	Train Loss = 12.18387 Val Loss = 12.95996
2025-07-02 02:05:44.948537 Epoch 25  	Train Loss = 12.17182 Val Loss = 13.00953
2025-07-02 02:12:05.008997 Epoch 26  	Train Loss = 12.16094 Val Loss = 13.07182
2025-07-02 02:18:25.741309 Epoch 27  	Train Loss = 12.14285 Val Loss = 12.98335
2025-07-02 02:24:46.355107 Epoch 28  	Train Loss = 12.13397 Val Loss = 13.00281
2025-07-02 02:31:06.465839 Epoch 29  	Train Loss = 12.12048 Val Loss = 12.97305
2025-07-02 02:37:27.072178 Epoch 30  	Train Loss = 12.10798 Val Loss = 13.00149
2025-07-02 02:43:47.708421 Epoch 31  	Train Loss = 12.02764 Val Loss = 12.94838
2025-07-02 02:50:08.335217 Epoch 32  	Train Loss = 12.02002 Val Loss = 12.94279
2025-07-02 02:56:28.501639 Epoch 33  	Train Loss = 12.01678 Val Loss = 12.97980
2025-07-02 03:02:49.201637 Epoch 34  	Train Loss = 12.01529 Val Loss = 12.94310
2025-07-02 03:09:09.890949 Epoch 35  	Train Loss = 12.01157 Val Loss = 12.94626
2025-07-02 03:15:30.021797 Epoch 36  	Train Loss = 12.00924 Val Loss = 12.96159
2025-07-02 03:21:50.724969 Epoch 37  	Train Loss = 12.00795 Val Loss = 12.95805
2025-07-02 03:28:11.429115 Epoch 38  	Train Loss = 12.00477 Val Loss = 12.97947
2025-07-02 03:34:31.430488 Epoch 39  	Train Loss = 12.00391 Val Loss = 12.96837
2025-07-02 03:40:52.064916 Epoch 40  	Train Loss = 12.00072 Val Loss = 12.95839
2025-07-02 03:46:56.096717 Epoch 41  	Train Loss = 11.99259 Val Loss = 12.95704
2025-07-02 03:49:59.933825 Epoch 42  	Train Loss = 11.99075 Val Loss = 12.96188
2025-07-02 03:53:03.509005 Epoch 43  	Train Loss = 11.99125 Val Loss = 12.95785
2025-07-02 03:56:07.447011 Epoch 44  	Train Loss = 11.99034 Val Loss = 12.96609
2025-07-02 03:59:11.226469 Epoch 45  	Train Loss = 11.99049 Val Loss = 12.96124
2025-07-02 04:02:15.098174 Epoch 46  	Train Loss = 11.99001 Val Loss = 12.96490
2025-07-02 04:05:18.783932 Epoch 47  	Train Loss = 11.98993 Val Loss = 12.96073
2025-07-02 04:08:22.627541 Epoch 48  	Train Loss = 11.98896 Val Loss = 12.95753
2025-07-02 04:11:26.449810 Epoch 49  	Train Loss = 11.99012 Val Loss = 12.95722
2025-07-02 04:14:30.166121 Epoch 50  	Train Loss = 11.98891 Val Loss = 12.95904
2025-07-02 04:17:33.918784 Epoch 51  	Train Loss = 11.98920 Val Loss = 12.96368
2025-07-02 04:20:37.643741 Epoch 52  	Train Loss = 11.98889 Val Loss = 12.95489
Early stopping at epoch: 52
Best at epoch 32:
Train Loss = 12.02002
Train RMSE = 20.40653, MAE = 12.31133, MAPE = 11.26137
Val Loss = 12.94279
Val RMSE = 21.90503, MAE = 13.45909, MAPE = 12.61198
Saved Model: ../saved_models/STHDformer-PEMS03-2025-07-01-23-27-08.pt
--------- Test ---------
All Steps RMSE = 26.67649, MAE = 15.14459, MAPE = 15.24290
Step 1 RMSE = 20.72654, MAE = 12.47217, MAPE = 12.92734
Step 2 RMSE = 22.85946, MAE = 13.21003, MAPE = 13.63633
Step 3 RMSE = 24.14664, MAE = 13.80820, MAPE = 14.12893
Step 4 RMSE = 25.25030, MAE = 14.34595, MAPE = 14.60105
Step 5 RMSE = 26.06552, MAE = 14.75834, MAPE = 14.90848
Step 6 RMSE = 26.78601, MAE = 15.16236, MAPE = 15.22158
Step 7 RMSE = 27.46555, MAE = 15.53465, MAPE = 15.52781
Step 8 RMSE = 28.04364, MAE = 15.87293, MAPE = 15.82548
Step 9 RMSE = 28.58911, MAE = 16.20624, MAPE = 16.13590
Step 10 RMSE = 29.04861, MAE = 16.49113, MAPE = 16.37472
Step 11 RMSE = 29.50768, MAE = 16.78106, MAPE = 16.64958
Step 12 RMSE = 29.95457, MAE = 17.09168, MAPE = 16.97757
Inference time: 22.55 s
