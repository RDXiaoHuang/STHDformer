PEMS08
Trainset:	x-(10700, 12, 170, 3)	y-(10700, 12, 170, 1)
Valset:  	x-(3567, 12, 170, 3)  	y-(3567, 12, 170, 1)
Testset:	x-(3566, 12, 170, 3)	y-(3566, 12, 170, 1)

Random seed = 1
--------- STHDformer ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0015,
    "milestones": [
        25,
        45,
        65
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 30,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 170,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "steps_per_week": 7,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers_t": 1,
        "num_layers_c": 1,
        "num_layers_s": 1,
        "num_layers_mlp": 2,
        "dropout": 0.1,
        "adaptive_embedding_dim": 80,
        "node_dim": 64,
        "use_temporal_heterogeneity": false,
        "use_spatial_heterogeneity": true,
        "use_temporal_cross": true,
        "use_spatial_cross": true,
        "use_mixed_proj": true
    }
}
==============================================================================================================
Layer (type:depth-idx)                                       Output Shape              Param #
==============================================================================================================
STHDformer                                                   [16, 12, 170, 1]          139,944
├─Linear: 1-1                                                [16, 12, 170, 24]         96
├─Embedding: 1-2                                             [16, 12, 170, 24]         6,912
├─Embedding: 1-3                                             [16, 12, 170, 24]         168
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-1                               [16, 12, 170, 152]        23,256
│    │    └─AttentionLayer: 3-1                              [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-2                                     [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-3                                   [16, 170, 12, 152]        304
│    │    └─Sequential: 3-4                                  [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-5                                     [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-6                                   [16, 170, 12, 152]        304
├─ModuleList: 1-5                                            --                        --
│    └─TemporalTCNLayer: 2-2                                 [16, 12, 170, 152]        --
│    │    └─Conv1d: 3-7                                      [2720, 152, 12]           69,464
│    │    └─ReLU: 3-8                                        [2720, 152, 12]           --
│    │    └─Dropout: 3-9                                     [2720, 152, 12]           --
│    │    └─LayerNorm: 3-10                                  [2720, 12, 152]           304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-3                         [16, 12, 170, 152]        --
│    │    └─Cross_AttentionLayer: 3-11                       [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-12                                    [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-13                                  [16, 170, 12, 152]        304
│    │    └─Sequential: 3-14                                 [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-15                                    [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-16                                  [16, 170, 12, 152]        304
├─Dual_graph: 1-7                                            [16, 12, 170, 128]        --
│    └─Graph_projection: 2-4                                 [1, 170, 64]              --
│    │    └─Linear: 3-17                                     [1, 170, 64]              10,944
│    │    └─ReLU: 3-18                                       [1, 170, 64]              --
│    │    └─Dropout: 3-19                                    [1, 170, 64]              --
│    │    └─Linear: 3-20                                     [1, 170, 64]              4,160
│    └─Graph_projection: 2-5                                 [1, 170, 64]              --
│    │    └─Linear: 3-21                                     [1, 170, 64]              10,944
│    │    └─ReLU: 3-22                                       [1, 170, 64]              --
│    │    └─Dropout: 3-23                                    [1, 170, 64]              --
│    │    └─Linear: 3-24                                     [1, 170, 64]              4,160
├─Fusion_Model: 1-8                                          [16, 12, 170, 152]        --
│    └─Sequential: 2-6                                       [16, 12, 170, 80]         --
│    │    └─MLP: 3-25                                        [16, 12, 170, 208]        86,944
│    │    └─MLP: 3-26                                        [16, 12, 170, 208]        86,944
│    │    └─Linear: 3-27                                     [16, 12, 170, 80]         16,720
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-7                               [16, 12, 170, 152]        (recursive)
│    │    └─Sequential: 3-28                                 [16, 12, 170, 152]        23,256
│    │    └─AttentionLayer: 3-29                             [16, 12, 170, 152]        (recursive)
│    │    └─Dropout: 3-30                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-31                                  [16, 12, 170, 152]        (recursive)
│    │    └─Sequential: 3-32                                 [16, 12, 170, 152]        (recursive)
│    │    └─Dropout: 3-33                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-34                                  [16, 12, 170, 152]        (recursive)
├─ModuleList: 1-10                                           --                        --
│    └─Spatial_Heterogeneity_SelfAttentionLayer: 2-8         [16, 12, 170, 152]        --
│    │    └─Spatial_Heterogeneity_AttentionLayer: 3-35       [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-36                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-37                                  [16, 12, 170, 152]        304
│    │    └─Sequential: 3-38                                 [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-39                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-40                                  [16, 12, 170, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-9                         [16, 12, 170, 152]        (recursive)
│    │    └─Cross_AttentionLayer: 3-41                       [16, 12, 170, 152]        (recursive)
│    │    └─Dropout: 3-42                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-43                                  [16, 12, 170, 152]        (recursive)
│    │    └─Sequential: 3-44                                 [16, 12, 170, 152]        (recursive)
│    │    └─Dropout: 3-45                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-46                                  [16, 12, 170, 152]        (recursive)
├─Linear: 1-12                                               [16, 170, 12]             21,900
==============================================================================================================
Total params: 1,021,708
Trainable params: 1,021,708
Non-trainable params: 0
Total mult-adds (G): 2.29
==============================================================================================================
Input size (MB): 0.39
Forward/backward pass size (MB): 2100.01
Params size (MB): 3.43
Estimated Total Size (MB): 2103.84
==============================================================================================================

Loss: HuberLoss

2025-06-26 20:27:04.722647 Epoch 1  	Train Loss = 26.65296 Val Loss = 19.45643
2025-06-26 20:27:48.072197 Epoch 2  	Train Loss = 18.70696 Val Loss = 17.08211
2025-06-26 20:28:31.654549 Epoch 3  	Train Loss = 17.27149 Val Loss = 15.96583
2025-06-26 20:29:15.376565 Epoch 4  	Train Loss = 16.46969 Val Loss = 17.13014
2025-06-26 20:29:59.197419 Epoch 5  	Train Loss = 15.97557 Val Loss = 16.21598
2025-06-26 20:30:43.046240 Epoch 6  	Train Loss = 15.67524 Val Loss = 15.12104
2025-06-26 20:31:26.944657 Epoch 7  	Train Loss = 15.44747 Val Loss = 15.85938
2025-06-26 20:32:10.834688 Epoch 8  	Train Loss = 15.30349 Val Loss = 16.31540
2025-06-26 20:32:54.762193 Epoch 9  	Train Loss = 15.12113 Val Loss = 14.86148
2025-06-26 20:33:38.667494 Epoch 10  	Train Loss = 14.86261 Val Loss = 14.82309
2025-06-26 20:34:22.568613 Epoch 11  	Train Loss = 14.89526 Val Loss = 14.64657
2025-06-26 20:35:06.438628 Epoch 12  	Train Loss = 14.73179 Val Loss = 14.74004
2025-06-26 20:35:50.293012 Epoch 13  	Train Loss = 14.62814 Val Loss = 14.31246
2025-06-26 20:36:34.158637 Epoch 14  	Train Loss = 14.42717 Val Loss = 14.35438
2025-06-26 20:37:17.999229 Epoch 15  	Train Loss = 14.24276 Val Loss = 14.63113
2025-06-26 20:38:01.838723 Epoch 16  	Train Loss = 14.16613 Val Loss = 14.21043
2025-06-26 20:38:45.697538 Epoch 17  	Train Loss = 14.21841 Val Loss = 14.18905
2025-06-26 20:39:29.551963 Epoch 18  	Train Loss = 13.95809 Val Loss = 14.34291
2025-06-26 20:40:13.458280 Epoch 19  	Train Loss = 13.95612 Val Loss = 14.16645
2025-06-26 20:40:57.311311 Epoch 20  	Train Loss = 13.84739 Val Loss = 13.73686
2025-06-26 20:41:41.165977 Epoch 21  	Train Loss = 13.70867 Val Loss = 14.07805
2025-06-26 20:42:24.990856 Epoch 22  	Train Loss = 13.74433 Val Loss = 14.08219
2025-06-26 20:43:08.878205 Epoch 23  	Train Loss = 13.71611 Val Loss = 13.99668
2025-06-26 20:43:52.763226 Epoch 24  	Train Loss = 13.54466 Val Loss = 13.80565
2025-06-26 20:44:36.595812 Epoch 25  	Train Loss = 13.69175 Val Loss = 13.90151
2025-06-26 20:45:20.463592 Epoch 26  	Train Loss = 12.88207 Val Loss = 13.22329
2025-06-26 20:46:04.203353 Epoch 27  	Train Loss = 12.76938 Val Loss = 13.18501
2025-06-26 20:46:47.872379 Epoch 28  	Train Loss = 12.73620 Val Loss = 13.17415
2025-06-26 20:47:31.615309 Epoch 29  	Train Loss = 12.71125 Val Loss = 13.18236
2025-06-26 20:48:15.342962 Epoch 30  	Train Loss = 12.68697 Val Loss = 13.19656
2025-06-26 20:48:59.026473 Epoch 31  	Train Loss = 12.67082 Val Loss = 13.15703
2025-06-26 20:49:42.750183 Epoch 32  	Train Loss = 12.65194 Val Loss = 13.16158
2025-06-26 20:50:26.457559 Epoch 33  	Train Loss = 12.63778 Val Loss = 13.18462
2025-06-26 20:51:10.218996 Epoch 34  	Train Loss = 12.61775 Val Loss = 13.14649
2025-06-26 20:51:53.916500 Epoch 35  	Train Loss = 12.60946 Val Loss = 13.18220
2025-06-26 20:52:37.656311 Epoch 36  	Train Loss = 12.58867 Val Loss = 13.18893
2025-06-26 20:53:21.341399 Epoch 37  	Train Loss = 12.58104 Val Loss = 13.21205
2025-06-26 20:54:05.053497 Epoch 38  	Train Loss = 12.57254 Val Loss = 13.15807
2025-06-26 20:54:48.768304 Epoch 39  	Train Loss = 12.55309 Val Loss = 13.18988
2025-06-26 20:55:32.478552 Epoch 40  	Train Loss = 12.54512 Val Loss = 13.15191
2025-06-26 20:56:16.165520 Epoch 41  	Train Loss = 12.53709 Val Loss = 13.18078
2025-06-26 20:56:59.860461 Epoch 42  	Train Loss = 12.52399 Val Loss = 13.21287
2025-06-26 20:57:43.613369 Epoch 43  	Train Loss = 12.52000 Val Loss = 13.24358
2025-06-26 20:58:27.379950 Epoch 44  	Train Loss = 12.51140 Val Loss = 13.16415
2025-06-26 20:59:11.064266 Epoch 45  	Train Loss = 12.49501 Val Loss = 13.10943
2025-06-26 20:59:54.759405 Epoch 46  	Train Loss = 12.41814 Val Loss = 13.09631
2025-06-26 21:00:38.513907 Epoch 47  	Train Loss = 12.41170 Val Loss = 13.12014
2025-06-26 21:01:22.211982 Epoch 48  	Train Loss = 12.41031 Val Loss = 13.10794
2025-06-26 21:02:05.891687 Epoch 49  	Train Loss = 12.40476 Val Loss = 13.10276
2025-06-26 21:02:49.552976 Epoch 50  	Train Loss = 12.40511 Val Loss = 13.12175
2025-06-26 21:03:33.252302 Epoch 51  	Train Loss = 12.40251 Val Loss = 13.10763
2025-06-26 21:04:16.923852 Epoch 52  	Train Loss = 12.40016 Val Loss = 13.10623
2025-06-26 21:05:01.053064 Epoch 53  	Train Loss = 12.39823 Val Loss = 13.12922
2025-06-26 21:05:45.382613 Epoch 54  	Train Loss = 12.39600 Val Loss = 13.12108
2025-06-26 21:06:29.287414 Epoch 55  	Train Loss = 12.39404 Val Loss = 13.11167
2025-06-26 21:07:12.989395 Epoch 56  	Train Loss = 12.39335 Val Loss = 13.12517
2025-06-26 21:07:56.686071 Epoch 57  	Train Loss = 12.39264 Val Loss = 13.11265
2025-06-26 21:08:40.398281 Epoch 58  	Train Loss = 12.38925 Val Loss = 13.11854
2025-06-26 21:09:24.226419 Epoch 59  	Train Loss = 12.38868 Val Loss = 13.11838
2025-06-26 21:10:07.973356 Epoch 60  	Train Loss = 12.38621 Val Loss = 13.10707
2025-06-26 21:10:51.683170 Epoch 61  	Train Loss = 12.38360 Val Loss = 13.11901
2025-06-26 21:11:35.386562 Epoch 62  	Train Loss = 12.38254 Val Loss = 13.12155
2025-06-26 21:12:19.064757 Epoch 63  	Train Loss = 12.38187 Val Loss = 13.10562
2025-06-26 21:13:02.757134 Epoch 64  	Train Loss = 12.38105 Val Loss = 13.11597
2025-06-26 21:13:46.421036 Epoch 65  	Train Loss = 12.37818 Val Loss = 13.11290
2025-06-26 21:14:30.121410 Epoch 66  	Train Loss = 12.36895 Val Loss = 13.10950
2025-06-26 21:15:13.783571 Epoch 67  	Train Loss = 12.36663 Val Loss = 13.10755
2025-06-26 21:15:57.464921 Epoch 68  	Train Loss = 12.36842 Val Loss = 13.11110
2025-06-26 21:16:41.140542 Epoch 69  	Train Loss = 12.36826 Val Loss = 13.10519
2025-06-26 21:17:24.836342 Epoch 70  	Train Loss = 12.36822 Val Loss = 13.10861
2025-06-26 21:18:08.536152 Epoch 71  	Train Loss = 12.36624 Val Loss = 13.11105
2025-06-26 21:18:52.247391 Epoch 72  	Train Loss = 12.36796 Val Loss = 13.11053
2025-06-26 21:19:35.966537 Epoch 73  	Train Loss = 12.36589 Val Loss = 13.10478
2025-06-26 21:20:19.703160 Epoch 74  	Train Loss = 12.36774 Val Loss = 13.10701
2025-06-26 21:21:03.478487 Epoch 75  	Train Loss = 12.36611 Val Loss = 13.10773
2025-06-26 21:21:47.205774 Epoch 76  	Train Loss = 12.36548 Val Loss = 13.10621
Early stopping at epoch: 76
Best at epoch 46:
Train Loss = 12.41814
Train RMSE = 22.24417, MAE = 12.64020, MAPE = 8.24693
Val Loss = 13.09631
Val RMSE = 24.07900, MAE = 13.53437, MAPE = 9.80999
Saved Model: ../saved_models/STHDformer-PEMS08-2025-06-26-20-26-16.pt
--------- Test ---------
All Steps RMSE = 23.22010, MAE = 13.44687, MAPE = 8.77015
Step 1 RMSE = 19.43919, MAE = 11.68460, MAPE = 7.67576
Step 2 RMSE = 20.52206, MAE = 12.11565, MAPE = 7.93296
Step 3 RMSE = 21.37812, MAE = 12.49860, MAPE = 8.15743
Step 4 RMSE = 22.12063, MAE = 12.81279, MAPE = 8.35233
Step 5 RMSE = 22.70448, MAE = 13.09288, MAPE = 8.52563
Step 6 RMSE = 23.24501, MAE = 13.35410, MAPE = 8.70621
Step 7 RMSE = 23.72700, MAE = 13.62151, MAPE = 8.87367
Step 8 RMSE = 24.15931, MAE = 13.87048, MAPE = 9.03281
Step 9 RMSE = 24.54433, MAE = 14.15999, MAPE = 9.21697
Step 10 RMSE = 24.89536, MAE = 14.39583, MAPE = 9.37816
Step 11 RMSE = 25.30183, MAE = 14.75589, MAPE = 9.61591
Step 12 RMSE = 25.68742, MAE = 15.00007, MAPE = 9.77404
Inference time: 5.46 s
