PEMS04
Trainset:	x-(10181, 12, 307, 3)	y-(10181, 12, 307, 1)
Valset:  	x-(3394, 12, 307, 3)  	y-(3394, 12, 307, 1)
Testset:	x-(3394, 12, 307, 3)	y-(3394, 12, 307, 1)

Random seed = 1
--------- STHDformer ---------
{
    "num_nodes": 307,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0005,
    "milestones": [
        15,
        30,
        50
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 307,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "steps_per_week": 7,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers_t": 1,
        "num_layers_c": 1,
        "num_layers_s": 1,
        "num_layers_mlp": 2,
        "dropout": 0.1,
        "adaptive_embedding_dim": 80,
        "node_dim": 64,
        "use_temporal_heterogeneity": true,
        "use_spatial_heterogeneity": false,
        "use_temporal_cross": true,
        "use_spatial_cross": true,
        "use_mixed_proj": true
    }
}
==============================================================================================================
Layer (type:depth-idx)                                       Output Shape              Param #
==============================================================================================================
STHDformer                                                   [16, 12, 307, 1]          271,464
├─Linear: 1-1                                                [16, 12, 307, 24]         96
├─Embedding: 1-2                                             [16, 12, 307, 24]         6,912
├─Embedding: 1-3                                             [16, 12, 307, 24]         168
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-1                               [16, 12, 307, 152]        23,256
│    │    └─AttentionLayer: 3-1                              [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-2                                     [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-3                                   [16, 307, 12, 152]        304
│    │    └─Sequential: 3-4                                  [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-5                                     [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-6                                   [16, 307, 12, 152]        304
├─ModuleList: 1-5                                            --                        --
│    └─Temporal_Heterogeneity_SelfAttentionLayer: 2-2        [16, 12, 307, 152]        --
│    │    └─Temporal_Heterogeneity_AttentionLayer: 3-7       [16, 307, 12, 152]        93,072
│    │    └─Dropout: 3-8                                     [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-9                                   [16, 307, 12, 152]        304
│    │    └─Sequential: 3-10                                 [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-11                                    [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-12                                  [16, 307, 12, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-3                         [16, 12, 307, 152]        --
│    │    └─Cross_AttentionLayer: 3-13                       [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-14                                    [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-15                                  [16, 307, 12, 152]        304
│    │    └─Sequential: 3-16                                 [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-17                                    [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-18                                  [16, 307, 12, 152]        304
├─Dual_graph: 1-7                                            [16, 12, 307, 128]        --
│    └─Graph_projection: 2-4                                 [1, 307, 64]              --
│    │    └─Linear: 3-19                                     [1, 307, 64]              19,712
│    │    └─ReLU: 3-20                                       [1, 307, 64]              --
│    │    └─Dropout: 3-21                                    [1, 307, 64]              --
│    │    └─Linear: 3-22                                     [1, 307, 64]              4,160
│    └─Graph_projection: 2-5                                 [1, 307, 64]              --
│    │    └─Linear: 3-23                                     [1, 307, 64]              19,712
│    │    └─ReLU: 3-24                                       [1, 307, 64]              --
│    │    └─Dropout: 3-25                                    [1, 307, 64]              --
│    │    └─Linear: 3-26                                     [1, 307, 64]              4,160
├─Fusion_Model: 1-8                                          [16, 12, 307, 152]        --
│    └─Sequential: 2-6                                       [16, 12, 307, 80]         --
│    │    └─MLP: 3-27                                        [16, 12, 307, 208]        86,944
│    │    └─MLP: 3-28                                        [16, 12, 307, 208]        86,944
│    │    └─Linear: 3-29                                     [16, 12, 307, 80]         16,720
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-7                               [16, 12, 307, 152]        (recursive)
│    │    └─Sequential: 3-30                                 [16, 12, 307, 152]        23,256
│    │    └─AttentionLayer: 3-31                             [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-32                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-33                                  [16, 12, 307, 152]        (recursive)
│    │    └─Sequential: 3-34                                 [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-35                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-36                                  [16, 12, 307, 152]        (recursive)
├─ModuleList: 1-10                                           --                        --
│    └─SpatialGATLayer: 2-8                                  [16, 12, 307, 152]        76
│    │    └─Linear: 3-37                                     [192, 307, 152]           23,256
│    │    └─LeakyReLU: 3-38                                  [192, 4, 307, 307]        --
│    │    └─Dropout: 3-39                                    [192, 4, 307, 307]        --
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-9                         [16, 12, 307, 152]        (recursive)
│    │    └─Cross_AttentionLayer: 3-40                       [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-41                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-42                                  [16, 12, 307, 152]        (recursive)
│    │    └─Sequential: 3-43                                 [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-44                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-45                                  [16, 12, 307, 152]        (recursive)
├─Linear: 1-12                                               [16, 307, 12]             21,900
==============================================================================================================
Total params: 1,124,376
Trainable params: 1,124,376
Non-trainable params: 0
Total mult-adds (M): 22.15
==============================================================================================================
Input size (MB): 0.71
Forward/backward pass size (MB): 3720.71
Params size (MB): 3.32
Estimated Total Size (MB): 3724.74
==============================================================================================================

Loss: HuberLoss

2025-07-01 18:08:26.401197 Epoch 1  	Train Loss = 30.59938 Val Loss = 23.83373
2025-07-01 18:15:10.064797 Epoch 2  	Train Loss = 22.58333 Val Loss = 21.64071
2025-07-01 18:21:53.521160 Epoch 3  	Train Loss = 21.27634 Val Loss = 20.34286
2025-07-01 18:28:36.957417 Epoch 4  	Train Loss = 20.11725 Val Loss = 20.33808
2025-07-01 18:35:21.119260 Epoch 5  	Train Loss = 19.61857 Val Loss = 20.20214
2025-07-01 18:42:04.574034 Epoch 6  	Train Loss = 19.25630 Val Loss = 21.92427
2025-07-01 18:48:48.051337 Epoch 7  	Train Loss = 19.07254 Val Loss = 20.27939
2025-07-01 18:55:31.621507 Epoch 8  	Train Loss = 18.85671 Val Loss = 22.60359
2025-07-01 19:02:15.178038 Epoch 9  	Train Loss = 18.64847 Val Loss = 23.11539
2025-07-01 19:08:59.227729 Epoch 10  	Train Loss = 18.47927 Val Loss = 21.13720
2025-07-01 19:15:42.540845 Epoch 11  	Train Loss = 18.54457 Val Loss = 20.36002
2025-07-01 19:22:26.147363 Epoch 12  	Train Loss = 18.27498 Val Loss = 22.21263
2025-07-01 19:29:09.737886 Epoch 13  	Train Loss = 18.00964 Val Loss = 24.79702
2025-07-01 19:35:53.372550 Epoch 14  	Train Loss = 18.02487 Val Loss = 19.25352
2025-07-01 19:42:37.097534 Epoch 15  	Train Loss = 18.01768 Val Loss = 22.40894
2025-07-01 19:49:20.751347 Epoch 16  	Train Loss = 17.10191 Val Loss = 20.38922
2025-07-01 19:56:04.166996 Epoch 17  	Train Loss = 16.97380 Val Loss = 20.23054
2025-07-01 20:02:47.179944 Epoch 18  	Train Loss = 16.93137 Val Loss = 19.54939
2025-07-01 20:09:31.320649 Epoch 19  	Train Loss = 16.90347 Val Loss = 19.85243
2025-07-01 20:16:14.714841 Epoch 20  	Train Loss = 16.87868 Val Loss = 20.17478
2025-07-01 20:22:58.003091 Epoch 21  	Train Loss = 16.84659 Val Loss = 19.51005
2025-07-01 20:29:41.606714 Epoch 22  	Train Loss = 16.81899 Val Loss = 19.47298
2025-07-01 20:36:25.144694 Epoch 23  	Train Loss = 16.79670 Val Loss = 19.39011
2025-07-01 20:43:08.750110 Epoch 24  	Train Loss = 16.76851 Val Loss = 19.11667
2025-07-01 20:49:52.214888 Epoch 25  	Train Loss = 16.73922 Val Loss = 18.93220
2025-07-01 20:56:35.761959 Epoch 26  	Train Loss = 16.72245 Val Loss = 19.32339
2025-07-01 21:03:19.352202 Epoch 27  	Train Loss = 16.69946 Val Loss = 19.00368
2025-07-01 21:10:02.925312 Epoch 28  	Train Loss = 16.68250 Val Loss = 19.40476
2025-07-01 21:16:46.435223 Epoch 29  	Train Loss = 16.66820 Val Loss = 18.85707
2025-07-01 21:23:27.486069 Epoch 30  	Train Loss = 16.65229 Val Loss = 18.82569
2025-07-01 21:28:43.121158 Epoch 31  	Train Loss = 16.54606 Val Loss = 18.78475
2025-07-01 21:33:47.688288 Epoch 32  	Train Loss = 16.54465 Val Loss = 18.89412
2025-07-01 21:38:40.608980 Epoch 33  	Train Loss = 16.53649 Val Loss = 18.83178
2025-07-01 21:42:06.049362 Epoch 34  	Train Loss = 16.52745 Val Loss = 18.91038
2025-07-01 21:45:31.454067 Epoch 35  	Train Loss = 16.53109 Val Loss = 18.92435
2025-07-01 21:48:56.722721 Epoch 36  	Train Loss = 16.52356 Val Loss = 18.78780
2025-07-01 21:52:21.975507 Epoch 37  	Train Loss = 16.52009 Val Loss = 18.86798
2025-07-01 21:55:47.239735 Epoch 38  	Train Loss = 16.52088 Val Loss = 18.78221
2025-07-01 21:59:12.670722 Epoch 39  	Train Loss = 16.52155 Val Loss = 18.76768
2025-07-01 22:02:38.498575 Epoch 40  	Train Loss = 16.51276 Val Loss = 18.73182
2025-07-01 22:06:05.037415 Epoch 41  	Train Loss = 16.51089 Val Loss = 18.79038
2025-07-01 22:09:30.275594 Epoch 42  	Train Loss = 16.51240 Val Loss = 18.75333
2025-07-01 22:12:55.910858 Epoch 43  	Train Loss = 16.50787 Val Loss = 18.82872
2025-07-01 22:16:21.035137 Epoch 44  	Train Loss = 16.50446 Val Loss = 18.73347
2025-07-01 22:19:46.489542 Epoch 45  	Train Loss = 16.50043 Val Loss = 18.67046
2025-07-01 22:23:11.803701 Epoch 46  	Train Loss = 16.50143 Val Loss = 18.72490
2025-07-01 22:26:37.244582 Epoch 47  	Train Loss = 16.49711 Val Loss = 18.69713
2025-07-01 22:30:02.724976 Epoch 48  	Train Loss = 16.49216 Val Loss = 18.74543
2025-07-01 22:33:28.395542 Epoch 49  	Train Loss = 16.49361 Val Loss = 18.81538
2025-07-01 22:36:54.041748 Epoch 50  	Train Loss = 16.48746 Val Loss = 18.61104
2025-07-01 22:40:19.397710 Epoch 51  	Train Loss = 16.47533 Val Loss = 18.66290
2025-07-01 22:43:22.402444 Epoch 52  	Train Loss = 16.46402 Val Loss = 18.64255
2025-07-01 22:45:15.329664 Epoch 53  	Train Loss = 16.47906 Val Loss = 18.64551
2025-07-01 22:46:57.840612 Epoch 54  	Train Loss = 16.47248 Val Loss = 18.64344
2025-07-01 22:48:37.554389 Epoch 55  	Train Loss = 16.47587 Val Loss = 18.63069
2025-07-01 22:50:17.064698 Epoch 56  	Train Loss = 16.47584 Val Loss = 18.64380
2025-07-01 22:51:56.542493 Epoch 57  	Train Loss = 16.47657 Val Loss = 18.65479
2025-07-01 22:53:36.028714 Epoch 58  	Train Loss = 16.47488 Val Loss = 18.63469
2025-07-01 22:55:15.532491 Epoch 59  	Train Loss = 16.47507 Val Loss = 18.66158
2025-07-01 22:56:54.983442 Epoch 60  	Train Loss = 16.47252 Val Loss = 18.67408
2025-07-01 22:58:34.353024 Epoch 61  	Train Loss = 16.46952 Val Loss = 18.67817
2025-07-01 23:00:13.847081 Epoch 62  	Train Loss = 16.48093 Val Loss = 18.64078
2025-07-01 23:01:53.284764 Epoch 63  	Train Loss = 16.47560 Val Loss = 18.65156
2025-07-01 23:03:32.753186 Epoch 64  	Train Loss = 16.47610 Val Loss = 18.66009
2025-07-01 23:05:12.205605 Epoch 65  	Train Loss = 16.47014 Val Loss = 18.66164
2025-07-01 23:06:51.928476 Epoch 66  	Train Loss = 16.46979 Val Loss = 18.64456
2025-07-01 23:08:31.358577 Epoch 67  	Train Loss = 16.46488 Val Loss = 18.67414
2025-07-01 23:10:10.821345 Epoch 68  	Train Loss = 16.47580 Val Loss = 18.67416
2025-07-01 23:11:50.169139 Epoch 69  	Train Loss = 16.46858 Val Loss = 18.63612
2025-07-01 23:13:29.600909 Epoch 70  	Train Loss = 16.47043 Val Loss = 18.64391
Early stopping at epoch: 70
Best at epoch 50:
Train Loss = 16.48746
Train RMSE = 28.70631, MAE = 17.76643, MAPE = 12.19857
Val Loss = 18.61104
Val RMSE = 31.30851, MAE = 19.31606, MAPE = 11.98330
Saved Model: ../saved_models/STHDformer-PEMS04-2025-07-01-18-01-45.pt
--------- Test ---------
All Steps RMSE = 30.85100, MAE = 19.28973, MAPE = 12.18706
Step 1 RMSE = 27.17496, MAE = 16.88579, MAPE = 11.00814
Step 2 RMSE = 28.21982, MAE = 17.48229, MAPE = 11.35914
Step 3 RMSE = 29.05909, MAE = 18.01301, MAPE = 11.61488
Step 4 RMSE = 29.68940, MAE = 18.41558, MAPE = 11.81100
Step 5 RMSE = 30.22174, MAE = 18.74326, MAPE = 11.93048
Step 6 RMSE = 30.84186, MAE = 19.24380, MAPE = 12.13511
Step 7 RMSE = 31.27816, MAE = 19.55336, MAPE = 12.31472
Step 8 RMSE = 31.65173, MAE = 19.80684, MAPE = 12.41775
Step 9 RMSE = 32.08233, MAE = 20.15974, MAPE = 12.56978
Step 10 RMSE = 32.56985, MAE = 20.58805, MAPE = 12.77443
Step 11 RMSE = 33.04887, MAE = 21.02650, MAPE = 13.02525
Step 12 RMSE = 33.66924, MAE = 21.55825, MAPE = 13.28376
Inference time: 20.95 s
