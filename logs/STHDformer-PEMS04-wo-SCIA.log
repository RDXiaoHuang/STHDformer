PEMS04
Trainset:	x-(10181, 12, 307, 3)	y-(10181, 12, 307, 1)
Valset:  	x-(3394, 12, 307, 3)  	y-(3394, 12, 307, 1)
Testset:	x-(3394, 12, 307, 3)	y-(3394, 12, 307, 1)

Random seed = 1
--------- STHDformer ---------
{
    "num_nodes": 307,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0005,
    "milestones": [
        15,
        30,
        50
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 307,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "steps_per_week": 7,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers_t": 1,
        "num_layers_c": 1,
        "num_layers_s": 1,
        "num_layers_mlp": 2,
        "dropout": 0.1,
        "adaptive_embedding_dim": 80,
        "node_dim": 64,
        "use_temporal_heterogeneity": true,
        "use_spatial_heterogeneity": true,
        "use_temporal_cross": true,
        "use_spatial_cross": false,
        "use_mixed_proj": true
    }
}
==============================================================================================================
Layer (type:depth-idx)                                       Output Shape              Param #
==============================================================================================================
STHDformer                                                   [16, 12, 307, 1]          271,464
├─Linear: 1-1                                                [16, 12, 307, 24]         96
├─Embedding: 1-2                                             [16, 12, 307, 24]         6,912
├─Embedding: 1-3                                             [16, 12, 307, 24]         168
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-1                               [16, 12, 307, 152]        23,256
│    │    └─AttentionLayer: 3-1                              [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-2                                     [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-3                                   [16, 307, 12, 152]        304
│    │    └─Sequential: 3-4                                  [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-5                                     [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-6                                   [16, 307, 12, 152]        304
├─ModuleList: 1-5                                            --                        --
│    └─Temporal_Heterogeneity_SelfAttentionLayer: 2-2        [16, 12, 307, 152]        --
│    │    └─Temporal_Heterogeneity_AttentionLayer: 3-7       [16, 307, 12, 152]        93,072
│    │    └─Dropout: 3-8                                     [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-9                                   [16, 307, 12, 152]        304
│    │    └─Sequential: 3-10                                 [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-11                                    [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-12                                  [16, 307, 12, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-3                         [16, 12, 307, 152]        --
│    │    └─Cross_AttentionLayer: 3-13                       [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-14                                    [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-15                                  [16, 307, 12, 152]        304
│    │    └─Sequential: 3-16                                 [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-17                                    [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-18                                  [16, 307, 12, 152]        304
├─Dual_graph: 1-7                                            [16, 12, 307, 128]        --
│    └─Graph_projection: 2-4                                 [1, 307, 64]              --
│    │    └─Linear: 3-19                                     [1, 307, 64]              19,712
│    │    └─ReLU: 3-20                                       [1, 307, 64]              --
│    │    └─Dropout: 3-21                                    [1, 307, 64]              --
│    │    └─Linear: 3-22                                     [1, 307, 64]              4,160
│    └─Graph_projection: 2-5                                 [1, 307, 64]              --
│    │    └─Linear: 3-23                                     [1, 307, 64]              19,712
│    │    └─ReLU: 3-24                                       [1, 307, 64]              --
│    │    └─Dropout: 3-25                                    [1, 307, 64]              --
│    │    └─Linear: 3-26                                     [1, 307, 64]              4,160
├─Fusion_Model: 1-8                                          [16, 12, 307, 152]        --
│    └─Sequential: 2-6                                       [16, 12, 307, 80]         --
│    │    └─MLP: 3-27                                        [16, 12, 307, 208]        86,944
│    │    └─MLP: 3-28                                        [16, 12, 307, 208]        86,944
│    │    └─Linear: 3-29                                     [16, 12, 307, 80]         16,720
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-7                               [16, 12, 307, 152]        (recursive)
│    │    └─Sequential: 3-30                                 [16, 12, 307, 152]        23,256
│    │    └─AttentionLayer: 3-31                             [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-32                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-33                                  [16, 12, 307, 152]        (recursive)
│    │    └─Sequential: 3-34                                 [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-35                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-36                                  [16, 12, 307, 152]        (recursive)
├─ModuleList: 1-10                                           --                        --
│    └─Spatial_Heterogeneity_SelfAttentionLayer: 2-8         [16, 12, 307, 152]        --
│    │    └─Spatial_Heterogeneity_AttentionLayer: 3-37       [16, 12, 307, 152]        93,024
│    │    └─Dropout: 3-38                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-39                                  [16, 12, 307, 152]        304
│    │    └─Sequential: 3-40                                 [16, 12, 307, 152]        78,232
│    │    └─Dropout: 3-41                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-42                                  [16, 12, 307, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-9                         [16, 12, 307, 152]        (recursive)
│    │    └─Cross_AttentionLayer: 3-43                       [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-44                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-45                                  [16, 12, 307, 152]        (recursive)
│    │    └─Sequential: 3-46                                 [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-47                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-48                                  [16, 12, 307, 152]        (recursive)
├─Linear: 1-12                                               [16, 307, 12]             21,900
==============================================================================================================
Total params: 1,272,908
Trainable params: 1,272,908
Non-trainable params: 0
Total mult-adds (M): 20.43
==============================================================================================================
Input size (MB): 0.71
Forward/backward pass size (MB): 4271.48
Params size (MB): 3.91
Estimated Total Size (MB): 4276.10
==============================================================================================================

Loss: HuberLoss

2025-06-04 18:10:32.524269 Epoch 1  	Train Loss = 29.92052 Val Loss = 26.60144
2025-06-04 18:15:53.074892 Epoch 2  	Train Loss = 21.95283 Val Loss = 21.98471
2025-06-04 18:21:12.715520 Epoch 3  	Train Loss = 20.90994 Val Loss = 21.57260
2025-06-04 18:26:32.759467 Epoch 4  	Train Loss = 20.00381 Val Loss = 20.59386
2025-06-04 18:31:52.541485 Epoch 5  	Train Loss = 19.66616 Val Loss = 21.96485
2025-06-04 18:37:12.520283 Epoch 6  	Train Loss = 19.16710 Val Loss = 19.82089
2025-06-04 18:42:32.584454 Epoch 7  	Train Loss = 19.08797 Val Loss = 19.33877
2025-06-04 18:47:52.259223 Epoch 8  	Train Loss = 18.65022 Val Loss = 19.16511
2025-06-04 18:53:11.382083 Epoch 9  	Train Loss = 18.59321 Val Loss = 19.63391
2025-06-04 18:58:31.007784 Epoch 10  	Train Loss = 18.54151 Val Loss = 19.55500
2025-06-04 19:03:51.061265 Epoch 11  	Train Loss = 18.34488 Val Loss = 20.59357
2025-06-04 19:09:10.452614 Epoch 12  	Train Loss = 18.14937 Val Loss = 19.13799
2025-06-04 19:14:30.389694 Epoch 13  	Train Loss = 18.17043 Val Loss = 18.40610
2025-06-04 19:19:50.021217 Epoch 14  	Train Loss = 17.89079 Val Loss = 18.75472
2025-06-04 19:25:09.733626 Epoch 15  	Train Loss = 18.00777 Val Loss = 18.76368
2025-06-04 19:30:29.699553 Epoch 16  	Train Loss = 17.04048 Val Loss = 17.90700
2025-06-04 19:35:49.332737 Epoch 17  	Train Loss = 16.89154 Val Loss = 17.90264
2025-06-04 19:41:08.556494 Epoch 18  	Train Loss = 16.82458 Val Loss = 17.78221
2025-06-04 19:46:28.329875 Epoch 19  	Train Loss = 16.79790 Val Loss = 17.75223
2025-06-04 19:51:47.568010 Epoch 20  	Train Loss = 16.76869 Val Loss = 17.75140
2025-06-04 19:57:07.258811 Epoch 21  	Train Loss = 16.73297 Val Loss = 17.73305
2025-06-04 20:02:26.616593 Epoch 22  	Train Loss = 16.70748 Val Loss = 17.97309
2025-06-04 20:07:46.076726 Epoch 23  	Train Loss = 16.67911 Val Loss = 17.77544
2025-06-04 20:13:05.527869 Epoch 24  	Train Loss = 16.63971 Val Loss = 17.79053
2025-06-04 20:18:24.756659 Epoch 25  	Train Loss = 16.62627 Val Loss = 17.79333
2025-06-04 20:23:44.023459 Epoch 26  	Train Loss = 16.60656 Val Loss = 17.78692
2025-06-04 20:29:03.270654 Epoch 27  	Train Loss = 16.57999 Val Loss = 17.79078
2025-06-04 20:34:22.583526 Epoch 28  	Train Loss = 16.55353 Val Loss = 17.73097
2025-06-04 20:39:42.145725 Epoch 29  	Train Loss = 16.53272 Val Loss = 17.85517
2025-06-04 20:45:01.478941 Epoch 30  	Train Loss = 16.51112 Val Loss = 17.71906
2025-06-04 20:50:20.682409 Epoch 31  	Train Loss = 16.41399 Val Loss = 17.67288
2025-06-04 20:55:39.872241 Epoch 32  	Train Loss = 16.40154 Val Loss = 17.63571
2025-06-04 21:00:59.049931 Epoch 33  	Train Loss = 16.40387 Val Loss = 17.65652
2025-06-04 21:06:18.221631 Epoch 34  	Train Loss = 16.39861 Val Loss = 17.66541
2025-06-04 21:11:37.667300 Epoch 35  	Train Loss = 16.39634 Val Loss = 17.64315
2025-06-04 21:16:56.771771 Epoch 36  	Train Loss = 16.39393 Val Loss = 17.64856
2025-06-04 21:22:15.869378 Epoch 37  	Train Loss = 16.38618 Val Loss = 17.66424
2025-06-04 21:27:35.087198 Epoch 38  	Train Loss = 16.38838 Val Loss = 17.67156
2025-06-04 21:32:54.393856 Epoch 39  	Train Loss = 16.38096 Val Loss = 17.63757
2025-06-04 21:38:13.784769 Epoch 40  	Train Loss = 16.38038 Val Loss = 17.66271
2025-06-04 21:43:32.904344 Epoch 41  	Train Loss = 16.37942 Val Loss = 17.64659
2025-06-04 21:48:51.853559 Epoch 42  	Train Loss = 16.37590 Val Loss = 17.64868
2025-06-04 21:54:11.037106 Epoch 43  	Train Loss = 16.36848 Val Loss = 17.67349
2025-06-04 21:59:29.934786 Epoch 44  	Train Loss = 16.36825 Val Loss = 17.64023
2025-06-04 22:04:49.053535 Epoch 45  	Train Loss = 16.37079 Val Loss = 17.65439
2025-06-04 22:10:08.274354 Epoch 46  	Train Loss = 16.35772 Val Loss = 17.66258
2025-06-04 22:15:27.061359 Epoch 47  	Train Loss = 16.35290 Val Loss = 17.64801
2025-06-04 22:20:46.078636 Epoch 48  	Train Loss = 16.36093 Val Loss = 17.66551
2025-06-04 22:26:05.166439 Epoch 49  	Train Loss = 16.34935 Val Loss = 17.63501
2025-06-04 22:31:23.729701 Epoch 50  	Train Loss = 16.35037 Val Loss = 17.63166
2025-06-04 22:36:42.758383 Epoch 51  	Train Loss = 16.34508 Val Loss = 17.63995
2025-06-04 22:42:01.499266 Epoch 52  	Train Loss = 16.34539 Val Loss = 17.64100
2025-06-04 22:46:27.132336 Epoch 53  	Train Loss = 16.34355 Val Loss = 17.63569
2025-06-04 22:49:59.615274 Epoch 54  	Train Loss = 16.34016 Val Loss = 17.63703
2025-06-04 22:53:32.047375 Epoch 55  	Train Loss = 16.34232 Val Loss = 17.63359
2025-06-04 22:57:04.522745 Epoch 56  	Train Loss = 16.33511 Val Loss = 17.63157
2025-06-04 23:00:36.617937 Epoch 57  	Train Loss = 16.33863 Val Loss = 17.63537
2025-06-04 23:04:09.047261 Epoch 58  	Train Loss = 16.33669 Val Loss = 17.63398
2025-06-04 23:07:41.274834 Epoch 59  	Train Loss = 16.33812 Val Loss = 17.62829
2025-06-04 23:11:13.737671 Epoch 60  	Train Loss = 16.32773 Val Loss = 17.63057
2025-06-04 23:14:45.980510 Epoch 61  	Train Loss = 16.33927 Val Loss = 17.63673
2025-06-04 23:18:18.409611 Epoch 62  	Train Loss = 16.33474 Val Loss = 17.63898
2025-06-04 23:21:50.552506 Epoch 63  	Train Loss = 16.33398 Val Loss = 17.63090
2025-06-04 23:25:22.930053 Epoch 64  	Train Loss = 16.33781 Val Loss = 17.64325
2025-06-04 23:28:55.376701 Epoch 65  	Train Loss = 16.33826 Val Loss = 17.63729
2025-06-04 23:32:27.701778 Epoch 66  	Train Loss = 16.33827 Val Loss = 17.64127
2025-06-04 23:36:00.193493 Epoch 67  	Train Loss = 16.33543 Val Loss = 17.63775
2025-06-04 23:39:32.380785 Epoch 68  	Train Loss = 16.33839 Val Loss = 17.63316
2025-06-04 23:43:04.823321 Epoch 69  	Train Loss = 16.33579 Val Loss = 17.63477
2025-06-04 23:46:37.360079 Epoch 70  	Train Loss = 16.32977 Val Loss = 17.63650
2025-06-04 23:50:10.080186 Epoch 71  	Train Loss = 16.32916 Val Loss = 17.63223
2025-06-04 23:53:42.496632 Epoch 72  	Train Loss = 16.32673 Val Loss = 17.63502
2025-06-04 23:57:14.713749 Epoch 73  	Train Loss = 16.32600 Val Loss = 17.63500
2025-06-05 00:00:46.904844 Epoch 74  	Train Loss = 16.33557 Val Loss = 17.63640
2025-06-05 00:04:19.379254 Epoch 75  	Train Loss = 16.33581 Val Loss = 17.63456
2025-06-05 00:07:51.686496 Epoch 76  	Train Loss = 16.33250 Val Loss = 17.63753
2025-06-05 00:11:24.009449 Epoch 77  	Train Loss = 16.33251 Val Loss = 17.63156
2025-06-05 00:14:54.813131 Epoch 78  	Train Loss = 16.32788 Val Loss = 17.63539
2025-06-05 00:17:18.626738 Epoch 79  	Train Loss = 16.33000 Val Loss = 17.63211
Early stopping at epoch: 79
Best at epoch 59:
Train Loss = 16.33812
Train RMSE = 27.89482, MAE = 16.72877, MAPE = 11.90598
Val Loss = 17.62829
Val RMSE = 30.63517, MAE = 18.30415, MAPE = 11.74525
Saved Model: ../saved_models/STHDformer-PEMS04-2025-06-04-18-05-10.pt
--------- Test ---------
All Steps RMSE = 30.07842, MAE = 18.27199, MAPE = 11.92354
Step 1 RMSE = 27.06738, MAE = 16.70806, MAPE = 11.01580
Step 2 RMSE = 27.97224, MAE = 17.13779, MAPE = 11.30158
Step 3 RMSE = 28.68402, MAE = 17.51517, MAPE = 11.51483
Step 4 RMSE = 29.23878, MAE = 17.80680, MAPE = 11.66012
Step 5 RMSE = 29.70855, MAE = 18.06119, MAPE = 11.78876
Step 6 RMSE = 30.11234, MAE = 18.27596, MAPE = 11.89047
Step 7 RMSE = 30.49672, MAE = 18.48821, MAPE = 12.01953
Step 8 RMSE = 30.83130, MAE = 18.67880, MAPE = 12.13942
Step 9 RMSE = 31.14996, MAE = 18.86682, MAPE = 12.24644
Step 10 RMSE = 31.43060, MAE = 19.03579, MAPE = 12.36040
Step 11 RMSE = 31.73066, MAE = 19.23364, MAPE = 12.49653
Step 12 RMSE = 32.07394, MAE = 19.45555, MAPE = 12.64824
Inference time: 12.65 s
