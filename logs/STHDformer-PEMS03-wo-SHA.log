PEMS03
Trainset:	x-(15711, 12, 358, 3)	y-(15711, 12, 358, 1)
Valset:  	x-(5237, 12, 358, 3)  	y-(5237, 12, 358, 1)
Testset:	x-(5237, 12, 358, 3)	y-(5237, 12, 358, 1)

Random seed = 1
--------- STHDformer ---------
{
    "num_nodes": 358,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0005,
    "milestones": [
        15,
        30,
        40
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 358,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "steps_per_week": 7,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers_t": 1,
        "num_layers_c": 1,
        "num_layers_s": 1,
        "num_layers_mlp": 2,
        "dropout": 0.1,
        "adaptive_embedding_dim": 80,
        "node_dim": 64,
        "use_temporal_heterogeneity": false,
        "use_spatial_heterogeneity": false,
        "use_temporal_cross": true,
        "use_spatial_cross": true,
        "use_mixed_proj": true
    }
}
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
STHDformer                                    [16, 12, 358, 1]          320,424
├─Linear: 1-1                                 [16, 12, 358, 24]         96
├─Embedding: 1-2                              [16, 12, 358, 24]         6,912
├─Embedding: 1-3                              [16, 12, 358, 24]         168
├─ModuleList: 1-9                             --                        (recursive)
│    └─SelfAttentionLayer: 2-1                [16, 12, 358, 152]        23,256
│    │    └─AttentionLayer: 3-1               [16, 358, 12, 152]        93,024
│    │    └─Dropout: 3-2                      [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-3                    [16, 358, 12, 152]        304
│    │    └─Sequential: 3-4                   [16, 358, 12, 152]        78,232
│    │    └─Dropout: 3-5                      [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-6                    [16, 358, 12, 152]        304
├─ModuleList: 1-5                             --                        --
│    └─TemporalTCNLayer: 2-2                  [16, 12, 358, 152]        --
│    │    └─Conv1d: 3-7                       [5728, 152, 12]           69,464
│    │    └─ReLU: 3-8                         [5728, 152, 12]           --
│    │    └─Dropout: 3-9                      [5728, 152, 12]           --
│    │    └─LayerNorm: 3-10                   [5728, 12, 152]           304
├─ModuleList: 1-11                            --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-3          [16, 12, 358, 152]        --
│    │    └─Cross_AttentionLayer: 3-11        [16, 358, 12, 152]        93,024
│    │    └─Dropout: 3-12                     [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-13                   [16, 358, 12, 152]        304
│    │    └─Sequential: 3-14                  [16, 358, 12, 152]        78,232
│    │    └─Dropout: 3-15                     [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-16                   [16, 358, 12, 152]        304
├─Dual_graph: 1-7                             [16, 12, 358, 128]        --
│    └─Graph_projection: 2-4                  [1, 358, 64]              --
│    │    └─Linear: 3-17                      [1, 358, 64]              22,976
│    │    └─ReLU: 3-18                        [1, 358, 64]              --
│    │    └─Dropout: 3-19                     [1, 358, 64]              --
│    │    └─Linear: 3-20                      [1, 358, 64]              4,160
│    └─Graph_projection: 2-5                  [1, 358, 64]              --
│    │    └─Linear: 3-21                      [1, 358, 64]              22,976
│    │    └─ReLU: 3-22                        [1, 358, 64]              --
│    │    └─Dropout: 3-23                     [1, 358, 64]              --
│    │    └─Linear: 3-24                      [1, 358, 64]              4,160
├─Fusion_Model: 1-8                           [16, 12, 358, 152]        --
│    └─Sequential: 2-6                        [16, 12, 358, 80]         --
│    │    └─MLP: 3-25                         [16, 12, 358, 208]        86,944
│    │    └─MLP: 3-26                         [16, 12, 358, 208]        86,944
│    │    └─Linear: 3-27                      [16, 12, 358, 80]         16,720
├─ModuleList: 1-9                             --                        (recursive)
│    └─SelfAttentionLayer: 2-7                [16, 12, 358, 152]        (recursive)
│    │    └─Sequential: 3-28                  [16, 12, 358, 152]        23,256
│    │    └─AttentionLayer: 3-29              [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-30                     [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-31                   [16, 12, 358, 152]        (recursive)
│    │    └─Sequential: 3-32                  [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-33                     [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-34                   [16, 12, 358, 152]        (recursive)
├─ModuleList: 1-10                            --                        --
│    └─SpatialGATLayer: 2-8                   [16, 12, 358, 152]        76
│    │    └─Linear: 3-35                      [192, 358, 152]           23,256
│    │    └─LeakyReLU: 3-36                   [192, 4, 358, 358]        --
│    │    └─Dropout: 3-37                     [192, 4, 358, 358]        --
├─ModuleList: 1-11                            --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-9          [16, 12, 358, 152]        (recursive)
│    │    └─Cross_AttentionLayer: 3-38        [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-39                     [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-40                   [16, 12, 358, 152]        (recursive)
│    │    └─Sequential: 3-41                  [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-42                     [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-43                   [16, 12, 358, 152]        (recursive)
├─Linear: 1-12                                [16, 358, 12]             21,900
===============================================================================================
Total params: 1,077,720
Trainable params: 1,077,720
Non-trainable params: 0
Total mult-adds (G): 4.80
===============================================================================================
Input size (MB): 0.82
Forward/backward pass size (MB): 3780.11
Params size (MB): 2.94
Estimated Total Size (MB): 3783.87
===============================================================================================

Loss: HuberLoss

2025-07-01 23:31:17.987563 Epoch 1  	Train Loss = 21.80423 Val Loss = 15.64350
2025-07-01 23:36:09.685413 Epoch 2  	Train Loss = 16.23905 Val Loss = 14.76624
2025-07-01 23:41:01.278859 Epoch 3  	Train Loss = 15.07108 Val Loss = 15.18981
2025-07-01 23:45:53.283120 Epoch 4  	Train Loss = 14.63863 Val Loss = 14.33249
2025-07-01 23:50:45.368413 Epoch 5  	Train Loss = 14.30327 Val Loss = 14.16046
2025-07-01 23:55:37.008504 Epoch 6  	Train Loss = 14.00235 Val Loss = 13.74948
2025-07-02 00:00:28.604549 Epoch 7  	Train Loss = 13.83477 Val Loss = 13.90295
2025-07-02 00:05:20.169328 Epoch 8  	Train Loss = 13.75216 Val Loss = 13.62546
2025-07-02 00:10:12.577948 Epoch 9  	Train Loss = 13.48768 Val Loss = 14.03056
2025-07-02 00:15:04.076624 Epoch 10  	Train Loss = 13.39415 Val Loss = 13.60301
2025-07-02 00:19:55.583112 Epoch 11  	Train Loss = 13.36288 Val Loss = 14.18930
2025-07-02 00:24:47.093109 Epoch 12  	Train Loss = 13.29679 Val Loss = 13.87989
2025-07-02 00:29:39.569595 Epoch 13  	Train Loss = 13.16027 Val Loss = 13.35370
2025-07-02 00:34:31.031706 Epoch 14  	Train Loss = 13.14709 Val Loss = 13.36251
2025-07-02 00:39:22.543431 Epoch 15  	Train Loss = 13.02156 Val Loss = 13.28063
2025-07-02 00:44:14.013526 Epoch 16  	Train Loss = 12.35693 Val Loss = 12.97958
2025-07-02 00:49:06.255255 Epoch 17  	Train Loss = 12.30393 Val Loss = 12.97423
2025-07-02 00:53:58.096810 Epoch 18  	Train Loss = 12.27790 Val Loss = 13.03270
2025-07-02 00:58:49.626014 Epoch 19  	Train Loss = 12.25947 Val Loss = 12.98669
2025-07-02 01:03:41.122189 Epoch 20  	Train Loss = 12.23826 Val Loss = 12.97793
2025-07-02 01:08:32.834608 Epoch 21  	Train Loss = 12.21950 Val Loss = 12.97102
2025-07-02 01:13:25.080022 Epoch 22  	Train Loss = 12.20956 Val Loss = 13.00984
2025-07-02 01:18:16.595976 Epoch 23  	Train Loss = 12.18854 Val Loss = 12.95644
2025-07-02 01:23:08.169365 Epoch 24  	Train Loss = 12.18125 Val Loss = 12.96639
2025-07-02 01:27:59.689114 Epoch 25  	Train Loss = 12.16532 Val Loss = 12.97127
2025-07-02 01:32:52.083981 Epoch 26  	Train Loss = 12.14914 Val Loss = 12.98799
2025-07-02 01:37:43.651482 Epoch 27  	Train Loss = 12.14094 Val Loss = 12.95885
2025-07-02 01:42:35.253995 Epoch 28  	Train Loss = 12.12624 Val Loss = 13.00439
2025-07-02 01:47:26.774577 Epoch 29  	Train Loss = 12.11367 Val Loss = 12.95979
2025-07-02 01:52:19.086765 Epoch 30  	Train Loss = 12.11098 Val Loss = 12.99508
2025-07-02 01:57:10.608172 Epoch 31  	Train Loss = 12.02396 Val Loss = 12.89146
2025-07-02 02:02:02.018626 Epoch 32  	Train Loss = 12.01576 Val Loss = 12.91445
2025-07-02 02:06:53.455831 Epoch 33  	Train Loss = 12.01297 Val Loss = 12.88527
2025-07-02 02:11:45.396280 Epoch 34  	Train Loss = 12.00971 Val Loss = 12.88863
2025-07-02 02:16:37.433531 Epoch 35  	Train Loss = 12.00832 Val Loss = 12.92124
2025-07-02 02:21:28.894180 Epoch 36  	Train Loss = 12.00574 Val Loss = 12.88858
2025-07-02 02:26:20.354370 Epoch 37  	Train Loss = 12.00364 Val Loss = 12.89629
2025-07-02 02:31:11.894242 Epoch 38  	Train Loss = 12.00276 Val Loss = 12.94276
2025-07-02 02:36:04.240723 Epoch 39  	Train Loss = 12.00145 Val Loss = 12.89657
2025-07-02 02:40:55.740274 Epoch 40  	Train Loss = 11.99947 Val Loss = 12.91739
2025-07-02 02:45:47.214780 Epoch 41  	Train Loss = 11.98885 Val Loss = 12.90419
2025-07-02 02:50:38.756767 Epoch 42  	Train Loss = 11.98783 Val Loss = 12.90433
2025-07-02 02:55:31.124626 Epoch 43  	Train Loss = 11.98794 Val Loss = 12.90073
2025-07-02 03:00:22.588393 Epoch 44  	Train Loss = 11.98798 Val Loss = 12.91410
2025-07-02 03:05:14.039622 Epoch 45  	Train Loss = 11.98782 Val Loss = 12.90897
2025-07-02 03:10:05.532815 Epoch 46  	Train Loss = 11.98742 Val Loss = 12.90120
2025-07-02 03:14:57.677862 Epoch 47  	Train Loss = 11.98673 Val Loss = 12.90675
2025-07-02 03:19:49.409091 Epoch 48  	Train Loss = 11.98675 Val Loss = 12.90515
2025-07-02 03:24:40.950244 Epoch 49  	Train Loss = 11.98721 Val Loss = 12.90257
2025-07-02 03:29:32.480424 Epoch 50  	Train Loss = 11.98692 Val Loss = 12.91038
2025-07-02 03:34:24.107608 Epoch 51  	Train Loss = 11.98650 Val Loss = 12.90373
2025-07-02 03:39:16.241818 Epoch 52  	Train Loss = 11.98580 Val Loss = 12.91172
2025-07-02 03:44:07.747308 Epoch 53  	Train Loss = 11.98535 Val Loss = 12.90469
Early stopping at epoch: 53
Best at epoch 33:
Train Loss = 12.01297
Train RMSE = 20.42971, MAE = 12.33660, MAPE = 11.25060
Val Loss = 12.88527
Val RMSE = 21.74791, MAE = 13.40858, MAPE = 12.59169
Saved Model: ../saved_models/STHDformer-PEMS03-2025-07-01-23-26-47.pt
--------- Test ---------
All Steps RMSE = 27.04987, MAE = 15.00151, MAPE = 15.11135
Step 1 RMSE = 21.56920, MAE = 12.47546, MAPE = 13.13833
Step 2 RMSE = 23.42299, MAE = 13.16722, MAPE = 13.67650
Step 3 RMSE = 24.74780, MAE = 13.74372, MAPE = 14.09182
Step 4 RMSE = 25.79241, MAE = 14.21394, MAPE = 14.40416
Step 5 RMSE = 26.56986, MAE = 14.61576, MAPE = 14.72503
Step 6 RMSE = 27.23595, MAE = 15.00858, MAPE = 15.06139
Step 7 RMSE = 27.88076, MAE = 15.38633, MAPE = 15.36933
Step 8 RMSE = 28.34207, MAE = 15.69104, MAPE = 15.62417
Step 9 RMSE = 28.81817, MAE = 16.01507, MAPE = 15.94722
Step 10 RMSE = 29.22794, MAE = 16.29002, MAPE = 16.10221
Step 11 RMSE = 29.62533, MAE = 16.56441, MAPE = 16.43979
Step 12 RMSE = 29.96652, MAE = 16.84641, MAPE = 16.75617
Inference time: 30.62 s
