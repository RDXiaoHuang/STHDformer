PEMS03
Trainset:	x-(15711, 12, 358, 3)	y-(15711, 12, 358, 1)
Valset:  	x-(5237, 12, 358, 3)  	y-(5237, 12, 358, 1)
Testset:	x-(5237, 12, 358, 3)	y-(5237, 12, 358, 1)

Random seed = 1
--------- STHDformer ---------
{
    "num_nodes": 358,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0005,
    "milestones": [
        15,
        30,
        40
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 358,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "steps_per_week": 7,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers_t": 1,
        "num_layers_c": 1,
        "num_layers_s": 1,
        "num_layers_mlp": 2,
        "dropout": 0.1,
        "adaptive_embedding_dim": 80,
        "node_dim": 64,
        "use_temporal_heterogeneity": false,
        "use_spatial_heterogeneity": true,
        "use_temporal_cross": true,
        "use_spatial_cross": true,
        "use_mixed_proj": true
    }
}
==============================================================================================================
Layer (type:depth-idx)                                       Output Shape              Param #
==============================================================================================================
STHDformer                                                   [16, 12, 358, 1]          320,424
├─Linear: 1-1                                                [16, 12, 358, 24]         96
├─Embedding: 1-2                                             [16, 12, 358, 24]         6,912
├─Embedding: 1-3                                             [16, 12, 358, 24]         168
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-1                               [16, 12, 358, 152]        23,256
│    │    └─AttentionLayer: 3-1                              [16, 358, 12, 152]        93,024
│    │    └─Dropout: 3-2                                     [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-3                                   [16, 358, 12, 152]        304
│    │    └─Sequential: 3-4                                  [16, 358, 12, 152]        78,232
│    │    └─Dropout: 3-5                                     [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-6                                   [16, 358, 12, 152]        304
├─ModuleList: 1-5                                            --                        --
│    └─TemporalTCNLayer: 2-2                                 [16, 12, 358, 152]        --
│    │    └─Conv1d: 3-7                                      [5728, 152, 12]           69,464
│    │    └─ReLU: 3-8                                        [5728, 152, 12]           --
│    │    └─Dropout: 3-9                                     [5728, 152, 12]           --
│    │    └─LayerNorm: 3-10                                  [5728, 12, 152]           304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-3                         [16, 12, 358, 152]        --
│    │    └─Cross_AttentionLayer: 3-11                       [16, 358, 12, 152]        93,024
│    │    └─Dropout: 3-12                                    [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-13                                  [16, 358, 12, 152]        304
│    │    └─Sequential: 3-14                                 [16, 358, 12, 152]        78,232
│    │    └─Dropout: 3-15                                    [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-16                                  [16, 358, 12, 152]        304
├─Dual_graph: 1-7                                            [16, 12, 358, 128]        --
│    └─Graph_projection: 2-4                                 [1, 358, 64]              --
│    │    └─Linear: 3-17                                     [1, 358, 64]              22,976
│    │    └─ReLU: 3-18                                       [1, 358, 64]              --
│    │    └─Dropout: 3-19                                    [1, 358, 64]              --
│    │    └─Linear: 3-20                                     [1, 358, 64]              4,160
│    └─Graph_projection: 2-5                                 [1, 358, 64]              --
│    │    └─Linear: 3-21                                     [1, 358, 64]              22,976
│    │    └─ReLU: 3-22                                       [1, 358, 64]              --
│    │    └─Dropout: 3-23                                    [1, 358, 64]              --
│    │    └─Linear: 3-24                                     [1, 358, 64]              4,160
├─Fusion_Model: 1-8                                          [16, 12, 358, 152]        --
│    └─Sequential: 2-6                                       [16, 12, 358, 80]         --
│    │    └─MLP: 3-25                                        [16, 12, 358, 208]        86,944
│    │    └─MLP: 3-26                                        [16, 12, 358, 208]        86,944
│    │    └─Linear: 3-27                                     [16, 12, 358, 80]         16,720
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-7                               [16, 12, 358, 152]        (recursive)
│    │    └─Sequential: 3-28                                 [16, 12, 358, 152]        23,256
│    │    └─AttentionLayer: 3-29                             [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-30                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-31                                  [16, 12, 358, 152]        (recursive)
│    │    └─Sequential: 3-32                                 [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-33                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-34                                  [16, 12, 358, 152]        (recursive)
├─ModuleList: 1-10                                           --                        --
│    └─Spatial_Heterogeneity_SelfAttentionLayer: 2-8         [16, 12, 358, 152]        --
│    │    └─Spatial_Heterogeneity_AttentionLayer: 3-35       [16, 12, 358, 152]        93,024
│    │    └─Dropout: 3-36                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-37                                  [16, 12, 358, 152]        304
│    │    └─Sequential: 3-38                                 [16, 12, 358, 152]        78,232
│    │    └─Dropout: 3-39                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-40                                  [16, 12, 358, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-9                         [16, 12, 358, 152]        (recursive)
│    │    └─Cross_AttentionLayer: 3-41                       [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-42                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-43                                  [16, 12, 358, 152]        (recursive)
│    │    └─Sequential: 3-44                                 [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-45                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-46                                  [16, 12, 358, 152]        (recursive)
├─Linear: 1-12                                               [16, 358, 12]             21,900
==============================================================================================================
Total params: 1,226,252
Trainable params: 1,226,252
Non-trainable params: 0
Total mult-adds (G): 4.79
==============================================================================================================
Input size (MB): 0.82
Forward/backward pass size (MB): 4422.38
Params size (MB): 3.53
Estimated Total Size (MB): 4426.74
==============================================================================================================

Loss: HuberLoss

2025-06-27 04:57:29.611035 Epoch 1  	Train Loss = 22.36463 Val Loss = 16.50452
2025-06-27 05:10:22.810453 Epoch 2  	Train Loss = 16.37401 Val Loss = 14.92105
2025-06-27 05:23:13.145513 Epoch 3  	Train Loss = 15.13651 Val Loss = 15.00643
2025-06-27 05:36:03.271882 Epoch 4  	Train Loss = 14.57087 Val Loss = 15.01988
2025-06-27 05:48:54.536757 Epoch 5  	Train Loss = 14.37935 Val Loss = 14.36731
2025-06-27 06:01:47.132558 Epoch 6  	Train Loss = 14.14247 Val Loss = 14.19953
2025-06-27 06:14:40.197813 Epoch 7  	Train Loss = 13.89762 Val Loss = 14.13309
2025-06-27 06:27:32.842460 Epoch 8  	Train Loss = 13.69573 Val Loss = 14.05221
2025-06-27 06:40:25.436052 Epoch 9  	Train Loss = 13.51720 Val Loss = 13.71014
2025-06-27 06:52:36.218372 Epoch 10  	Train Loss = 13.48853 Val Loss = 13.92693
2025-06-27 07:02:56.481355 Epoch 11  	Train Loss = 13.38255 Val Loss = 13.74442
2025-06-27 07:13:16.793999 Epoch 12  	Train Loss = 13.31420 Val Loss = 13.66838
2025-06-27 07:23:37.335554 Epoch 13  	Train Loss = 13.15231 Val Loss = 13.50719
2025-06-27 07:33:57.479948 Epoch 14  	Train Loss = 13.13711 Val Loss = 13.51379
2025-06-27 07:44:17.586210 Epoch 15  	Train Loss = 12.99477 Val Loss = 13.77106
2025-06-27 07:54:38.374667 Epoch 16  	Train Loss = 12.37548 Val Loss = 13.13628
2025-06-27 08:04:58.421384 Epoch 17  	Train Loss = 12.31049 Val Loss = 13.07990
2025-06-27 08:15:18.354897 Epoch 18  	Train Loss = 12.28983 Val Loss = 13.11081
2025-06-27 08:25:38.776798 Epoch 19  	Train Loss = 12.26272 Val Loss = 13.10225
2025-06-27 08:35:58.686065 Epoch 20  	Train Loss = 12.24632 Val Loss = 13.12449
2025-06-27 08:46:18.807311 Epoch 21  	Train Loss = 12.22903 Val Loss = 13.00827
2025-06-27 08:56:39.676668 Epoch 22  	Train Loss = 12.21364 Val Loss = 13.08270
2025-06-27 09:06:59.903147 Epoch 23  	Train Loss = 12.19647 Val Loss = 13.08169
2025-06-27 09:17:20.335172 Epoch 24  	Train Loss = 12.18472 Val Loss = 13.08904
2025-06-27 09:27:40.852461 Epoch 25  	Train Loss = 12.17033 Val Loss = 13.09293
2025-06-27 09:38:00.831079 Epoch 26  	Train Loss = 12.15556 Val Loss = 13.02896
2025-06-27 09:48:21.324493 Epoch 27  	Train Loss = 12.14339 Val Loss = 13.07808
2025-06-27 09:58:41.841576 Epoch 28  	Train Loss = 12.13109 Val Loss = 13.00999
2025-06-27 10:09:02.089535 Epoch 29  	Train Loss = 12.11955 Val Loss = 13.05259
2025-06-27 10:19:22.731622 Epoch 30  	Train Loss = 12.10566 Val Loss = 13.00349
2025-06-27 10:29:43.554691 Epoch 31  	Train Loss = 12.02548 Val Loss = 13.02549
2025-06-27 10:40:12.382576 Epoch 32  	Train Loss = 12.01748 Val Loss = 13.01742
2025-06-27 10:51:03.669436 Epoch 33  	Train Loss = 12.01483 Val Loss = 13.02995
2025-06-27 11:01:54.095406 Epoch 34  	Train Loss = 12.01213 Val Loss = 13.04354
2025-06-27 11:12:21.312306 Epoch 35  	Train Loss = 12.01051 Val Loss = 12.99910
2025-06-27 11:22:43.078276 Epoch 36  	Train Loss = 12.00769 Val Loss = 12.99549
2025-06-27 11:33:03.815401 Epoch 37  	Train Loss = 12.00598 Val Loss = 13.01867
2025-06-27 11:43:25.468369 Epoch 38  	Train Loss = 12.00359 Val Loss = 13.02005
2025-06-27 12:34:35.964024 Epoch 39  	Train Loss = 12.00191 Val Loss = 13.02835
2025-06-27 12:39:45.526308 Epoch 40  	Train Loss = 12.00002 Val Loss = 13.03548
2025-06-27 12:44:55.305163 Epoch 41  	Train Loss = 11.99011 Val Loss = 13.03238
2025-06-27 12:50:04.529714 Epoch 42  	Train Loss = 11.98876 Val Loss = 13.02587
2025-06-27 12:55:13.969521 Epoch 43  	Train Loss = 11.98894 Val Loss = 13.01991
2025-06-27 13:00:23.810859 Epoch 44  	Train Loss = 11.98946 Val Loss = 13.01878
2025-06-27 13:05:32.906036 Epoch 45  	Train Loss = 11.98756 Val Loss = 13.02752
2025-06-27 13:10:42.414189 Epoch 46  	Train Loss = 11.98844 Val Loss = 13.01965
2025-06-27 13:15:52.457368 Epoch 47  	Train Loss = 11.98764 Val Loss = 13.02864
2025-06-27 13:21:01.886583 Epoch 48  	Train Loss = 11.98722 Val Loss = 13.01656
2025-06-27 13:26:11.428791 Epoch 49  	Train Loss = 11.98719 Val Loss = 13.01587
2025-06-27 13:31:21.477233 Epoch 50  	Train Loss = 11.98724 Val Loss = 13.02625
2025-06-27 13:36:30.980939 Epoch 51  	Train Loss = 11.98725 Val Loss = 13.01066
2025-06-27 13:41:40.683394 Epoch 52  	Train Loss = 11.98660 Val Loss = 13.02057
2025-06-27 13:46:50.682505 Epoch 53  	Train Loss = 11.98688 Val Loss = 13.02265
2025-06-27 13:52:00.284308 Epoch 54  	Train Loss = 11.98693 Val Loss = 13.01930
2025-06-27 13:57:10.187260 Epoch 55  	Train Loss = 11.98572 Val Loss = 13.00961
2025-06-27 14:02:20.044597 Epoch 56  	Train Loss = 11.98553 Val Loss = 13.02515
Early stopping at epoch: 56
Best at epoch 36:
Train Loss = 12.00769
Train RMSE = 20.44123, MAE = 12.33959, MAPE = 11.31559
Val Loss = 12.99549
Val RMSE = 21.91013, MAE = 13.51446, MAPE = 12.72563
Saved Model: ../saved_models/STHDformer-PEMS03-2025-06-27-04-44-29.pt
--------- Test ---------
All Steps RMSE = 26.47811, MAE = 15.18567, MAPE = 15.26515
Step 1 RMSE = 20.81917, MAE = 12.48417, MAPE = 13.04436
Step 2 RMSE = 22.63271, MAE = 13.23519, MAPE = 13.70872
Step 3 RMSE = 23.95683, MAE = 13.84978, MAPE = 14.19526
Step 4 RMSE = 25.02242, MAE = 14.34298, MAPE = 14.50026
Step 5 RMSE = 25.85014, MAE = 14.79313, MAPE = 14.87940
Step 6 RMSE = 26.54639, MAE = 15.19309, MAPE = 15.25243
Step 7 RMSE = 27.20509, MAE = 15.57019, MAPE = 15.54690
Step 8 RMSE = 27.76915, MAE = 15.91371, MAPE = 15.76799
Step 9 RMSE = 28.33045, MAE = 16.26468, MAPE = 16.26872
Step 10 RMSE = 28.80680, MAE = 16.53700, MAPE = 16.34636
Step 11 RMSE = 29.33749, MAE = 16.86556, MAPE = 16.76120
Step 12 RMSE = 29.83338, MAE = 17.17838, MAPE = 16.91016
Inference time: 68.95 s
