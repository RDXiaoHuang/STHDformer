PEMS08
Trainset:	x-(10700, 12, 170, 3)	y-(10700, 12, 170, 1)
Valset:  	x-(3567, 12, 170, 3)  	y-(3567, 12, 170, 1)
Testset:	x-(3566, 12, 170, 3)	y-(3566, 12, 170, 1)

Random seed = 1
--------- STHDformer ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0015,
    "milestones": [
        25,
        45,
        65
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 30,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 170,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "steps_per_week": 7,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers_t": 1,
        "num_layers_c": 1,
        "num_layers_s": 1,
        "num_layers_mlp": 2,
        "dropout": 0.1,
        "adaptive_embedding_dim": 80,
        "node_dim": 64,
        "use_temporal_heterogeneity": true,
        "use_spatial_heterogeneity": true,
        "use_temporal_cross": true,
        "use_spatial_cross": false,
        "use_mixed_proj": true
    }
}
==============================================================================================================
Layer (type:depth-idx)                                       Output Shape              Param #
==============================================================================================================
STHDformer                                                   [16, 12, 170, 1]          139,944
├─Linear: 1-1                                                [16, 12, 170, 24]         96
├─Embedding: 1-2                                             [16, 12, 170, 24]         6,912
├─Embedding: 1-3                                             [16, 12, 170, 24]         168
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-1                               [16, 12, 170, 152]        23,256
│    │    └─AttentionLayer: 3-1                              [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-2                                     [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-3                                   [16, 170, 12, 152]        304
│    │    └─Sequential: 3-4                                  [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-5                                     [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-6                                   [16, 170, 12, 152]        304
├─ModuleList: 1-5                                            --                        --
│    └─Temporal_Heterogeneity_SelfAttentionLayer: 2-2        [16, 12, 170, 152]        --
│    │    └─Temporal_Heterogeneity_AttentionLayer: 3-7       [16, 170, 12, 152]        93,072
│    │    └─Dropout: 3-8                                     [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-9                                   [16, 170, 12, 152]        304
│    │    └─Sequential: 3-10                                 [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-11                                    [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-12                                  [16, 170, 12, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-3                         [16, 12, 170, 152]        --
│    │    └─Cross_AttentionLayer: 3-13                       [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-14                                    [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-15                                  [16, 170, 12, 152]        304
│    │    └─Sequential: 3-16                                 [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-17                                    [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-18                                  [16, 170, 12, 152]        304
├─Dual_graph: 1-7                                            [16, 12, 170, 128]        --
│    └─Graph_projection: 2-4                                 [1, 170, 64]              --
│    │    └─Linear: 3-19                                     [1, 170, 64]              10,944
│    │    └─ReLU: 3-20                                       [1, 170, 64]              --
│    │    └─Dropout: 3-21                                    [1, 170, 64]              --
│    │    └─Linear: 3-22                                     [1, 170, 64]              4,160
│    └─Graph_projection: 2-5                                 [1, 170, 64]              --
│    │    └─Linear: 3-23                                     [1, 170, 64]              10,944
│    │    └─ReLU: 3-24                                       [1, 170, 64]              --
│    │    └─Dropout: 3-25                                    [1, 170, 64]              --
│    │    └─Linear: 3-26                                     [1, 170, 64]              4,160
├─Fusion_Model: 1-8                                          [16, 12, 170, 152]        --
│    └─Sequential: 2-6                                       [16, 12, 170, 80]         --
│    │    └─MLP: 3-27                                        [16, 12, 170, 208]        86,944
│    │    └─MLP: 3-28                                        [16, 12, 170, 208]        86,944
│    │    └─Linear: 3-29                                     [16, 12, 170, 80]         16,720
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-7                               [16, 12, 170, 152]        (recursive)
│    │    └─Sequential: 3-30                                 [16, 12, 170, 152]        23,256
│    │    └─AttentionLayer: 3-31                             [16, 12, 170, 152]        (recursive)
│    │    └─Dropout: 3-32                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-33                                  [16, 12, 170, 152]        (recursive)
│    │    └─Sequential: 3-34                                 [16, 12, 170, 152]        (recursive)
│    │    └─Dropout: 3-35                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-36                                  [16, 12, 170, 152]        (recursive)
├─ModuleList: 1-10                                           --                        --
│    └─Spatial_Heterogeneity_SelfAttentionLayer: 2-8         [16, 12, 170, 152]        --
│    │    └─Spatial_Heterogeneity_AttentionLayer: 3-37       [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-38                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-39                                  [16, 12, 170, 152]        304
│    │    └─Sequential: 3-40                                 [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-41                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-42                                  [16, 12, 170, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-9                         [16, 12, 170, 152]        (recursive)
│    │    └─Cross_AttentionLayer: 3-43                       [16, 12, 170, 152]        (recursive)
│    │    └─Dropout: 3-44                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-45                                  [16, 12, 170, 152]        (recursive)
│    │    └─Sequential: 3-46                                 [16, 12, 170, 152]        (recursive)
│    │    └─Dropout: 3-47                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-48                                  [16, 12, 170, 152]        (recursive)
├─Linear: 1-12                                               [16, 170, 12]             21,900
==============================================================================================================
Total params: 1,123,852
Trainable params: 1,123,852
Non-trainable params: 0
Total mult-adds (M): 20.42
==============================================================================================================
Input size (MB): 0.39
Forward/backward pass size (MB): 2365.32
Params size (MB): 3.84
Estimated Total Size (MB): 2369.56
==============================================================================================================

Loss: HuberLoss

2025-06-04 01:20:24.615307 Epoch 1  	Train Loss = 24.64917 Val Loss = 19.11471
2025-06-04 01:26:18.327300 Epoch 2  	Train Loss = 17.94838 Val Loss = 17.31670
2025-06-04 01:32:12.449461 Epoch 3  	Train Loss = 16.95469 Val Loss = 16.29827
2025-06-04 01:38:06.163519 Epoch 4  	Train Loss = 16.32512 Val Loss = 16.15340
2025-06-04 01:43:59.994406 Epoch 5  	Train Loss = 15.95177 Val Loss = 15.25057
2025-06-04 01:49:53.902007 Epoch 6  	Train Loss = 15.57417 Val Loss = 15.55737
2025-06-04 01:55:47.663862 Epoch 7  	Train Loss = 15.43388 Val Loss = 15.48097
2025-06-04 02:01:41.630965 Epoch 8  	Train Loss = 15.14309 Val Loss = 15.08541
2025-06-04 02:07:34.885332 Epoch 9  	Train Loss = 15.01151 Val Loss = 14.90719
2025-06-04 02:13:28.614107 Epoch 10  	Train Loss = 14.92502 Val Loss = 14.93708
2025-06-04 02:19:21.855477 Epoch 11  	Train Loss = 14.70190 Val Loss = 14.73817
2025-06-04 02:25:15.429511 Epoch 12  	Train Loss = 14.66758 Val Loss = 14.87797
2025-06-04 02:31:08.985627 Epoch 13  	Train Loss = 14.43856 Val Loss = 14.79758
2025-06-04 02:37:02.940751 Epoch 14  	Train Loss = 14.43495 Val Loss = 14.73057
2025-06-04 02:42:56.562078 Epoch 15  	Train Loss = 14.33531 Val Loss = 14.16979
2025-06-04 02:48:49.934502 Epoch 16  	Train Loss = 14.24940 Val Loss = 14.26231
2025-06-04 02:54:43.860523 Epoch 17  	Train Loss = 14.04055 Val Loss = 14.14410
2025-06-04 03:00:37.412299 Epoch 18  	Train Loss = 13.93991 Val Loss = 14.02963
2025-06-04 03:06:30.989024 Epoch 19  	Train Loss = 14.01775 Val Loss = 13.87001
2025-06-04 03:12:24.481357 Epoch 20  	Train Loss = 13.83127 Val Loss = 13.90024
2025-06-04 03:18:18.390571 Epoch 21  	Train Loss = 13.76243 Val Loss = 13.80941
2025-06-04 03:24:11.622765 Epoch 22  	Train Loss = 13.73660 Val Loss = 13.78730
2025-06-04 03:30:05.433622 Epoch 23  	Train Loss = 13.66445 Val Loss = 13.99157
2025-06-04 03:35:59.208156 Epoch 24  	Train Loss = 13.62821 Val Loss = 13.97492
2025-06-04 03:41:52.766005 Epoch 25  	Train Loss = 13.52883 Val Loss = 13.68369
2025-06-04 03:47:46.234900 Epoch 26  	Train Loss = 12.80445 Val Loss = 13.16751
2025-06-04 03:53:39.217978 Epoch 27  	Train Loss = 12.71526 Val Loss = 13.21081
2025-06-04 03:59:32.502575 Epoch 28  	Train Loss = 12.68166 Val Loss = 13.16134
2025-06-04 04:05:25.153508 Epoch 29  	Train Loss = 12.65549 Val Loss = 13.27108
2025-06-04 04:11:18.589894 Epoch 30  	Train Loss = 12.63681 Val Loss = 13.13583
2025-06-04 04:17:11.567914 Epoch 31  	Train Loss = 12.61694 Val Loss = 13.18483
2025-06-04 04:23:04.810590 Epoch 32  	Train Loss = 12.60146 Val Loss = 13.15372
2025-06-04 04:28:58.168354 Epoch 33  	Train Loss = 12.57914 Val Loss = 13.15236
2025-06-04 04:34:51.363068 Epoch 34  	Train Loss = 12.56929 Val Loss = 13.10850
2025-06-04 04:40:44.891088 Epoch 35  	Train Loss = 12.54845 Val Loss = 13.30550
2025-06-04 04:46:37.615376 Epoch 36  	Train Loss = 12.53421 Val Loss = 13.11255
2025-06-04 04:52:31.247046 Epoch 37  	Train Loss = 12.52296 Val Loss = 13.17322
2025-06-04 04:58:24.493548 Epoch 38  	Train Loss = 12.51409 Val Loss = 13.14853
2025-06-04 05:04:17.726003 Epoch 39  	Train Loss = 12.50766 Val Loss = 13.18449
2025-06-04 05:10:10.842500 Epoch 40  	Train Loss = 12.48687 Val Loss = 13.14270
2025-06-04 05:16:04.394247 Epoch 41  	Train Loss = 12.47855 Val Loss = 13.20019
2025-06-04 05:21:57.644620 Epoch 42  	Train Loss = 12.46828 Val Loss = 13.13440
2025-06-04 05:27:50.424097 Epoch 43  	Train Loss = 12.45681 Val Loss = 13.16046
2025-06-04 05:33:43.843724 Epoch 44  	Train Loss = 12.44899 Val Loss = 13.13890
2025-06-04 05:39:36.779066 Epoch 45  	Train Loss = 12.44325 Val Loss = 13.13606
2025-06-04 05:45:29.874864 Epoch 46  	Train Loss = 12.35706 Val Loss = 13.09474
2025-06-04 05:51:23.023599 Epoch 47  	Train Loss = 12.34933 Val Loss = 13.10034
2025-06-04 05:57:16.430490 Epoch 48  	Train Loss = 12.34719 Val Loss = 13.09509
2025-06-04 06:03:09.179878 Epoch 49  	Train Loss = 12.34813 Val Loss = 13.10065
2025-06-04 06:09:02.648090 Epoch 50  	Train Loss = 12.34308 Val Loss = 13.09069
2025-06-04 06:14:56.051871 Epoch 51  	Train Loss = 12.34002 Val Loss = 13.07682
2025-06-04 06:20:48.973235 Epoch 52  	Train Loss = 12.33805 Val Loss = 13.11099
2025-06-04 06:26:42.092124 Epoch 53  	Train Loss = 12.33608 Val Loss = 13.08923
2025-06-04 06:32:34.900107 Epoch 54  	Train Loss = 12.33485 Val Loss = 13.08963
2025-06-04 06:38:28.295183 Epoch 55  	Train Loss = 12.33296 Val Loss = 13.08355
2025-06-04 06:44:20.930518 Epoch 56  	Train Loss = 12.33301 Val Loss = 13.08325
2025-06-04 06:50:14.326648 Epoch 57  	Train Loss = 12.33126 Val Loss = 13.08142
2025-06-04 06:56:07.301538 Epoch 58  	Train Loss = 12.32962 Val Loss = 13.09958
2025-06-04 07:02:00.458133 Epoch 59  	Train Loss = 12.32854 Val Loss = 13.09988
2025-06-04 07:07:52.800803 Epoch 60  	Train Loss = 12.32804 Val Loss = 13.09417
2025-06-04 07:12:57.592253 Epoch 61  	Train Loss = 12.32651 Val Loss = 13.09140
2025-06-04 07:17:50.565107 Epoch 62  	Train Loss = 12.32525 Val Loss = 13.10183
2025-06-04 07:22:43.306322 Epoch 63  	Train Loss = 12.32214 Val Loss = 13.07986
2025-06-04 07:27:36.505955 Epoch 64  	Train Loss = 12.31885 Val Loss = 13.08797
2025-06-04 07:32:28.863405 Epoch 65  	Train Loss = 12.31997 Val Loss = 13.08652
2025-06-04 07:36:39.550447 Epoch 66  	Train Loss = 12.30938 Val Loss = 13.08761
2025-06-04 07:40:33.484988 Epoch 67  	Train Loss = 12.30895 Val Loss = 13.08730
2025-06-04 07:44:27.438790 Epoch 68  	Train Loss = 12.31249 Val Loss = 13.09074
2025-06-04 07:48:21.358766 Epoch 69  	Train Loss = 12.30687 Val Loss = 13.09141
2025-06-04 07:52:15.208252 Epoch 70  	Train Loss = 12.30761 Val Loss = 13.09224
2025-06-04 07:56:09.106136 Epoch 71  	Train Loss = 12.30622 Val Loss = 13.08666
2025-06-04 08:00:03.050861 Epoch 72  	Train Loss = 12.30764 Val Loss = 13.09010
2025-06-04 08:03:56.825380 Epoch 73  	Train Loss = 12.30780 Val Loss = 13.08763
2025-06-04 08:07:50.829455 Epoch 74  	Train Loss = 12.30688 Val Loss = 13.08759
2025-06-04 08:11:44.587612 Epoch 75  	Train Loss = 12.30500 Val Loss = 13.08799
2025-06-04 08:15:38.486023 Epoch 76  	Train Loss = 12.30698 Val Loss = 13.08434
2025-06-04 08:19:31.844791 Epoch 77  	Train Loss = 12.30628 Val Loss = 13.08940
2025-06-04 08:23:25.082314 Epoch 78  	Train Loss = 12.30678 Val Loss = 13.08878
2025-06-04 08:27:18.815265 Epoch 79  	Train Loss = 12.30602 Val Loss = 13.08622
2025-06-04 08:31:12.490672 Epoch 80  	Train Loss = 12.30526 Val Loss = 13.09272
2025-06-04 08:35:06.300629 Epoch 81  	Train Loss = 12.30517 Val Loss = 13.09087
Early stopping at epoch: 81
Best at epoch 51:
Train Loss = 12.34002
Train RMSE = 22.08731, MAE = 12.51334, MAPE = 8.22711
Val Loss = 13.07682
Val RMSE = 23.99380, MAE = 13.50717, MAPE = 10.18599
Saved Model: ../saved_models/STHDformer-PEMS08-2025-06-04-01-14-30.pt
--------- Test ---------
All Steps RMSE = 23.27602, MAE = 13.46652, MAPE = 8.85510
Step 1 RMSE = 19.49102, MAE = 11.75249, MAPE = 7.78856
Step 2 RMSE = 20.57209, MAE = 12.19363, MAPE = 8.02092
Step 3 RMSE = 21.42718, MAE = 12.58147, MAPE = 8.25571
Step 4 RMSE = 22.14722, MAE = 12.90470, MAPE = 8.47532
Step 5 RMSE = 22.73604, MAE = 13.18536, MAPE = 8.65590
Step 6 RMSE = 23.27843, MAE = 13.44541, MAPE = 8.79467
Step 7 RMSE = 23.77657, MAE = 13.69495, MAPE = 8.96626
Step 8 RMSE = 24.21686, MAE = 13.92614, MAPE = 9.11201
Step 9 RMSE = 24.62286, MAE = 14.13386, MAPE = 9.28614
Step 10 RMSE = 24.97883, MAE = 14.33664, MAPE = 9.42947
Step 11 RMSE = 25.35020, MAE = 14.56879, MAPE = 9.62117
Step 12 RMSE = 25.78933, MAE = 14.87489, MAPE = 9.85512
Inference time: 29.06 s
