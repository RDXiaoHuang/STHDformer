PEMS08
Trainset:	x-(10700, 12, 170, 3)	y-(10700, 12, 170, 1)
Valset:  	x-(3567, 12, 170, 3)  	y-(3567, 12, 170, 1)
Testset:	x-(3566, 12, 170, 3)	y-(3566, 12, 170, 1)

Random seed = 1
--------- STHDformer ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0015,
    "milestones": [
        25,
        45,
        65
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 30,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 170,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "steps_per_week": 7,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers_t": 1,
        "num_layers_c": 1,
        "num_layers_s": 1,
        "num_layers_mlp": 2,
        "dropout": 0.1,
        "adaptive_embedding_dim": 80,
        "node_dim": 64,
        "use_temporal_heterogeneity": true,
        "use_spatial_heterogeneity": true,
        "use_temporal_cross": false,
        "use_spatial_cross": true,
        "use_mixed_proj": true
    }
}
==============================================================================================================
Layer (type:depth-idx)                                       Output Shape              Param #
==============================================================================================================
STHDformer                                                   [16, 12, 170, 1]          139,944
├─Linear: 1-1                                                [16, 12, 170, 24]         96
├─Embedding: 1-2                                             [16, 12, 170, 24]         6,912
├─Embedding: 1-3                                             [16, 12, 170, 24]         168
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-1                               [16, 12, 170, 152]        23,256
│    │    └─AttentionLayer: 3-1                              [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-2                                     [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-3                                   [16, 170, 12, 152]        304
│    │    └─Sequential: 3-4                                  [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-5                                     [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-6                                   [16, 170, 12, 152]        304
├─ModuleList: 1-5                                            --                        --
│    └─Temporal_Heterogeneity_SelfAttentionLayer: 2-2        [16, 12, 170, 152]        --
│    │    └─Temporal_Heterogeneity_AttentionLayer: 3-7       [16, 170, 12, 152]        93,072
│    │    └─Dropout: 3-8                                     [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-9                                   [16, 170, 12, 152]        304
│    │    └─Sequential: 3-10                                 [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-11                                    [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-12                                  [16, 170, 12, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-3                         [16, 12, 170, 152]        --
│    │    └─Cross_AttentionLayer: 3-13                       [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-14                                    [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-15                                  [16, 170, 12, 152]        304
│    │    └─Sequential: 3-16                                 [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-17                                    [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-18                                  [16, 170, 12, 152]        304
├─Dual_graph: 1-7                                            [16, 12, 170, 128]        --
│    └─Graph_projection: 2-4                                 [1, 170, 64]              --
│    │    └─Linear: 3-19                                     [1, 170, 64]              10,944
│    │    └─ReLU: 3-20                                       [1, 170, 64]              --
│    │    └─Dropout: 3-21                                    [1, 170, 64]              --
│    │    └─Linear: 3-22                                     [1, 170, 64]              4,160
│    └─Graph_projection: 2-5                                 [1, 170, 64]              --
│    │    └─Linear: 3-23                                     [1, 170, 64]              10,944
│    │    └─ReLU: 3-24                                       [1, 170, 64]              --
│    │    └─Dropout: 3-25                                    [1, 170, 64]              --
│    │    └─Linear: 3-26                                     [1, 170, 64]              4,160
├─Fusion_Model: 1-8                                          [16, 12, 170, 152]        --
│    └─Sequential: 2-6                                       [16, 12, 170, 80]         --
│    │    └─MLP: 3-27                                        [16, 12, 170, 208]        86,944
│    │    └─MLP: 3-28                                        [16, 12, 170, 208]        86,944
│    │    └─Linear: 3-29                                     [16, 12, 170, 80]         16,720
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-7                               [16, 12, 170, 152]        (recursive)
│    │    └─Sequential: 3-30                                 [16, 12, 170, 152]        23,256
│    │    └─AttentionLayer: 3-31                             [16, 12, 170, 152]        (recursive)
│    │    └─Dropout: 3-32                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-33                                  [16, 12, 170, 152]        (recursive)
│    │    └─Sequential: 3-34                                 [16, 12, 170, 152]        (recursive)
│    │    └─Dropout: 3-35                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-36                                  [16, 12, 170, 152]        (recursive)
├─ModuleList: 1-10                                           --                        --
│    └─Spatial_Heterogeneity_SelfAttentionLayer: 2-8         [16, 12, 170, 152]        --
│    │    └─Spatial_Heterogeneity_AttentionLayer: 3-37       [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-38                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-39                                  [16, 12, 170, 152]        304
│    │    └─Sequential: 3-40                                 [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-41                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-42                                  [16, 12, 170, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-9                         [16, 12, 170, 152]        (recursive)
│    │    └─Cross_AttentionLayer: 3-43                       [16, 12, 170, 152]        (recursive)
│    │    └─Dropout: 3-44                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-45                                  [16, 12, 170, 152]        (recursive)
│    │    └─Sequential: 3-46                                 [16, 12, 170, 152]        (recursive)
│    │    └─Dropout: 3-47                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-48                                  [16, 12, 170, 152]        (recursive)
├─Linear: 1-12                                               [16, 170, 12]             21,900
==============================================================================================================
Total params: 1,123,852
Trainable params: 1,123,852
Non-trainable params: 0
Total mult-adds (M): 20.42
==============================================================================================================
Input size (MB): 0.39
Forward/backward pass size (MB): 2365.32
Params size (MB): 3.84
Estimated Total Size (MB): 2369.56
==============================================================================================================

Loss: HuberLoss

2025-06-04 01:20:04.891337 Epoch 1  	Train Loss = 25.69667 Val Loss = 18.42505
2025-06-04 01:25:58.570757 Epoch 2  	Train Loss = 18.40644 Val Loss = 17.15708
2025-06-04 01:31:52.697945 Epoch 3  	Train Loss = 17.13184 Val Loss = 16.60676
2025-06-04 01:37:46.529549 Epoch 4  	Train Loss = 16.64641 Val Loss = 16.10624
2025-06-04 01:43:40.380942 Epoch 5  	Train Loss = 16.14398 Val Loss = 15.66364
2025-06-04 01:49:34.294833 Epoch 6  	Train Loss = 15.70442 Val Loss = 15.82977
2025-06-04 01:55:27.864668 Epoch 7  	Train Loss = 15.76863 Val Loss = 15.30176
2025-06-04 02:01:21.848831 Epoch 8  	Train Loss = 15.29696 Val Loss = 14.90444
2025-06-04 02:07:15.113227 Epoch 9  	Train Loss = 15.28032 Val Loss = 15.05923
2025-06-04 02:13:09.048326 Epoch 10  	Train Loss = 15.13466 Val Loss = 15.16467
2025-06-04 02:19:02.303422 Epoch 11  	Train Loss = 15.04847 Val Loss = 14.88970
2025-06-04 02:24:55.747823 Epoch 12  	Train Loss = 14.89470 Val Loss = 14.84525
2025-06-04 02:30:49.369552 Epoch 13  	Train Loss = 14.90033 Val Loss = 15.18045
2025-06-04 02:36:43.155869 Epoch 14  	Train Loss = 14.73821 Val Loss = 16.18490
2025-06-04 02:42:37.068745 Epoch 15  	Train Loss = 14.63960 Val Loss = 14.50263
2025-06-04 02:48:30.165024 Epoch 16  	Train Loss = 14.49302 Val Loss = 14.22252
2025-06-04 02:54:24.068181 Epoch 17  	Train Loss = 14.45326 Val Loss = 14.26798
2025-06-04 03:00:17.622276 Epoch 18  	Train Loss = 14.32099 Val Loss = 14.04158
2025-06-04 03:06:11.217535 Epoch 19  	Train Loss = 14.30427 Val Loss = 14.29392
2025-06-04 03:12:04.695515 Epoch 20  	Train Loss = 14.16481 Val Loss = 14.20294
2025-06-04 03:17:58.598526 Epoch 21  	Train Loss = 14.00054 Val Loss = 14.21731
2025-06-04 03:23:52.215650 Epoch 22  	Train Loss = 14.07706 Val Loss = 14.44969
2025-06-04 03:29:45.652342 Epoch 23  	Train Loss = 13.92551 Val Loss = 14.07483
2025-06-04 03:35:39.522878 Epoch 24  	Train Loss = 13.85823 Val Loss = 14.38733
2025-06-04 03:41:33.016964 Epoch 25  	Train Loss = 13.87319 Val Loss = 13.83698
2025-06-04 03:47:26.479818 Epoch 26  	Train Loss = 13.08102 Val Loss = 13.25054
2025-06-04 03:53:19.627403 Epoch 27  	Train Loss = 13.00038 Val Loss = 13.23937
2025-06-04 03:59:12.960798 Epoch 28  	Train Loss = 12.96744 Val Loss = 13.22272
2025-06-04 04:05:05.485907 Epoch 29  	Train Loss = 12.94937 Val Loss = 13.28275
2025-06-04 04:10:58.839541 Epoch 30  	Train Loss = 12.93066 Val Loss = 13.20540
2025-06-04 04:16:52.025704 Epoch 31  	Train Loss = 12.90618 Val Loss = 13.19032
2025-06-04 04:22:45.268026 Epoch 32  	Train Loss = 12.89555 Val Loss = 13.21960
2025-06-04 04:28:38.480934 Epoch 33  	Train Loss = 12.87935 Val Loss = 13.16746
2025-06-04 04:34:31.594935 Epoch 34  	Train Loss = 12.86257 Val Loss = 13.19809
2025-06-04 04:40:25.129472 Epoch 35  	Train Loss = 12.85312 Val Loss = 13.29794
2025-06-04 04:46:17.863130 Epoch 36  	Train Loss = 12.84501 Val Loss = 13.15120
2025-06-04 04:52:11.476054 Epoch 37  	Train Loss = 12.82691 Val Loss = 13.22845
2025-06-04 04:58:04.723030 Epoch 38  	Train Loss = 12.81232 Val Loss = 13.16418
2025-06-04 05:03:58.029426 Epoch 39  	Train Loss = 12.80456 Val Loss = 13.19838
2025-06-04 05:09:51.308029 Epoch 40  	Train Loss = 12.80067 Val Loss = 13.18826
2025-06-04 05:15:44.625366 Epoch 41  	Train Loss = 12.78159 Val Loss = 13.15450
2025-06-04 05:21:38.081890 Epoch 42  	Train Loss = 12.77698 Val Loss = 13.16504
2025-06-04 05:27:30.662537 Epoch 43  	Train Loss = 12.76277 Val Loss = 13.21964
2025-06-04 05:33:24.272873 Epoch 44  	Train Loss = 12.76072 Val Loss = 13.13257
2025-06-04 05:39:17.255997 Epoch 45  	Train Loss = 12.75398 Val Loss = 13.13590
2025-06-04 05:45:10.217456 Epoch 46  	Train Loss = 12.66372 Val Loss = 13.11780
2025-06-04 05:51:03.241759 Epoch 47  	Train Loss = 12.64840 Val Loss = 13.08737
2025-06-04 05:56:56.648410 Epoch 48  	Train Loss = 12.65010 Val Loss = 13.09960
2025-06-04 06:02:49.806758 Epoch 49  	Train Loss = 12.64587 Val Loss = 13.09048
2025-06-04 06:08:42.862847 Epoch 50  	Train Loss = 12.64403 Val Loss = 13.09787
2025-06-04 06:14:36.296665 Epoch 51  	Train Loss = 12.64046 Val Loss = 13.08819
2025-06-04 06:20:29.288793 Epoch 52  	Train Loss = 12.63794 Val Loss = 13.10019
2025-06-04 06:26:22.356585 Epoch 53  	Train Loss = 12.63728 Val Loss = 13.08534
2025-06-04 06:32:15.175846 Epoch 54  	Train Loss = 12.63492 Val Loss = 13.09584
2025-06-04 06:38:08.518040 Epoch 55  	Train Loss = 12.63334 Val Loss = 13.10895
2025-06-04 06:44:01.169045 Epoch 56  	Train Loss = 12.63003 Val Loss = 13.09234
2025-06-04 06:49:54.591363 Epoch 57  	Train Loss = 12.63014 Val Loss = 13.08403
2025-06-04 06:55:47.838881 Epoch 58  	Train Loss = 12.62746 Val Loss = 13.11409
2025-06-04 07:01:40.920217 Epoch 59  	Train Loss = 12.62489 Val Loss = 13.09973
2025-06-04 07:07:33.290557 Epoch 60  	Train Loss = 12.62634 Val Loss = 13.10588
2025-06-04 07:12:41.445693 Epoch 61  	Train Loss = 12.61953 Val Loss = 13.08301
2025-06-04 07:17:34.531265 Epoch 62  	Train Loss = 12.62216 Val Loss = 13.09765
2025-06-04 07:22:27.130541 Epoch 63  	Train Loss = 12.61774 Val Loss = 13.09326
2025-06-04 07:27:20.221560 Epoch 64  	Train Loss = 12.61713 Val Loss = 13.07994
2025-06-04 07:32:12.809273 Epoch 65  	Train Loss = 12.61375 Val Loss = 13.09718
2025-06-04 07:36:26.645655 Epoch 66  	Train Loss = 12.60278 Val Loss = 13.08878
2025-06-04 07:40:20.474051 Epoch 67  	Train Loss = 12.60508 Val Loss = 13.09320
2025-06-04 07:44:14.413269 Epoch 68  	Train Loss = 12.60332 Val Loss = 13.09055
2025-06-04 07:48:08.331014 Epoch 69  	Train Loss = 12.60364 Val Loss = 13.09120
2025-06-04 07:52:02.212372 Epoch 70  	Train Loss = 12.60190 Val Loss = 13.09737
2025-06-04 07:55:56.120891 Epoch 71  	Train Loss = 12.60511 Val Loss = 13.08462
2025-06-04 07:59:50.026999 Epoch 72  	Train Loss = 12.60336 Val Loss = 13.09164
2025-06-04 08:03:43.799582 Epoch 73  	Train Loss = 12.60290 Val Loss = 13.08630
2025-06-04 08:07:37.779464 Epoch 74  	Train Loss = 12.60149 Val Loss = 13.09008
2025-06-04 08:11:31.565543 Epoch 75  	Train Loss = 12.60296 Val Loss = 13.09138
2025-06-04 08:15:25.442233 Epoch 76  	Train Loss = 12.60379 Val Loss = 13.08466
2025-06-04 08:19:18.966376 Epoch 77  	Train Loss = 12.60214 Val Loss = 13.09074
2025-06-04 08:23:12.091135 Epoch 78  	Train Loss = 12.60171 Val Loss = 13.08804
2025-06-04 08:27:05.422004 Epoch 79  	Train Loss = 12.60373 Val Loss = 13.08536
2025-06-04 08:30:58.930609 Epoch 80  	Train Loss = 12.60155 Val Loss = 13.08906
2025-06-04 08:34:52.613286 Epoch 81  	Train Loss = 12.60153 Val Loss = 13.09058
2025-06-04 08:38:25.049558 Epoch 82  	Train Loss = 12.60143 Val Loss = 13.09084
2025-06-04 08:41:20.088989 Epoch 83  	Train Loss = 12.59821 Val Loss = 13.08565
2025-06-04 08:43:56.415555 Epoch 84  	Train Loss = 12.59872 Val Loss = 13.09065
2025-06-04 08:45:52.917720 Epoch 85  	Train Loss = 12.59995 Val Loss = 13.08512
2025-06-04 08:47:49.538379 Epoch 86  	Train Loss = 12.60124 Val Loss = 13.08577
2025-06-04 08:49:46.062716 Epoch 87  	Train Loss = 12.60204 Val Loss = 13.09169
2025-06-04 08:51:39.135352 Epoch 88  	Train Loss = 12.59959 Val Loss = 13.08804
2025-06-04 08:52:41.485912 Epoch 89  	Train Loss = 12.60203 Val Loss = 13.08838
2025-06-04 08:53:40.119092 Epoch 90  	Train Loss = 12.60045 Val Loss = 13.08782
2025-06-04 08:54:39.049909 Epoch 91  	Train Loss = 12.59897 Val Loss = 13.08614
2025-06-04 08:55:37.626820 Epoch 92  	Train Loss = 12.59976 Val Loss = 13.08747
2025-06-04 08:56:36.288256 Epoch 93  	Train Loss = 12.59879 Val Loss = 13.08456
2025-06-04 08:57:34.990701 Epoch 94  	Train Loss = 12.60061 Val Loss = 13.08571
Early stopping at epoch: 94
Best at epoch 64:
Train Loss = 12.61713
Train RMSE = 22.46733, MAE = 12.72991, MAPE = 8.34875
Val Loss = 13.07994
Val RMSE = 23.95916, MAE = 13.51083, MAPE = 10.29567
Saved Model: ../saved_models/STHDformer-PEMS08-2025-06-04-01-14-17.pt
--------- Test ---------
All Steps RMSE = 23.00870, MAE = 13.34142, MAPE = 8.75258
Step 1 RMSE = 19.38958, MAE = 11.66066, MAPE = 7.66881
Step 2 RMSE = 20.47191, MAE = 12.10607, MAPE = 7.93321
Step 3 RMSE = 21.29718, MAE = 12.49256, MAPE = 8.16754
Step 4 RMSE = 21.97732, MAE = 12.80064, MAPE = 8.36883
Step 5 RMSE = 22.53689, MAE = 13.07214, MAPE = 8.54819
Step 6 RMSE = 23.01758, MAE = 13.31552, MAPE = 8.72312
Step 7 RMSE = 23.47690, MAE = 13.55480, MAPE = 8.89115
Step 8 RMSE = 23.89018, MAE = 13.77985, MAPE = 9.03266
Step 9 RMSE = 24.24731, MAE = 13.98078, MAPE = 9.16961
Step 10 RMSE = 24.60357, MAE = 14.18843, MAPE = 9.32820
Step 11 RMSE = 24.96544, MAE = 14.42025, MAPE = 9.48765
Step 12 RMSE = 25.39710, MAE = 14.72535, MAPE = 9.71214
Inference time: 7.19 s
