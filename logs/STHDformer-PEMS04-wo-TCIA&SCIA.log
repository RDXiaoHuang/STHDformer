PEMS04
Trainset:	x-(10181, 12, 307, 3)	y-(10181, 12, 307, 1)
Valset:  	x-(3394, 12, 307, 3)  	y-(3394, 12, 307, 1)
Testset:	x-(3394, 12, 307, 3)	y-(3394, 12, 307, 1)

Random seed = 1
--------- STHDformer ---------
{
    "num_nodes": 307,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0005,
    "milestones": [
        15,
        30,
        50
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 307,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "steps_per_week": 7,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers_t": 1,
        "num_layers_c": 1,
        "num_layers_s": 1,
        "num_layers_mlp": 2,
        "dropout": 0.1,
        "adaptive_embedding_dim": 80,
        "node_dim": 64,
        "use_temporal_heterogeneity": true,
        "use_spatial_heterogeneity": true,
        "use_temporal_cross": false,
        "use_spatial_cross": false,
        "use_mixed_proj": true
    }
}
==============================================================================================================
Layer (type:depth-idx)                                       Output Shape              Param #
==============================================================================================================
STHDformer                                                   [16, 12, 307, 1]          271,464
├─Linear: 1-1                                                [16, 12, 307, 24]         96
├─Embedding: 1-2                                             [16, 12, 307, 24]         6,912
├─Embedding: 1-3                                             [16, 12, 307, 24]         168
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-1                               [16, 12, 307, 152]        23,256
│    │    └─AttentionLayer: 3-1                              [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-2                                     [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-3                                   [16, 307, 12, 152]        304
│    │    └─Sequential: 3-4                                  [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-5                                     [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-6                                   [16, 307, 12, 152]        304
├─ModuleList: 1-5                                            --                        --
│    └─Temporal_Heterogeneity_SelfAttentionLayer: 2-2        [16, 12, 307, 152]        --
│    │    └─Temporal_Heterogeneity_AttentionLayer: 3-7       [16, 307, 12, 152]        93,072
│    │    └─Dropout: 3-8                                     [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-9                                   [16, 307, 12, 152]        304
│    │    └─Sequential: 3-10                                 [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-11                                    [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-12                                  [16, 307, 12, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-3                         [16, 12, 307, 152]        --
│    │    └─Cross_AttentionLayer: 3-13                       [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-14                                    [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-15                                  [16, 307, 12, 152]        304
│    │    └─Sequential: 3-16                                 [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-17                                    [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-18                                  [16, 307, 12, 152]        304
├─Dual_graph: 1-7                                            [16, 12, 307, 128]        --
│    └─Graph_projection: 2-4                                 [1, 307, 64]              --
│    │    └─Linear: 3-19                                     [1, 307, 64]              19,712
│    │    └─ReLU: 3-20                                       [1, 307, 64]              --
│    │    └─Dropout: 3-21                                    [1, 307, 64]              --
│    │    └─Linear: 3-22                                     [1, 307, 64]              4,160
│    └─Graph_projection: 2-5                                 [1, 307, 64]              --
│    │    └─Linear: 3-23                                     [1, 307, 64]              19,712
│    │    └─ReLU: 3-24                                       [1, 307, 64]              --
│    │    └─Dropout: 3-25                                    [1, 307, 64]              --
│    │    └─Linear: 3-26                                     [1, 307, 64]              4,160
├─Fusion_Model: 1-8                                          [16, 12, 307, 152]        --
│    └─Sequential: 2-6                                       [16, 12, 307, 80]         --
│    │    └─MLP: 3-27                                        [16, 12, 307, 208]        86,944
│    │    └─MLP: 3-28                                        [16, 12, 307, 208]        86,944
│    │    └─Linear: 3-29                                     [16, 12, 307, 80]         16,720
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-7                               [16, 12, 307, 152]        (recursive)
│    │    └─Sequential: 3-30                                 [16, 12, 307, 152]        23,256
│    │    └─AttentionLayer: 3-31                             [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-32                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-33                                  [16, 12, 307, 152]        (recursive)
│    │    └─Sequential: 3-34                                 [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-35                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-36                                  [16, 12, 307, 152]        (recursive)
├─ModuleList: 1-10                                           --                        --
│    └─Spatial_Heterogeneity_SelfAttentionLayer: 2-8         [16, 12, 307, 152]        --
│    │    └─Spatial_Heterogeneity_AttentionLayer: 3-37       [16, 12, 307, 152]        93,024
│    │    └─Dropout: 3-38                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-39                                  [16, 12, 307, 152]        304
│    │    └─Sequential: 3-40                                 [16, 12, 307, 152]        78,232
│    │    └─Dropout: 3-41                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-42                                  [16, 12, 307, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-9                         [16, 12, 307, 152]        (recursive)
│    │    └─Cross_AttentionLayer: 3-43                       [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-44                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-45                                  [16, 12, 307, 152]        (recursive)
│    │    └─Sequential: 3-46                                 [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-47                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-48                                  [16, 12, 307, 152]        (recursive)
├─Linear: 1-12                                               [16, 307, 12]             21,900
==============================================================================================================
Total params: 1,272,908
Trainable params: 1,272,908
Non-trainable params: 0
Total mult-adds (M): 20.43
==============================================================================================================
Input size (MB): 0.71
Forward/backward pass size (MB): 4271.48
Params size (MB): 3.91
Estimated Total Size (MB): 4276.10
==============================================================================================================

Loss: HuberLoss

2025-06-04 18:10:00.259233 Epoch 1  	Train Loss = 29.12833 Val Loss = 23.36822
2025-06-04 18:15:21.310986 Epoch 2  	Train Loss = 21.79315 Val Loss = 20.82698
2025-06-04 18:20:41.169853 Epoch 3  	Train Loss = 20.59967 Val Loss = 20.46763
2025-06-04 18:26:01.057361 Epoch 4  	Train Loss = 20.04535 Val Loss = 19.60904
2025-06-04 18:31:21.129654 Epoch 5  	Train Loss = 19.51491 Val Loss = 19.69437
2025-06-04 18:36:41.464654 Epoch 6  	Train Loss = 19.00324 Val Loss = 19.16544
2025-06-04 18:42:01.807301 Epoch 7  	Train Loss = 19.03998 Val Loss = 19.61072
2025-06-04 18:47:21.533860 Epoch 8  	Train Loss = 18.67797 Val Loss = 19.41314
2025-06-04 18:52:41.299933 Epoch 9  	Train Loss = 18.54014 Val Loss = 20.24084
2025-06-04 18:58:01.300503 Epoch 10  	Train Loss = 18.50040 Val Loss = 18.57410
2025-06-04 19:03:21.571367 Epoch 11  	Train Loss = 18.36767 Val Loss = 23.41467
2025-06-04 19:08:41.290455 Epoch 12  	Train Loss = 18.25864 Val Loss = 18.92027
2025-06-04 19:14:01.222480 Epoch 13  	Train Loss = 17.97283 Val Loss = 18.62547
2025-06-04 19:19:21.190614 Epoch 14  	Train Loss = 17.92456 Val Loss = 18.90524
2025-06-04 19:24:41.085243 Epoch 15  	Train Loss = 18.04125 Val Loss = 19.32877
2025-06-04 19:30:01.159846 Epoch 16  	Train Loss = 17.09064 Val Loss = 17.80014
2025-06-04 19:35:21.214945 Epoch 17  	Train Loss = 16.93956 Val Loss = 17.80538
2025-06-04 19:40:40.674395 Epoch 18  	Train Loss = 16.88272 Val Loss = 17.76080
2025-06-04 19:46:00.473739 Epoch 19  	Train Loss = 16.86082 Val Loss = 17.81070
2025-06-04 19:51:19.883242 Epoch 20  	Train Loss = 16.83023 Val Loss = 17.78891
2025-06-04 19:56:39.439400 Epoch 21  	Train Loss = 16.80809 Val Loss = 17.76129
2025-06-04 20:01:58.828642 Epoch 22  	Train Loss = 16.78080 Val Loss = 17.86843
2025-06-04 20:07:18.288750 Epoch 23  	Train Loss = 16.75204 Val Loss = 17.73392
2025-06-04 20:12:37.755344 Epoch 24  	Train Loss = 16.72313 Val Loss = 17.90871
2025-06-04 20:17:56.941210 Epoch 25  	Train Loss = 16.70831 Val Loss = 17.78697
2025-06-04 20:23:16.191282 Epoch 26  	Train Loss = 16.68575 Val Loss = 17.70319
2025-06-04 20:28:35.451149 Epoch 27  	Train Loss = 16.65761 Val Loss = 17.75336
2025-06-04 20:33:54.710128 Epoch 28  	Train Loss = 16.64184 Val Loss = 17.71570
2025-06-04 20:39:14.312329 Epoch 29  	Train Loss = 16.62817 Val Loss = 17.77315
2025-06-04 20:44:33.649449 Epoch 30  	Train Loss = 16.60315 Val Loss = 17.69775
2025-06-04 20:49:52.940897 Epoch 31  	Train Loss = 16.50759 Val Loss = 17.63860
2025-06-04 20:55:12.029861 Epoch 32  	Train Loss = 16.49627 Val Loss = 17.61835
2025-06-04 21:00:31.241884 Epoch 33  	Train Loss = 16.49745 Val Loss = 17.64130
2025-06-04 21:05:50.545715 Epoch 34  	Train Loss = 16.49444 Val Loss = 17.64078
2025-06-04 21:11:09.842099 Epoch 35  	Train Loss = 16.49099 Val Loss = 17.62974
2025-06-04 21:16:28.971552 Epoch 36  	Train Loss = 16.48984 Val Loss = 17.62875
2025-06-04 21:21:48.094311 Epoch 37  	Train Loss = 16.48093 Val Loss = 17.64144
2025-06-04 21:27:07.327282 Epoch 38  	Train Loss = 16.48312 Val Loss = 17.64612
2025-06-04 21:32:26.556596 Epoch 39  	Train Loss = 16.47459 Val Loss = 17.64017
2025-06-04 21:37:46.143367 Epoch 40  	Train Loss = 16.47603 Val Loss = 17.64447
2025-06-04 21:43:05.099876 Epoch 41  	Train Loss = 16.47575 Val Loss = 17.63404
2025-06-04 21:48:24.080219 Epoch 42  	Train Loss = 16.46963 Val Loss = 17.62317
2025-06-04 21:53:43.240895 Epoch 43  	Train Loss = 16.46468 Val Loss = 17.63331
2025-06-04 21:59:02.170411 Epoch 44  	Train Loss = 16.46389 Val Loss = 17.63396
2025-06-04 22:04:21.232444 Epoch 45  	Train Loss = 16.46461 Val Loss = 17.62835
2025-06-04 22:09:40.509754 Epoch 46  	Train Loss = 16.45505 Val Loss = 17.64084
2025-06-04 22:14:59.598012 Epoch 47  	Train Loss = 16.45248 Val Loss = 17.62822
2025-06-04 22:20:18.940265 Epoch 48  	Train Loss = 16.45590 Val Loss = 17.63888
2025-06-04 22:25:38.393857 Epoch 49  	Train Loss = 16.44645 Val Loss = 17.64135
2025-06-04 22:30:57.510768 Epoch 50  	Train Loss = 16.44671 Val Loss = 17.63454
2025-06-04 22:36:16.922010 Epoch 51  	Train Loss = 16.43996 Val Loss = 17.62829
2025-06-04 22:41:36.368971 Epoch 52  	Train Loss = 16.43936 Val Loss = 17.62293
Early stopping at epoch: 52
Best at epoch 32:
Train Loss = 16.49627
Train RMSE = 28.06794, MAE = 16.83926, MAPE = 12.10000
Val Loss = 17.61835
Val RMSE = 30.57292, MAE = 18.31301, MAPE = 11.80753
Saved Model: ../saved_models/STHDformer-PEMS04-2025-06-04-18-04-49.pt
--------- Test ---------
All Steps RMSE = 29.92114, MAE = 18.21703, MAPE = 11.95852
Step 1 RMSE = 27.02213, MAE = 16.68063, MAPE = 11.08429
Step 2 RMSE = 27.90249, MAE = 17.10045, MAPE = 11.35787
Step 3 RMSE = 28.58591, MAE = 17.47373, MAPE = 11.56270
Step 4 RMSE = 29.12685, MAE = 17.75831, MAPE = 11.71737
Step 5 RMSE = 29.57207, MAE = 18.00261, MAPE = 11.82683
Step 6 RMSE = 29.95123, MAE = 18.21603, MAPE = 11.92452
Step 7 RMSE = 30.31142, MAE = 18.42113, MAPE = 12.05051
Step 8 RMSE = 30.64178, MAE = 18.60900, MAPE = 12.15786
Step 9 RMSE = 30.93772, MAE = 18.79714, MAPE = 12.25607
Step 10 RMSE = 31.22086, MAE = 18.96920, MAPE = 12.38253
Step 11 RMSE = 31.50166, MAE = 19.16229, MAPE = 12.50805
Step 12 RMSE = 31.86619, MAE = 19.41354, MAPE = 12.67339
Inference time: 38.48 s
