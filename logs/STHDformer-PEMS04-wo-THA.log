PEMS04
Trainset:	x-(10181, 12, 307, 3)	y-(10181, 12, 307, 1)
Valset:  	x-(3394, 12, 307, 3)  	y-(3394, 12, 307, 1)
Testset:	x-(3394, 12, 307, 3)	y-(3394, 12, 307, 1)

Random seed = 1
--------- STHDformer ---------
{
    "num_nodes": 307,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0005,
    "milestones": [
        15,
        30,
        50
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 307,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "steps_per_week": 7,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers_t": 1,
        "num_layers_c": 1,
        "num_layers_s": 1,
        "num_layers_mlp": 2,
        "dropout": 0.1,
        "adaptive_embedding_dim": 80,
        "node_dim": 64,
        "use_temporal_heterogeneity": false,
        "use_spatial_heterogeneity": true,
        "use_temporal_cross": true,
        "use_spatial_cross": true,
        "use_mixed_proj": true
    }
}
==============================================================================================================
Layer (type:depth-idx)                                       Output Shape              Param #
==============================================================================================================
STHDformer                                                   [16, 12, 307, 1]          271,464
├─Linear: 1-1                                                [16, 12, 307, 24]         96
├─Embedding: 1-2                                             [16, 12, 307, 24]         6,912
├─Embedding: 1-3                                             [16, 12, 307, 24]         168
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-1                               [16, 12, 307, 152]        23,256
│    │    └─AttentionLayer: 3-1                              [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-2                                     [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-3                                   [16, 307, 12, 152]        304
│    │    └─Sequential: 3-4                                  [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-5                                     [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-6                                   [16, 307, 12, 152]        304
├─ModuleList: 1-5                                            --                        --
│    └─TemporalTCNLayer: 2-2                                 [16, 12, 307, 152]        --
│    │    └─Conv1d: 3-7                                      [4912, 152, 12]           69,464
│    │    └─ReLU: 3-8                                        [4912, 152, 12]           --
│    │    └─Dropout: 3-9                                     [4912, 152, 12]           --
│    │    └─LayerNorm: 3-10                                  [4912, 12, 152]           304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-3                         [16, 12, 307, 152]        --
│    │    └─Cross_AttentionLayer: 3-11                       [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-12                                    [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-13                                  [16, 307, 12, 152]        304
│    │    └─Sequential: 3-14                                 [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-15                                    [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-16                                  [16, 307, 12, 152]        304
├─Dual_graph: 1-7                                            [16, 12, 307, 128]        --
│    └─Graph_projection: 2-4                                 [1, 307, 64]              --
│    │    └─Linear: 3-17                                     [1, 307, 64]              19,712
│    │    └─ReLU: 3-18                                       [1, 307, 64]              --
│    │    └─Dropout: 3-19                                    [1, 307, 64]              --
│    │    └─Linear: 3-20                                     [1, 307, 64]              4,160
│    └─Graph_projection: 2-5                                 [1, 307, 64]              --
│    │    └─Linear: 3-21                                     [1, 307, 64]              19,712
│    │    └─ReLU: 3-22                                       [1, 307, 64]              --
│    │    └─Dropout: 3-23                                    [1, 307, 64]              --
│    │    └─Linear: 3-24                                     [1, 307, 64]              4,160
├─Fusion_Model: 1-8                                          [16, 12, 307, 152]        --
│    └─Sequential: 2-6                                       [16, 12, 307, 80]         --
│    │    └─MLP: 3-25                                        [16, 12, 307, 208]        86,944
│    │    └─MLP: 3-26                                        [16, 12, 307, 208]        86,944
│    │    └─Linear: 3-27                                     [16, 12, 307, 80]         16,720
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-7                               [16, 12, 307, 152]        (recursive)
│    │    └─Sequential: 3-28                                 [16, 12, 307, 152]        23,256
│    │    └─AttentionLayer: 3-29                             [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-30                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-31                                  [16, 12, 307, 152]        (recursive)
│    │    └─Sequential: 3-32                                 [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-33                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-34                                  [16, 12, 307, 152]        (recursive)
├─ModuleList: 1-10                                           --                        --
│    └─Spatial_Heterogeneity_SelfAttentionLayer: 2-8         [16, 12, 307, 152]        --
│    │    └─Spatial_Heterogeneity_AttentionLayer: 3-35       [16, 12, 307, 152]        93,024
│    │    └─Dropout: 3-36                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-37                                  [16, 12, 307, 152]        304
│    │    └─Sequential: 3-38                                 [16, 12, 307, 152]        78,232
│    │    └─Dropout: 3-39                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-40                                  [16, 12, 307, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-9                         [16, 12, 307, 152]        (recursive)
│    │    └─Cross_AttentionLayer: 3-41                       [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-42                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-43                                  [16, 12, 307, 152]        (recursive)
│    │    └─Sequential: 3-44                                 [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-45                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-46                                  [16, 12, 307, 152]        (recursive)
├─Linear: 1-12                                               [16, 307, 12]             21,900
==============================================================================================================
Total params: 1,170,764
Trainable params: 1,170,764
Non-trainable params: 0
Total mult-adds (G): 4.11
==============================================================================================================
Input size (MB): 0.71
Forward/backward pass size (MB): 3792.38
Params size (MB): 3.50
Estimated Total Size (MB): 3796.59
==============================================================================================================

Loss: HuberLoss

2025-06-27 03:09:31.866438 Epoch 1  	Train Loss = 30.75435 Val Loss = 23.22182
2025-06-27 03:16:16.837431 Epoch 2  	Train Loss = 22.52543 Val Loss = 25.77872
2025-06-27 03:22:59.634335 Epoch 3  	Train Loss = 21.13472 Val Loss = 20.51177
2025-06-27 03:29:43.989332 Epoch 4  	Train Loss = 20.12155 Val Loss = 20.90417
2025-06-27 03:36:28.027759 Epoch 5  	Train Loss = 19.59964 Val Loss = 20.09491
2025-06-27 03:43:10.983925 Epoch 6  	Train Loss = 19.33128 Val Loss = 21.31054
2025-06-27 03:49:55.467565 Epoch 7  	Train Loss = 19.10351 Val Loss = 21.33338
2025-06-27 03:56:39.244190 Epoch 8  	Train Loss = 18.63633 Val Loss = 19.53662
2025-06-27 04:03:22.522119 Epoch 9  	Train Loss = 18.77210 Val Loss = 19.20855
2025-06-27 04:10:06.781830 Epoch 10  	Train Loss = 18.44695 Val Loss = 18.84033
2025-06-27 04:16:50.369270 Epoch 11  	Train Loss = 18.24296 Val Loss = 19.69073
2025-06-27 04:23:34.429364 Epoch 12  	Train Loss = 18.21552 Val Loss = 19.10788
2025-06-27 04:30:18.394725 Epoch 13  	Train Loss = 18.08355 Val Loss = 18.78122
2025-06-27 04:37:01.676444 Epoch 14  	Train Loss = 18.02078 Val Loss = 18.44559
2025-06-27 04:42:56.078560 Epoch 15  	Train Loss = 17.92593 Val Loss = 18.93371
2025-06-27 04:48:34.386298 Epoch 16  	Train Loss = 17.05666 Val Loss = 17.79618
2025-06-27 04:55:35.989850 Epoch 17  	Train Loss = 16.94354 Val Loss = 17.75925
2025-06-27 05:02:36.875454 Epoch 18  	Train Loss = 16.89608 Val Loss = 17.78947
2025-06-27 05:09:37.763497 Epoch 19  	Train Loss = 16.86291 Val Loss = 17.73699
2025-06-27 05:16:36.634439 Epoch 20  	Train Loss = 16.84593 Val Loss = 17.78281
2025-06-27 05:23:34.093540 Epoch 21  	Train Loss = 16.81625 Val Loss = 17.75740
2025-06-27 05:30:32.256866 Epoch 22  	Train Loss = 16.80421 Val Loss = 17.73877
2025-06-27 05:37:29.809080 Epoch 23  	Train Loss = 16.76377 Val Loss = 17.70684
2025-06-27 05:44:28.173409 Epoch 24  	Train Loss = 16.73774 Val Loss = 17.71366
2025-06-27 05:51:28.112506 Epoch 25  	Train Loss = 16.72451 Val Loss = 17.68121
2025-06-27 05:58:28.461179 Epoch 26  	Train Loss = 16.70809 Val Loss = 17.67807
2025-06-27 06:05:28.625409 Epoch 27  	Train Loss = 16.69029 Val Loss = 17.72523
2025-06-27 06:12:28.858780 Epoch 28  	Train Loss = 16.66885 Val Loss = 17.69217
2025-06-27 06:19:28.829060 Epoch 29  	Train Loss = 16.64925 Val Loss = 17.73543
2025-06-27 06:26:29.111024 Epoch 30  	Train Loss = 16.63090 Val Loss = 17.69733
2025-06-27 06:33:29.313064 Epoch 31  	Train Loss = 16.53285 Val Loss = 17.60224
2025-06-27 06:40:29.624707 Epoch 32  	Train Loss = 16.51694 Val Loss = 17.59201
2025-06-27 06:47:30.537365 Epoch 33  	Train Loss = 16.51774 Val Loss = 17.60382
2025-06-27 06:53:34.165475 Epoch 34  	Train Loss = 16.51224 Val Loss = 17.59762
2025-06-27 06:59:11.449853 Epoch 35  	Train Loss = 16.51518 Val Loss = 17.61659
2025-06-27 07:04:48.250274 Epoch 36  	Train Loss = 16.50981 Val Loss = 17.59011
2025-06-27 07:10:25.636069 Epoch 37  	Train Loss = 16.50393 Val Loss = 17.58382
2025-06-27 07:16:01.989476 Epoch 38  	Train Loss = 16.50168 Val Loss = 17.60785
2025-06-27 07:21:38.904204 Epoch 39  	Train Loss = 16.50218 Val Loss = 17.59639
2025-06-27 07:27:15.353535 Epoch 40  	Train Loss = 16.49083 Val Loss = 17.60364
2025-06-27 07:32:52.518596 Epoch 41  	Train Loss = 16.49865 Val Loss = 17.58593
2025-06-27 07:38:29.032139 Epoch 42  	Train Loss = 16.49496 Val Loss = 17.61182
2025-06-27 07:44:06.136031 Epoch 43  	Train Loss = 16.48375 Val Loss = 17.58553
2025-06-27 07:49:43.438169 Epoch 44  	Train Loss = 16.48545 Val Loss = 17.60940
2025-06-27 07:55:20.183826 Epoch 45  	Train Loss = 16.47686 Val Loss = 17.59890
2025-06-27 08:00:57.222982 Epoch 46  	Train Loss = 16.47909 Val Loss = 17.58724
2025-06-27 08:06:33.625925 Epoch 47  	Train Loss = 16.48283 Val Loss = 17.58344
2025-06-27 08:12:10.404858 Epoch 48  	Train Loss = 16.47724 Val Loss = 17.59327
2025-06-27 08:17:46.506142 Epoch 49  	Train Loss = 16.47519 Val Loss = 17.58498
2025-06-27 08:23:23.575004 Epoch 50  	Train Loss = 16.46554 Val Loss = 17.58528
2025-06-27 08:28:59.843795 Epoch 51  	Train Loss = 16.45744 Val Loss = 17.58178
2025-06-27 08:34:36.391149 Epoch 52  	Train Loss = 16.45748 Val Loss = 17.58124
2025-06-27 08:40:13.305367 Epoch 53  	Train Loss = 16.45454 Val Loss = 17.58132
2025-06-27 08:45:50.764557 Epoch 54  	Train Loss = 16.46172 Val Loss = 17.58116
2025-06-27 08:51:27.647324 Epoch 55  	Train Loss = 16.45216 Val Loss = 17.58290
2025-06-27 08:57:04.088186 Epoch 56  	Train Loss = 16.45535 Val Loss = 17.58020
2025-06-27 09:02:40.852105 Epoch 57  	Train Loss = 16.45466 Val Loss = 17.58015
2025-06-27 09:08:17.244081 Epoch 58  	Train Loss = 16.45855 Val Loss = 17.57915
2025-06-27 09:13:54.373167 Epoch 59  	Train Loss = 16.46220 Val Loss = 17.58213
2025-06-27 09:19:30.817246 Epoch 60  	Train Loss = 16.45293 Val Loss = 17.58178
2025-06-27 09:25:07.969241 Epoch 61  	Train Loss = 16.45709 Val Loss = 17.58047
2025-06-27 09:30:44.837012 Epoch 62  	Train Loss = 16.45644 Val Loss = 17.58281
2025-06-27 09:36:21.959259 Epoch 63  	Train Loss = 16.45455 Val Loss = 17.57643
2025-06-27 09:41:59.625259 Epoch 64  	Train Loss = 16.45566 Val Loss = 17.58013
2025-06-27 09:47:36.933394 Epoch 65  	Train Loss = 16.44718 Val Loss = 17.57814
2025-06-27 09:53:13.481536 Epoch 66  	Train Loss = 16.45183 Val Loss = 17.57599
2025-06-27 09:58:50.216954 Epoch 67  	Train Loss = 16.45580 Val Loss = 17.58042
2025-06-27 10:04:27.270641 Epoch 68  	Train Loss = 16.45292 Val Loss = 17.57951
2025-06-27 10:10:03.913190 Epoch 69  	Train Loss = 16.44661 Val Loss = 17.57899
2025-06-27 10:15:41.115977 Epoch 70  	Train Loss = 16.44937 Val Loss = 17.57781
2025-06-27 10:21:17.840424 Epoch 71  	Train Loss = 16.45612 Val Loss = 17.57778
2025-06-27 10:26:55.069803 Epoch 72  	Train Loss = 16.45873 Val Loss = 17.57862
2025-06-27 10:32:31.787578 Epoch 73  	Train Loss = 16.44723 Val Loss = 17.57993
2025-06-27 10:38:12.983111 Epoch 74  	Train Loss = 16.44927 Val Loss = 17.57621
2025-06-27 10:44:04.171719 Epoch 75  	Train Loss = 16.45021 Val Loss = 17.58158
2025-06-27 10:49:55.687427 Epoch 76  	Train Loss = 16.44962 Val Loss = 17.57924
2025-06-27 10:55:46.313951 Epoch 77  	Train Loss = 16.45655 Val Loss = 17.58073
2025-06-27 11:01:37.625953 Epoch 78  	Train Loss = 16.45017 Val Loss = 17.57665
2025-06-27 11:07:20.623217 Epoch 79  	Train Loss = 16.45051 Val Loss = 17.58328
2025-06-27 11:12:58.156558 Epoch 80  	Train Loss = 16.44654 Val Loss = 17.57867
2025-06-27 11:18:36.646276 Epoch 81  	Train Loss = 16.45066 Val Loss = 17.57808
2025-06-27 11:24:14.253337 Epoch 82  	Train Loss = 16.44545 Val Loss = 17.58485
2025-06-27 11:29:52.445645 Epoch 83  	Train Loss = 16.45438 Val Loss = 17.57918
2025-06-27 11:35:30.301642 Epoch 84  	Train Loss = 16.45333 Val Loss = 17.58070
2025-06-27 11:41:08.897885 Epoch 85  	Train Loss = 16.44713 Val Loss = 17.58027
2025-06-27 11:46:47.383955 Epoch 86  	Train Loss = 16.45174 Val Loss = 17.58060
Early stopping at epoch: 86
Best at epoch 66:
Train Loss = 16.45183
Train RMSE = 28.12033, MAE = 16.84472, MAPE = 12.02449
Val Loss = 17.57599
Val RMSE = 30.52805, MAE = 18.26082, MAPE = 11.75018
Saved Model: ../saved_models/STHDformer-PEMS04-2025-06-27-03-02-42.pt
--------- Test ---------
All Steps RMSE = 29.79478, MAE = 18.12825, MAPE = 11.91084
Step 1 RMSE = 26.93072, MAE = 16.61452, MAPE = 10.99265
Step 2 RMSE = 27.82916, MAE = 17.04192, MAPE = 11.30728
Step 3 RMSE = 28.51631, MAE = 17.41009, MAPE = 11.51774
Step 4 RMSE = 29.05308, MAE = 17.69399, MAPE = 11.67033
Step 5 RMSE = 29.50188, MAE = 17.93420, MAPE = 11.79025
Step 6 RMSE = 29.87741, MAE = 18.15481, MAPE = 11.89930
Step 7 RMSE = 30.22854, MAE = 18.35276, MAPE = 12.01954
Step 8 RMSE = 30.51160, MAE = 18.51752, MAPE = 12.11423
Step 9 RMSE = 30.78053, MAE = 18.68447, MAPE = 12.21157
Step 10 RMSE = 31.02759, MAE = 18.84702, MAPE = 12.32430
Step 11 RMSE = 31.28699, MAE = 19.03164, MAPE = 12.46360
Step 12 RMSE = 31.60655, MAE = 19.25583, MAPE = 12.61930
Inference time: 78.49 s
