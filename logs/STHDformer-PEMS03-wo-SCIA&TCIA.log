PEMS03
Trainset:	x-(15711, 12, 358, 3)	y-(15711, 12, 358, 1)
Valset:  	x-(5237, 12, 358, 3)  	y-(5237, 12, 358, 1)
Testset:	x-(5237, 12, 358, 3)	y-(5237, 12, 358, 1)

Random seed = 1
--------- STHDformer ---------
{
    "num_nodes": 358,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0005,
    "milestones": [
        15,
        30,
        40
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 358,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "steps_per_week": 7,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers_t": 1,
        "num_layers_c": 1,
        "num_layers_s": 1,
        "num_layers_mlp": 2,
        "dropout": 0.1,
        "adaptive_embedding_dim": 80,
        "node_dim": 64,
        "use_temporal_heterogeneity": true,
        "use_spatial_heterogeneity": true,
        "use_temporal_cross": false,
        "use_spatial_cross": false,
        "use_mixed_proj": true
    }
}
==============================================================================================================
Layer (type:depth-idx)                                       Output Shape              Param #
==============================================================================================================
STHDformer                                                   [16, 12, 358, 1]          320,424
├─Linear: 1-1                                                [16, 12, 358, 24]         96
├─Embedding: 1-2                                             [16, 12, 358, 24]         6,912
├─Embedding: 1-3                                             [16, 12, 358, 24]         168
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-1                               [16, 12, 358, 152]        23,256
│    │    └─AttentionLayer: 3-1                              [16, 358, 12, 152]        93,024
│    │    └─Dropout: 3-2                                     [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-3                                   [16, 358, 12, 152]        304
│    │    └─Sequential: 3-4                                  [16, 358, 12, 152]        78,232
│    │    └─Dropout: 3-5                                     [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-6                                   [16, 358, 12, 152]        304
├─ModuleList: 1-5                                            --                        --
│    └─Temporal_Heterogeneity_SelfAttentionLayer: 2-2        [16, 12, 358, 152]        --
│    │    └─Temporal_Heterogeneity_AttentionLayer: 3-7       [16, 358, 12, 152]        93,072
│    │    └─Dropout: 3-8                                     [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-9                                   [16, 358, 12, 152]        304
│    │    └─Sequential: 3-10                                 [16, 358, 12, 152]        78,232
│    │    └─Dropout: 3-11                                    [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-12                                  [16, 358, 12, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-3                         [16, 12, 358, 152]        --
│    │    └─Cross_AttentionLayer: 3-13                       [16, 358, 12, 152]        93,024
│    │    └─Dropout: 3-14                                    [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-15                                  [16, 358, 12, 152]        304
│    │    └─Sequential: 3-16                                 [16, 358, 12, 152]        78,232
│    │    └─Dropout: 3-17                                    [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-18                                  [16, 358, 12, 152]        304
├─Dual_graph: 1-7                                            [16, 12, 358, 128]        --
│    └─Graph_projection: 2-4                                 [1, 358, 64]              --
│    │    └─Linear: 3-19                                     [1, 358, 64]              22,976
│    │    └─ReLU: 3-20                                       [1, 358, 64]              --
│    │    └─Dropout: 3-21                                    [1, 358, 64]              --
│    │    └─Linear: 3-22                                     [1, 358, 64]              4,160
│    └─Graph_projection: 2-5                                 [1, 358, 64]              --
│    │    └─Linear: 3-23                                     [1, 358, 64]              22,976
│    │    └─ReLU: 3-24                                       [1, 358, 64]              --
│    │    └─Dropout: 3-25                                    [1, 358, 64]              --
│    │    └─Linear: 3-26                                     [1, 358, 64]              4,160
├─Fusion_Model: 1-8                                          [16, 12, 358, 152]        --
│    └─Sequential: 2-6                                       [16, 12, 358, 80]         --
│    │    └─MLP: 3-27                                        [16, 12, 358, 208]        86,944
│    │    └─MLP: 3-28                                        [16, 12, 358, 208]        86,944
│    │    └─Linear: 3-29                                     [16, 12, 358, 80]         16,720
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-7                               [16, 12, 358, 152]        (recursive)
│    │    └─Sequential: 3-30                                 [16, 12, 358, 152]        23,256
│    │    └─AttentionLayer: 3-31                             [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-32                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-33                                  [16, 12, 358, 152]        (recursive)
│    │    └─Sequential: 3-34                                 [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-35                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-36                                  [16, 12, 358, 152]        (recursive)
├─ModuleList: 1-10                                           --                        --
│    └─Spatial_Heterogeneity_SelfAttentionLayer: 2-8         [16, 12, 358, 152]        --
│    │    └─Spatial_Heterogeneity_AttentionLayer: 3-37       [16, 12, 358, 152]        93,024
│    │    └─Dropout: 3-38                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-39                                  [16, 12, 358, 152]        304
│    │    └─Sequential: 3-40                                 [16, 12, 358, 152]        78,232
│    │    └─Dropout: 3-41                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-42                                  [16, 12, 358, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-9                         [16, 12, 358, 152]        (recursive)
│    │    └─Cross_AttentionLayer: 3-43                       [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-44                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-45                                  [16, 12, 358, 152]        (recursive)
│    │    └─Sequential: 3-46                                 [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-47                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-48                                  [16, 12, 358, 152]        (recursive)
├─Linear: 1-12                                               [16, 358, 12]             21,900
==============================================================================================================
Total params: 1,328,396
Trainable params: 1,328,396
Non-trainable params: 0
Total mult-adds (M): 20.44
==============================================================================================================
Input size (MB): 0.82
Forward/backward pass size (MB): 4981.08
Params size (MB): 3.94
Estimated Total Size (MB): 4985.84
==============================================================================================================

Loss: HuberLoss

2025-06-05 21:01:15.040991 Epoch 1  	Train Loss = 20.88067 Val Loss = 15.48020
2025-06-05 21:04:26.731977 Epoch 2  	Train Loss = 15.63926 Val Loss = 14.67445
2025-06-05 21:07:37.762697 Epoch 3  	Train Loss = 14.94413 Val Loss = 14.31802
2025-06-05 21:10:48.696908 Epoch 4  	Train Loss = 14.38287 Val Loss = 14.05150
2025-06-05 21:13:59.133425 Epoch 5  	Train Loss = 14.02703 Val Loss = 14.83779
2025-06-05 21:17:10.005492 Epoch 6  	Train Loss = 13.78970 Val Loss = 13.87930
2025-06-05 21:20:20.673125 Epoch 7  	Train Loss = 13.74026 Val Loss = 13.78604
2025-06-05 21:23:31.438321 Epoch 8  	Train Loss = 13.54174 Val Loss = 13.77837
2025-06-05 21:26:42.193998 Epoch 9  	Train Loss = 13.52275 Val Loss = 14.99980
2025-06-05 21:29:53.032563 Epoch 10  	Train Loss = 13.38284 Val Loss = 13.67943
2025-06-05 21:33:03.837750 Epoch 11  	Train Loss = 13.28830 Val Loss = 13.51969
2025-06-05 21:36:14.624327 Epoch 12  	Train Loss = 13.28689 Val Loss = 13.62731
2025-06-05 21:39:25.444817 Epoch 13  	Train Loss = 13.11667 Val Loss = 13.52187
2025-06-05 21:42:36.133431 Epoch 14  	Train Loss = 13.09144 Val Loss = 13.64614
2025-06-05 21:45:46.751862 Epoch 15  	Train Loss = 13.04960 Val Loss = 13.60790
2025-06-05 21:48:57.325993 Epoch 16  	Train Loss = 12.35980 Val Loss = 13.07711
2025-06-05 21:52:07.786205 Epoch 17  	Train Loss = 12.30059 Val Loss = 13.08256
2025-06-05 21:55:18.067606 Epoch 18  	Train Loss = 12.28052 Val Loss = 13.14130
2025-06-05 21:58:28.398898 Epoch 19  	Train Loss = 12.26009 Val Loss = 13.11219
2025-06-05 22:01:38.637347 Epoch 20  	Train Loss = 12.24431 Val Loss = 13.14433
2025-06-05 22:04:48.976398 Epoch 21  	Train Loss = 12.22834 Val Loss = 13.10904
2025-06-05 22:07:59.179850 Epoch 22  	Train Loss = 12.21156 Val Loss = 13.17591
2025-06-05 22:11:09.247236 Epoch 23  	Train Loss = 12.19543 Val Loss = 13.18927
2025-06-05 22:14:19.387934 Epoch 24  	Train Loss = 12.18867 Val Loss = 13.20815
2025-06-05 22:17:29.567783 Epoch 25  	Train Loss = 12.16947 Val Loss = 13.08104
2025-06-05 22:20:39.661891 Epoch 26  	Train Loss = 12.15509 Val Loss = 13.09603
2025-06-05 22:23:49.775602 Epoch 27  	Train Loss = 12.14759 Val Loss = 13.10090
2025-06-05 22:26:59.961916 Epoch 28  	Train Loss = 12.13534 Val Loss = 13.06816
2025-06-05 22:30:10.258845 Epoch 29  	Train Loss = 12.12127 Val Loss = 13.12948
2025-06-05 22:33:20.291376 Epoch 30  	Train Loss = 12.11189 Val Loss = 13.10717
2025-06-05 22:36:30.528046 Epoch 31  	Train Loss = 12.03193 Val Loss = 13.06669
2025-06-05 22:39:40.905833 Epoch 32  	Train Loss = 12.02613 Val Loss = 13.06854
2025-06-05 22:42:51.125020 Epoch 33  	Train Loss = 12.02319 Val Loss = 13.06992
2025-06-05 22:46:01.314052 Epoch 34  	Train Loss = 12.02123 Val Loss = 13.08273
2025-06-05 22:49:11.433836 Epoch 35  	Train Loss = 12.01759 Val Loss = 13.08495
2025-06-05 22:52:21.594099 Epoch 36  	Train Loss = 12.01608 Val Loss = 13.06370
2025-06-05 22:55:31.756436 Epoch 37  	Train Loss = 12.01393 Val Loss = 13.10554
2025-06-05 22:58:41.811315 Epoch 38  	Train Loss = 12.01236 Val Loss = 13.09731
2025-06-05 23:01:51.938761 Epoch 39  	Train Loss = 12.01042 Val Loss = 13.06368
2025-06-05 23:05:02.189770 Epoch 40  	Train Loss = 12.00814 Val Loss = 13.08851
2025-06-05 23:08:12.351134 Epoch 41  	Train Loss = 12.00033 Val Loss = 13.08282
2025-06-05 23:11:22.692680 Epoch 42  	Train Loss = 11.99910 Val Loss = 13.07038
2025-06-05 23:14:32.854741 Epoch 43  	Train Loss = 11.99845 Val Loss = 13.07614
2025-06-05 23:17:43.045443 Epoch 44  	Train Loss = 11.99778 Val Loss = 13.06847
2025-06-05 23:20:53.212662 Epoch 45  	Train Loss = 11.99836 Val Loss = 13.07681
2025-06-05 23:24:03.523352 Epoch 46  	Train Loss = 11.99736 Val Loss = 13.07994
2025-06-05 23:27:13.877335 Epoch 47  	Train Loss = 11.99767 Val Loss = 13.07723
2025-06-05 23:30:24.050490 Epoch 48  	Train Loss = 11.99626 Val Loss = 13.07381
2025-06-05 23:33:34.228640 Epoch 49  	Train Loss = 11.99684 Val Loss = 13.07478
2025-06-05 23:36:44.449354 Epoch 50  	Train Loss = 11.99811 Val Loss = 13.06521
2025-06-05 23:39:54.719787 Epoch 51  	Train Loss = 11.99761 Val Loss = 13.07561
2025-06-05 23:43:04.807165 Epoch 52  	Train Loss = 11.99705 Val Loss = 13.07864
2025-06-05 23:46:15.152984 Epoch 53  	Train Loss = 11.99696 Val Loss = 13.07520
2025-06-05 23:49:25.595042 Epoch 54  	Train Loss = 11.99718 Val Loss = 13.08193
2025-06-05 23:52:35.898481 Epoch 55  	Train Loss = 11.99669 Val Loss = 13.07532
2025-06-05 23:55:46.167738 Epoch 56  	Train Loss = 11.99596 Val Loss = 13.07859
2025-06-05 23:58:56.302692 Epoch 57  	Train Loss = 11.99565 Val Loss = 13.07112
2025-06-06 00:02:06.521480 Epoch 58  	Train Loss = 11.99553 Val Loss = 13.07794
2025-06-06 00:05:17.032496 Epoch 59  	Train Loss = 11.99540 Val Loss = 13.07601
Early stopping at epoch: 59
Best at epoch 39:
Train Loss = 12.01042
Train RMSE = 20.36791, MAE = 12.30559, MAPE = 11.30007
Val Loss = 13.06368
Val RMSE = 22.04057, MAE = 13.58688, MAPE = 12.76298
Saved Model: ../saved_models/STHDformer-PEMS03-2025-06-05-20-58-02.pt
--------- Test ---------
All Steps RMSE = 27.57536, MAE = 15.25169, MAPE = 15.35051
Step 1 RMSE = 22.70355, MAE = 12.64383, MAPE = 13.26285
Step 2 RMSE = 24.22695, MAE = 13.36555, MAPE = 13.84288
Step 3 RMSE = 25.38366, MAE = 13.96640, MAPE = 14.36364
Step 4 RMSE = 26.33623, MAE = 14.47526, MAPE = 14.76316
Step 5 RMSE = 27.05974, MAE = 14.87440, MAPE = 15.08284
Step 6 RMSE = 27.67783, MAE = 15.25830, MAPE = 15.44012
Step 7 RMSE = 28.26900, MAE = 15.63660, MAPE = 15.66077
Step 8 RMSE = 28.73351, MAE = 15.94246, MAPE = 15.85150
Step 9 RMSE = 29.18898, MAE = 16.25544, MAPE = 16.09029
Step 10 RMSE = 29.63092, MAE = 16.56740, MAPE = 16.44899
Step 11 RMSE = 30.02987, MAE = 16.85054, MAPE = 16.65562
Step 12 RMSE = 30.48992, MAE = 17.18385, MAPE = 16.74313
Inference time: 23.05 s
