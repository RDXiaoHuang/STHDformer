PEMS04
Trainset:	x-(10181, 12, 307, 3)	y-(10181, 12, 307, 1)
Valset:  	x-(3394, 12, 307, 3)  	y-(3394, 12, 307, 1)
Testset:	x-(3394, 12, 307, 3)	y-(3394, 12, 307, 1)

Random seed = 1
--------- STHDformer ---------
{
    "num_nodes": 307,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0005,
    "milestones": [
        15,
        30,
        50
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 307,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "steps_per_week": 7,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers_t": 1,
        "num_layers_c": 1,
        "num_layers_s": 1,
        "num_layers_mlp": 2,
        "dropout": 0.1,
        "adaptive_embedding_dim": 80,
        "node_dim": 64,
        "use_temporal_heterogeneity": true,
        "use_spatial_heterogeneity": true,
        "use_temporal_cross": true,
        "use_spatial_cross": true,
        "use_mixed_proj": true
    }
}
==============================================================================================================
Layer (type:depth-idx)                                       Output Shape              Param #
==============================================================================================================
STHDformer                                                   [16, 12, 307, 1]          271,464
├─Linear: 1-1                                                [16, 12, 307, 24]         96
├─Embedding: 1-2                                             [16, 12, 307, 24]         6,912
├─Embedding: 1-3                                             [16, 12, 307, 24]         168
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-1                               [16, 12, 307, 152]        23,256
│    │    └─AttentionLayer: 3-1                              [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-2                                     [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-3                                   [16, 307, 12, 152]        304
│    │    └─Sequential: 3-4                                  [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-5                                     [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-6                                   [16, 307, 12, 152]        304
├─ModuleList: 1-5                                            --                        --
│    └─Temporal_Heterogeneity_SelfAttentionLayer: 2-2        [16, 12, 307, 152]        --
│    │    └─Temporal_Heterogeneity_AttentionLayer: 3-7       [16, 307, 12, 152]        93,072
│    │    └─Dropout: 3-8                                     [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-9                                   [16, 307, 12, 152]        304
│    │    └─Sequential: 3-10                                 [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-11                                    [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-12                                  [16, 307, 12, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-3                         [16, 12, 307, 152]        --
│    │    └─Cross_AttentionLayer: 3-13                       [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-14                                    [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-15                                  [16, 307, 12, 152]        304
│    │    └─Sequential: 3-16                                 [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-17                                    [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-18                                  [16, 307, 12, 152]        304
├─Dual_graph: 1-7                                            [16, 12, 307, 128]        --
│    └─Graph_projection: 2-4                                 [1, 307, 64]              --
│    │    └─Linear: 3-19                                     [1, 307, 64]              19,712
│    │    └─ReLU: 3-20                                       [1, 307, 64]              --
│    │    └─Dropout: 3-21                                    [1, 307, 64]              --
│    │    └─Linear: 3-22                                     [1, 307, 64]              4,160
│    └─Graph_projection: 2-5                                 [1, 307, 64]              --
│    │    └─Linear: 3-23                                     [1, 307, 64]              19,712
│    │    └─ReLU: 3-24                                       [1, 307, 64]              --
│    │    └─Dropout: 3-25                                    [1, 307, 64]              --
│    │    └─Linear: 3-26                                     [1, 307, 64]              4,160
├─Fusion_Model: 1-8                                          [16, 12, 307, 152]        --
│    └─Sequential: 2-6                                       [16, 12, 307, 80]         --
│    │    └─MLP: 3-27                                        [16, 12, 307, 208]        86,944
│    │    └─MLP: 3-28                                        [16, 12, 307, 208]        86,944
│    │    └─Linear: 3-29                                     [16, 12, 307, 80]         16,720
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-7                               [16, 12, 307, 152]        (recursive)
│    │    └─Sequential: 3-30                                 [16, 12, 307, 152]        23,256
│    │    └─AttentionLayer: 3-31                             [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-32                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-33                                  [16, 12, 307, 152]        (recursive)
│    │    └─Sequential: 3-34                                 [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-35                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-36                                  [16, 12, 307, 152]        (recursive)
├─ModuleList: 1-10                                           --                        --
│    └─Spatial_Heterogeneity_SelfAttentionLayer: 2-8         [16, 12, 307, 152]        --
│    │    └─Spatial_Heterogeneity_AttentionLayer: 3-37       [16, 12, 307, 152]        93,024
│    │    └─Dropout: 3-38                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-39                                  [16, 12, 307, 152]        304
│    │    └─Sequential: 3-40                                 [16, 12, 307, 152]        78,232
│    │    └─Dropout: 3-41                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-42                                  [16, 12, 307, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-9                         [16, 12, 307, 152]        (recursive)
│    │    └─Cross_AttentionLayer: 3-43                       [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-44                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-45                                  [16, 12, 307, 152]        (recursive)
│    │    └─Sequential: 3-46                                 [16, 12, 307, 152]        (recursive)
│    │    └─Dropout: 3-47                                    [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-48                                  [16, 12, 307, 152]        (recursive)
├─Linear: 1-12                                               [16, 307, 12]             21,900
==============================================================================================================
Total params: 1,272,908
Trainable params: 1,272,908
Non-trainable params: 0
Total mult-adds (M): 20.43
==============================================================================================================
Input size (MB): 0.71
Forward/backward pass size (MB): 4271.48
Params size (MB): 3.91
Estimated Total Size (MB): 4276.10
==============================================================================================================

Loss: HuberLoss

2025-06-04 09:04:28.521700 Epoch 1  	Train Loss = 30.21707 Val Loss = 22.95697
2025-06-04 09:06:12.022158 Epoch 2  	Train Loss = 22.33004 Val Loss = 22.75158
2025-06-04 09:07:55.413818 Epoch 3  	Train Loss = 20.86741 Val Loss = 20.40470
2025-06-04 09:09:38.739220 Epoch 4  	Train Loss = 20.43174 Val Loss = 21.46227
2025-06-04 09:11:21.977311 Epoch 5  	Train Loss = 19.69310 Val Loss = 21.65380
2025-06-04 09:13:05.191592 Epoch 6  	Train Loss = 19.20532 Val Loss = 19.32890
2025-06-04 09:14:48.375843 Epoch 7  	Train Loss = 19.11080 Val Loss = 19.05147
2025-06-04 09:16:31.385929 Epoch 8  	Train Loss = 18.91043 Val Loss = 19.15695
2025-06-04 09:18:14.093141 Epoch 9  	Train Loss = 18.63126 Val Loss = 19.64298
2025-06-04 09:19:56.926138 Epoch 10  	Train Loss = 18.57990 Val Loss = 19.20767
2025-06-04 09:21:39.703769 Epoch 11  	Train Loss = 18.49571 Val Loss = 19.09159
2025-06-04 09:23:22.470272 Epoch 12  	Train Loss = 18.13761 Val Loss = 20.24554
2025-06-04 09:25:05.356071 Epoch 13  	Train Loss = 18.11715 Val Loss = 18.45344
2025-06-04 09:26:48.273051 Epoch 14  	Train Loss = 18.06212 Val Loss = 19.29439
2025-06-04 09:28:31.213735 Epoch 15  	Train Loss = 17.87658 Val Loss = 19.31032
2025-06-04 09:30:14.242677 Epoch 16  	Train Loss = 17.06089 Val Loss = 17.81494
2025-06-04 09:31:57.150147 Epoch 17  	Train Loss = 16.93653 Val Loss = 17.84925
2025-06-04 09:33:40.077992 Epoch 18  	Train Loss = 16.88923 Val Loss = 17.77186
2025-06-04 09:35:22.846931 Epoch 19  	Train Loss = 16.87117 Val Loss = 17.78728
2025-06-04 09:37:05.660680 Epoch 20  	Train Loss = 16.84116 Val Loss = 17.76015
2025-06-04 09:38:48.507269 Epoch 21  	Train Loss = 16.81855 Val Loss = 17.73760
2025-06-04 09:40:31.351049 Epoch 22  	Train Loss = 16.79941 Val Loss = 17.89457
2025-06-04 09:42:14.168080 Epoch 23  	Train Loss = 16.76695 Val Loss = 17.76711
2025-06-04 09:43:57.047011 Epoch 24  	Train Loss = 16.73480 Val Loss = 17.75050
2025-06-04 09:45:39.854964 Epoch 25  	Train Loss = 16.72232 Val Loss = 17.74816
2025-06-04 09:47:22.661485 Epoch 26  	Train Loss = 16.69403 Val Loss = 17.70394
2025-06-04 09:49:05.433169 Epoch 27  	Train Loss = 16.67063 Val Loss = 17.84393
2025-06-04 09:50:48.185844 Epoch 28  	Train Loss = 16.65619 Val Loss = 17.75330
2025-06-04 09:52:30.856057 Epoch 29  	Train Loss = 16.63379 Val Loss = 17.81235
2025-06-04 09:54:13.682099 Epoch 30  	Train Loss = 16.60993 Val Loss = 17.73152
2025-06-04 09:55:56.500055 Epoch 31  	Train Loss = 16.51336 Val Loss = 17.65109
2025-06-04 09:57:39.239717 Epoch 32  	Train Loss = 16.50115 Val Loss = 17.63907
2025-06-04 09:59:22.115707 Epoch 33  	Train Loss = 16.50363 Val Loss = 17.64649
2025-06-04 10:01:04.976995 Epoch 34  	Train Loss = 16.50068 Val Loss = 17.65688
2025-06-04 10:02:47.894522 Epoch 35  	Train Loss = 16.49610 Val Loss = 17.64104
2025-06-04 10:04:30.581424 Epoch 36  	Train Loss = 16.49567 Val Loss = 17.64474
2025-06-04 10:06:13.467582 Epoch 37  	Train Loss = 16.48742 Val Loss = 17.65714
2025-06-04 10:07:56.191836 Epoch 38  	Train Loss = 16.48821 Val Loss = 17.65891
2025-06-04 10:09:39.073398 Epoch 39  	Train Loss = 16.47979 Val Loss = 17.65706
2025-06-04 10:11:21.778332 Epoch 40  	Train Loss = 16.48124 Val Loss = 17.66793
2025-06-04 10:13:04.542252 Epoch 41  	Train Loss = 16.48157 Val Loss = 17.64671
2025-06-04 10:14:47.247729 Epoch 42  	Train Loss = 16.47744 Val Loss = 17.65886
2025-06-04 10:16:30.042660 Epoch 43  	Train Loss = 16.46900 Val Loss = 17.65748
2025-06-04 10:18:12.955806 Epoch 44  	Train Loss = 16.47302 Val Loss = 17.66018
2025-06-04 10:19:55.569679 Epoch 45  	Train Loss = 16.47002 Val Loss = 17.65040
2025-06-04 10:21:38.342875 Epoch 46  	Train Loss = 16.46188 Val Loss = 17.66506
2025-06-04 10:23:21.054621 Epoch 47  	Train Loss = 16.45686 Val Loss = 17.66176
2025-06-04 10:25:03.758514 Epoch 48  	Train Loss = 16.46195 Val Loss = 17.65832
2025-06-04 10:26:46.685600 Epoch 49  	Train Loss = 16.45222 Val Loss = 17.65004
2025-06-04 10:28:29.363848 Epoch 50  	Train Loss = 16.45276 Val Loss = 17.64861
2025-06-04 10:30:12.213162 Epoch 51  	Train Loss = 16.44613 Val Loss = 17.64684
2025-06-04 10:31:54.968695 Epoch 52  	Train Loss = 16.44735 Val Loss = 17.64459
Early stopping at epoch: 52
Best at epoch 32:
Train Loss = 16.50115
Train RMSE = 28.06912, MAE = 16.86548, MAPE = 12.06424
Val Loss = 17.63907
Val RMSE = 30.59582, MAE = 18.31329, MAPE = 11.78622
Saved Model: ../saved_models/STHDformer-PEMS04-2025-06-04-09-02-43.pt
--------- Test ---------
All Steps RMSE = 29.76094, MAE = 18.12493, MAPE = 11.92313
Step 1 RMSE = 26.91287, MAE = 16.60564, MAPE = 11.01319
Step 2 RMSE = 27.80736, MAE = 17.04333, MAPE = 11.32312
Step 3 RMSE = 28.48471, MAE = 17.41299, MAPE = 11.53607
Step 4 RMSE = 29.02238, MAE = 17.70055, MAPE = 11.68873
Step 5 RMSE = 29.45698, MAE = 17.93631, MAPE = 11.80179
Step 6 RMSE = 29.81301, MAE = 18.12717, MAPE = 11.89398
Step 7 RMSE = 30.17015, MAE = 18.33631, MAPE = 12.01407
Step 8 RMSE = 30.47067, MAE = 18.50303, MAPE = 12.12370
Step 9 RMSE = 30.76089, MAE = 18.69501, MAPE = 12.23305
Step 10 RMSE = 31.01961, MAE = 18.86739, MAPE = 12.36231
Step 11 RMSE = 31.25933, MAE = 19.03004, MAPE = 12.46672
Step 12 RMSE = 31.56726, MAE = 19.24117, MAPE = 12.62071
Inference time: 12.57 s
