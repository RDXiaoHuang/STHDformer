PEMS08
Trainset:	x-(10700, 12, 170, 3)	y-(10700, 12, 170, 1)
Valset:  	x-(3567, 12, 170, 3)  	y-(3567, 12, 170, 1)
Testset:	x-(3566, 12, 170, 3)	y-(3566, 12, 170, 1)

Random seed = 1
--------- STHDformer ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0015,
    "milestones": [
        25,
        45,
        65
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 30,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 170,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "steps_per_week": 7,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers_t": 1,
        "num_layers_c": 1,
        "num_layers_s": 1,
        "num_layers_mlp": 2,
        "dropout": 0.1,
        "adaptive_embedding_dim": 80,
        "node_dim": 64,
        "use_temporal_heterogeneity": true,
        "use_spatial_heterogeneity": true,
        "use_temporal_cross": true,
        "use_spatial_cross": true,
        "use_mixed_proj": true
    }
}
==============================================================================================================
Layer (type:depth-idx)                                       Output Shape              Param #
==============================================================================================================
STHDformer                                                   [16, 12, 170, 1]          139,944
├─Linear: 1-1                                                [16, 12, 170, 24]         96
├─Embedding: 1-2                                             [16, 12, 170, 24]         6,912
├─Embedding: 1-3                                             [16, 12, 170, 24]         168
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-1                               [16, 12, 170, 152]        23,256
│    │    └─AttentionLayer: 3-1                              [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-2                                     [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-3                                   [16, 170, 12, 152]        304
│    │    └─Sequential: 3-4                                  [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-5                                     [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-6                                   [16, 170, 12, 152]        304
├─ModuleList: 1-5                                            --                        --
│    └─Temporal_Heterogeneity_SelfAttentionLayer: 2-2        [16, 12, 170, 152]        --
│    │    └─Temporal_Heterogeneity_AttentionLayer: 3-7       [16, 170, 12, 152]        93,072
│    │    └─Dropout: 3-8                                     [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-9                                   [16, 170, 12, 152]        304
│    │    └─Sequential: 3-10                                 [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-11                                    [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-12                                  [16, 170, 12, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-3                         [16, 12, 170, 152]        --
│    │    └─Cross_AttentionLayer: 3-13                       [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-14                                    [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-15                                  [16, 170, 12, 152]        304
│    │    └─Sequential: 3-16                                 [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-17                                    [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-18                                  [16, 170, 12, 152]        304
├─Dual_graph: 1-7                                            [16, 12, 170, 128]        --
│    └─Graph_projection: 2-4                                 [1, 170, 64]              --
│    │    └─Linear: 3-19                                     [1, 170, 64]              10,944
│    │    └─ReLU: 3-20                                       [1, 170, 64]              --
│    │    └─Dropout: 3-21                                    [1, 170, 64]              --
│    │    └─Linear: 3-22                                     [1, 170, 64]              4,160
│    └─Graph_projection: 2-5                                 [1, 170, 64]              --
│    │    └─Linear: 3-23                                     [1, 170, 64]              10,944
│    │    └─ReLU: 3-24                                       [1, 170, 64]              --
│    │    └─Dropout: 3-25                                    [1, 170, 64]              --
│    │    └─Linear: 3-26                                     [1, 170, 64]              4,160
├─Fusion_Model: 1-8                                          [16, 12, 170, 152]        --
│    └─Sequential: 2-6                                       [16, 12, 170, 80]         --
│    │    └─MLP: 3-27                                        [16, 12, 170, 208]        86,944
│    │    └─MLP: 3-28                                        [16, 12, 170, 208]        86,944
│    │    └─Linear: 3-29                                     [16, 12, 170, 80]         16,720
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-7                               [16, 12, 170, 152]        (recursive)
│    │    └─Sequential: 3-30                                 [16, 12, 170, 152]        23,256
│    │    └─AttentionLayer: 3-31                             [16, 12, 170, 152]        (recursive)
│    │    └─Dropout: 3-32                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-33                                  [16, 12, 170, 152]        (recursive)
│    │    └─Sequential: 3-34                                 [16, 12, 170, 152]        (recursive)
│    │    └─Dropout: 3-35                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-36                                  [16, 12, 170, 152]        (recursive)
├─ModuleList: 1-10                                           --                        --
│    └─Spatial_Heterogeneity_SelfAttentionLayer: 2-8         [16, 12, 170, 152]        --
│    │    └─Spatial_Heterogeneity_AttentionLayer: 3-37       [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-38                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-39                                  [16, 12, 170, 152]        304
│    │    └─Sequential: 3-40                                 [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-41                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-42                                  [16, 12, 170, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-9                         [16, 12, 170, 152]        (recursive)
│    │    └─Cross_AttentionLayer: 3-43                       [16, 12, 170, 152]        (recursive)
│    │    └─Dropout: 3-44                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-45                                  [16, 12, 170, 152]        (recursive)
│    │    └─Sequential: 3-46                                 [16, 12, 170, 152]        (recursive)
│    │    └─Dropout: 3-47                                    [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-48                                  [16, 12, 170, 152]        (recursive)
├─Linear: 1-12                                               [16, 170, 12]             21,900
==============================================================================================================
Total params: 1,123,852
Trainable params: 1,123,852
Non-trainable params: 0
Total mult-adds (M): 20.42
==============================================================================================================
Input size (MB): 0.39
Forward/backward pass size (MB): 2365.32
Params size (MB): 3.84
Estimated Total Size (MB): 2369.56
==============================================================================================================

Loss: HuberLoss

2025-06-03 11:00:45.239922 Epoch 1  	Train Loss = 26.35656 Val Loss = 17.88898
2025-06-03 11:01:46.060938 Epoch 2  	Train Loss = 18.72825 Val Loss = 18.34649
2025-06-03 11:03:39.665619 Epoch 3  	Train Loss = 17.24490 Val Loss = 15.95487
2025-06-03 11:05:41.697778 Epoch 4  	Train Loss = 16.68710 Val Loss = 16.26019
2025-06-03 11:07:42.173484 Epoch 5  	Train Loss = 16.13370 Val Loss = 16.22965
2025-06-03 11:09:41.419152 Epoch 6  	Train Loss = 15.76567 Val Loss = 15.07613
2025-06-03 11:11:39.944986 Epoch 7  	Train Loss = 15.47479 Val Loss = 15.27009
2025-06-03 11:13:37.731830 Epoch 8  	Train Loss = 15.46722 Val Loss = 16.28789
2025-06-03 11:15:35.178471 Epoch 9  	Train Loss = 15.04964 Val Loss = 14.70211
2025-06-03 11:17:32.493868 Epoch 10  	Train Loss = 15.00585 Val Loss = 14.72079
2025-06-03 11:19:30.090137 Epoch 11  	Train Loss = 14.82772 Val Loss = 15.14153
2025-06-03 11:21:27.670040 Epoch 12  	Train Loss = 14.67759 Val Loss = 14.33245
2025-06-03 11:23:24.982286 Epoch 13  	Train Loss = 14.57710 Val Loss = 15.29350
2025-06-03 11:25:22.256598 Epoch 14  	Train Loss = 14.48098 Val Loss = 15.36266
2025-06-03 11:27:19.663945 Epoch 15  	Train Loss = 14.36596 Val Loss = 14.38802
2025-06-03 11:29:16.925645 Epoch 16  	Train Loss = 14.26278 Val Loss = 14.13683
2025-06-03 11:31:13.745767 Epoch 17  	Train Loss = 14.22199 Val Loss = 13.87298
2025-06-03 11:33:10.830482 Epoch 18  	Train Loss = 14.02509 Val Loss = 14.03210
2025-06-03 11:35:07.849331 Epoch 19  	Train Loss = 14.01491 Val Loss = 13.80052
2025-06-03 11:37:04.519529 Epoch 20  	Train Loss = 13.96402 Val Loss = 14.10327
2025-06-03 11:39:01.381507 Epoch 21  	Train Loss = 13.80548 Val Loss = 13.94721
2025-06-03 11:40:58.200009 Epoch 22  	Train Loss = 13.86415 Val Loss = 13.72534
2025-06-03 11:42:54.579300 Epoch 23  	Train Loss = 13.70278 Val Loss = 13.63876
2025-06-03 11:44:51.276994 Epoch 24  	Train Loss = 13.67641 Val Loss = 14.05817
2025-06-03 11:46:48.007076 Epoch 25  	Train Loss = 13.66229 Val Loss = 13.62582
2025-06-03 11:48:44.649177 Epoch 26  	Train Loss = 12.93708 Val Loss = 13.13168
2025-06-03 11:50:40.855058 Epoch 27  	Train Loss = 12.85854 Val Loss = 13.14376
2025-06-03 11:52:37.499497 Epoch 28  	Train Loss = 12.82850 Val Loss = 13.16282
2025-06-03 11:54:34.210611 Epoch 29  	Train Loss = 12.80982 Val Loss = 13.20291
2025-06-03 11:56:30.431447 Epoch 30  	Train Loss = 12.78790 Val Loss = 13.06616
2025-06-03 11:58:26.908211 Epoch 31  	Train Loss = 12.77287 Val Loss = 13.07606
2025-06-03 12:00:23.473279 Epoch 32  	Train Loss = 12.76024 Val Loss = 13.09206
2025-06-03 12:02:19.763318 Epoch 33  	Train Loss = 12.74202 Val Loss = 13.07594
2025-06-03 12:04:16.301496 Epoch 34  	Train Loss = 12.72981 Val Loss = 13.05860
2025-06-03 12:06:12.797098 Epoch 35  	Train Loss = 12.71597 Val Loss = 13.12911
2025-06-03 12:08:09.153108 Epoch 36  	Train Loss = 12.71194 Val Loss = 13.04537
2025-06-03 12:10:05.439694 Epoch 37  	Train Loss = 12.69318 Val Loss = 13.08193
2025-06-03 12:12:01.876370 Epoch 38  	Train Loss = 12.68313 Val Loss = 13.08375
2025-06-03 12:13:58.383230 Epoch 39  	Train Loss = 12.67193 Val Loss = 13.07690
2025-06-03 12:15:54.618183 Epoch 40  	Train Loss = 12.66294 Val Loss = 13.12906
2025-06-03 12:17:51.121285 Epoch 41  	Train Loss = 12.65245 Val Loss = 13.05311
2025-06-03 12:19:47.685446 Epoch 42  	Train Loss = 12.63880 Val Loss = 13.06814
2025-06-03 12:21:43.900714 Epoch 43  	Train Loss = 12.63420 Val Loss = 13.06151
2025-06-03 12:23:40.353188 Epoch 44  	Train Loss = 12.62806 Val Loss = 13.13243
2025-06-03 12:25:36.795969 Epoch 45  	Train Loss = 12.61909 Val Loss = 13.08725
2025-06-03 12:27:32.910328 Epoch 46  	Train Loss = 12.53990 Val Loss = 13.01876
2025-06-03 12:29:29.282195 Epoch 47  	Train Loss = 12.53300 Val Loss = 13.01373
2025-06-03 12:31:25.660436 Epoch 48  	Train Loss = 12.53086 Val Loss = 13.02732
2025-06-03 12:33:21.962349 Epoch 49  	Train Loss = 12.53011 Val Loss = 13.00584
2025-06-03 12:35:18.221759 Epoch 50  	Train Loss = 12.52632 Val Loss = 13.01184
2025-06-03 12:37:14.764401 Epoch 51  	Train Loss = 12.52631 Val Loss = 12.99919
2025-06-03 12:39:11.348924 Epoch 52  	Train Loss = 12.52003 Val Loss = 13.02211
2025-06-03 12:41:07.555816 Epoch 53  	Train Loss = 12.52305 Val Loss = 13.01412
2025-06-03 12:43:04.051597 Epoch 54  	Train Loss = 12.51884 Val Loss = 13.01172
2025-06-03 12:45:00.561929 Epoch 55  	Train Loss = 12.51755 Val Loss = 13.01648
2025-06-03 12:46:56.639615 Epoch 56  	Train Loss = 12.51613 Val Loss = 13.01481
2025-06-03 12:48:53.021414 Epoch 57  	Train Loss = 12.51575 Val Loss = 13.00963
2025-06-03 12:50:49.421379 Epoch 58  	Train Loss = 12.51229 Val Loss = 13.02046
2025-06-03 12:52:45.558781 Epoch 59  	Train Loss = 12.51211 Val Loss = 13.01594
2025-06-03 12:54:41.833433 Epoch 60  	Train Loss = 12.51117 Val Loss = 13.02453
2025-06-03 12:56:38.236315 Epoch 61  	Train Loss = 12.51257 Val Loss = 13.01675
2025-06-03 12:58:34.598603 Epoch 62  	Train Loss = 12.50663 Val Loss = 13.01790
2025-06-03 13:00:30.571834 Epoch 63  	Train Loss = 12.50599 Val Loss = 13.00615
2025-06-03 13:02:26.905083 Epoch 64  	Train Loss = 12.50590 Val Loss = 13.01108
2025-06-03 13:04:23.273025 Epoch 65  	Train Loss = 12.50218 Val Loss = 13.01087
2025-06-03 13:05:53.872872 Epoch 66  	Train Loss = 12.49454 Val Loss = 13.00829
2025-06-03 13:06:51.936938 Epoch 67  	Train Loss = 12.49353 Val Loss = 13.00924
2025-06-03 13:07:49.909415 Epoch 68  	Train Loss = 12.49111 Val Loss = 13.01013
2025-06-03 13:08:48.063992 Epoch 69  	Train Loss = 12.49406 Val Loss = 13.01289
2025-06-03 13:09:46.429791 Epoch 70  	Train Loss = 12.49194 Val Loss = 13.01572
2025-06-03 13:10:44.483787 Epoch 71  	Train Loss = 12.49436 Val Loss = 13.00653
2025-06-03 13:11:42.863893 Epoch 72  	Train Loss = 12.49237 Val Loss = 13.01220
2025-06-03 13:12:40.982870 Epoch 73  	Train Loss = 12.49297 Val Loss = 13.00881
2025-06-03 13:13:39.259628 Epoch 74  	Train Loss = 12.49191 Val Loss = 13.01243
2025-06-03 13:14:37.792022 Epoch 75  	Train Loss = 12.48896 Val Loss = 13.00910
2025-06-03 13:15:35.741481 Epoch 76  	Train Loss = 12.49278 Val Loss = 13.00968
2025-06-03 13:16:33.710728 Epoch 77  	Train Loss = 12.49090 Val Loss = 13.01157
2025-06-03 13:17:31.803981 Epoch 78  	Train Loss = 12.49338 Val Loss = 13.00821
2025-06-03 13:18:30.099899 Epoch 79  	Train Loss = 12.49095 Val Loss = 13.00904
2025-06-03 13:19:28.183508 Epoch 80  	Train Loss = 12.49302 Val Loss = 13.01159
2025-06-03 13:20:26.362061 Epoch 81  	Train Loss = 12.49252 Val Loss = 13.01081
Early stopping at epoch: 81
Best at epoch 51:
Train Loss = 12.52631
Train RMSE = 22.33256, MAE = 12.70457, MAPE = 8.28947
Val Loss = 12.99919
Val RMSE = 23.81775, MAE = 13.43394, MAPE = 9.87130
Saved Model: ../saved_models/STHDformer-PEMS08-2025-06-03-10-59-43.pt
--------- Test ---------
All Steps RMSE = 23.02266, MAE = 13.33228, MAPE = 8.70239
Step 1 RMSE = 19.31389, MAE = 11.58407, MAPE = 7.59443
Step 2 RMSE = 20.37113, MAE = 12.02060, MAPE = 7.86682
Step 3 RMSE = 21.19885, MAE = 12.39208, MAPE = 8.08789
Step 4 RMSE = 21.91021, MAE = 12.69756, MAPE = 8.28281
Step 5 RMSE = 22.47829, MAE = 12.96694, MAPE = 8.45966
Step 6 RMSE = 23.00805, MAE = 13.22441, MAPE = 8.63638
Step 7 RMSE = 23.46488, MAE = 13.46587, MAPE = 8.78243
Step 8 RMSE = 23.89975, MAE = 13.70006, MAPE = 8.94982
Step 9 RMSE = 24.28354, MAE = 13.98722, MAPE = 9.12477
Step 10 RMSE = 24.69130, MAE = 14.24582, MAPE = 9.30820
Step 11 RMSE = 25.12245, MAE = 14.68908, MAPE = 9.55535
Step 12 RMSE = 25.61492, MAE = 15.01366, MAPE = 9.78013
Inference time: 7.15 s
