PEMS03
Trainset:	x-(15711, 12, 358, 3)	y-(15711, 12, 358, 1)
Valset:  	x-(5237, 12, 358, 3)  	y-(5237, 12, 358, 1)
Testset:	x-(5237, 12, 358, 3)	y-(5237, 12, 358, 1)

Random seed = 1
--------- STHDformer ---------
{
    "num_nodes": 358,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0005,
    "milestones": [
        15,
        30,
        40
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 358,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "steps_per_week": 7,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers_t": 1,
        "num_layers_c": 1,
        "num_layers_s": 1,
        "num_layers_mlp": 2,
        "dropout": 0.1,
        "adaptive_embedding_dim": 80,
        "node_dim": 64,
        "use_temporal_heterogeneity": true,
        "use_spatial_heterogeneity": true,
        "use_temporal_cross": true,
        "use_spatial_cross": false,
        "use_mixed_proj": true
    }
}
==============================================================================================================
Layer (type:depth-idx)                                       Output Shape              Param #
==============================================================================================================
STHDformer                                                   [16, 12, 358, 1]          320,424
├─Linear: 1-1                                                [16, 12, 358, 24]         96
├─Embedding: 1-2                                             [16, 12, 358, 24]         6,912
├─Embedding: 1-3                                             [16, 12, 358, 24]         168
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-1                               [16, 12, 358, 152]        23,256
│    │    └─AttentionLayer: 3-1                              [16, 358, 12, 152]        93,024
│    │    └─Dropout: 3-2                                     [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-3                                   [16, 358, 12, 152]        304
│    │    └─Sequential: 3-4                                  [16, 358, 12, 152]        78,232
│    │    └─Dropout: 3-5                                     [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-6                                   [16, 358, 12, 152]        304
├─ModuleList: 1-5                                            --                        --
│    └─Temporal_Heterogeneity_SelfAttentionLayer: 2-2        [16, 12, 358, 152]        --
│    │    └─Temporal_Heterogeneity_AttentionLayer: 3-7       [16, 358, 12, 152]        93,072
│    │    └─Dropout: 3-8                                     [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-9                                   [16, 358, 12, 152]        304
│    │    └─Sequential: 3-10                                 [16, 358, 12, 152]        78,232
│    │    └─Dropout: 3-11                                    [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-12                                  [16, 358, 12, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-3                         [16, 12, 358, 152]        --
│    │    └─Cross_AttentionLayer: 3-13                       [16, 358, 12, 152]        93,024
│    │    └─Dropout: 3-14                                    [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-15                                  [16, 358, 12, 152]        304
│    │    └─Sequential: 3-16                                 [16, 358, 12, 152]        78,232
│    │    └─Dropout: 3-17                                    [16, 358, 12, 152]        --
│    │    └─LayerNorm: 3-18                                  [16, 358, 12, 152]        304
├─Dual_graph: 1-7                                            [16, 12, 358, 128]        --
│    └─Graph_projection: 2-4                                 [1, 358, 64]              --
│    │    └─Linear: 3-19                                     [1, 358, 64]              22,976
│    │    └─ReLU: 3-20                                       [1, 358, 64]              --
│    │    └─Dropout: 3-21                                    [1, 358, 64]              --
│    │    └─Linear: 3-22                                     [1, 358, 64]              4,160
│    └─Graph_projection: 2-5                                 [1, 358, 64]              --
│    │    └─Linear: 3-23                                     [1, 358, 64]              22,976
│    │    └─ReLU: 3-24                                       [1, 358, 64]              --
│    │    └─Dropout: 3-25                                    [1, 358, 64]              --
│    │    └─Linear: 3-26                                     [1, 358, 64]              4,160
├─Fusion_Model: 1-8                                          [16, 12, 358, 152]        --
│    └─Sequential: 2-6                                       [16, 12, 358, 80]         --
│    │    └─MLP: 3-27                                        [16, 12, 358, 208]        86,944
│    │    └─MLP: 3-28                                        [16, 12, 358, 208]        86,944
│    │    └─Linear: 3-29                                     [16, 12, 358, 80]         16,720
├─ModuleList: 1-9                                            --                        (recursive)
│    └─SelfAttentionLayer: 2-7                               [16, 12, 358, 152]        (recursive)
│    │    └─Sequential: 3-30                                 [16, 12, 358, 152]        23,256
│    │    └─AttentionLayer: 3-31                             [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-32                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-33                                  [16, 12, 358, 152]        (recursive)
│    │    └─Sequential: 3-34                                 [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-35                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-36                                  [16, 12, 358, 152]        (recursive)
├─ModuleList: 1-10                                           --                        --
│    └─Spatial_Heterogeneity_SelfAttentionLayer: 2-8         [16, 12, 358, 152]        --
│    │    └─Spatial_Heterogeneity_AttentionLayer: 3-37       [16, 12, 358, 152]        93,024
│    │    └─Dropout: 3-38                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-39                                  [16, 12, 358, 152]        304
│    │    └─Sequential: 3-40                                 [16, 12, 358, 152]        78,232
│    │    └─Dropout: 3-41                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-42                                  [16, 12, 358, 152]        304
├─ModuleList: 1-11                                           --                        (recursive)
│    └─Cross_SelfAttentionLayer: 2-9                         [16, 12, 358, 152]        (recursive)
│    │    └─Cross_AttentionLayer: 3-43                       [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-44                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-45                                  [16, 12, 358, 152]        (recursive)
│    │    └─Sequential: 3-46                                 [16, 12, 358, 152]        (recursive)
│    │    └─Dropout: 3-47                                    [16, 12, 358, 152]        --
│    │    └─LayerNorm: 3-48                                  [16, 12, 358, 152]        (recursive)
├─Linear: 1-12                                               [16, 358, 12]             21,900
==============================================================================================================
Total params: 1,328,396
Trainable params: 1,328,396
Non-trainable params: 0
Total mult-adds (M): 20.44
==============================================================================================================
Input size (MB): 0.82
Forward/backward pass size (MB): 4981.08
Params size (MB): 3.94
Estimated Total Size (MB): 4985.84
==============================================================================================================

Loss: HuberLoss

2025-06-05 11:47:59.558381 Epoch 1  	Train Loss = 21.36081 Val Loss = 16.23664
2025-06-05 11:57:46.709935 Epoch 2  	Train Loss = 16.11381 Val Loss = 14.99139
2025-06-05 12:07:33.060251 Epoch 3  	Train Loss = 15.10483 Val Loss = 14.53322
2025-06-05 12:17:18.695235 Epoch 4  	Train Loss = 14.64591 Val Loss = 14.28174
2025-06-05 12:27:04.240492 Epoch 5  	Train Loss = 14.22910 Val Loss = 13.89730
2025-06-05 12:36:49.703868 Epoch 6  	Train Loss = 13.93858 Val Loss = 14.78930
2025-06-05 12:46:35.092905 Epoch 7  	Train Loss = 13.85764 Val Loss = 13.70366
2025-06-05 12:56:20.506511 Epoch 8  	Train Loss = 13.63925 Val Loss = 13.81186
2025-06-05 13:06:06.130040 Epoch 9  	Train Loss = 13.52176 Val Loss = 14.95486
2025-06-05 13:15:51.792164 Epoch 10  	Train Loss = 13.47967 Val Loss = 13.53889
2025-06-05 13:25:37.229012 Epoch 11  	Train Loss = 13.26820 Val Loss = 13.50713
2025-06-05 13:35:22.621995 Epoch 12  	Train Loss = 13.23679 Val Loss = 13.56129
2025-06-05 13:45:08.037555 Epoch 13  	Train Loss = 13.18875 Val Loss = 13.46120
2025-06-05 13:54:52.742286 Epoch 14  	Train Loss = 13.06348 Val Loss = 13.91162
2025-06-05 14:04:37.181990 Epoch 15  	Train Loss = 13.02156 Val Loss = 13.82737
2025-06-05 14:14:21.843380 Epoch 16  	Train Loss = 12.31557 Val Loss = 13.05379
2025-06-05 14:24:07.366702 Epoch 17  	Train Loss = 12.24755 Val Loss = 13.06108
2025-06-05 14:33:52.944438 Epoch 18  	Train Loss = 12.22407 Val Loss = 13.10927
2025-06-05 14:43:38.672957 Epoch 19  	Train Loss = 12.20223 Val Loss = 13.08520
2025-06-05 14:53:24.722097 Epoch 20  	Train Loss = 12.18345 Val Loss = 13.09202
2025-06-05 15:03:10.434053 Epoch 21  	Train Loss = 12.16677 Val Loss = 13.07372
2025-06-05 15:12:56.310131 Epoch 22  	Train Loss = 12.14849 Val Loss = 13.12436
2025-06-05 15:22:42.252728 Epoch 23  	Train Loss = 12.13217 Val Loss = 13.10769
2025-06-05 15:32:27.844497 Epoch 24  	Train Loss = 12.12051 Val Loss = 13.13199
2025-06-05 15:42:13.556516 Epoch 25  	Train Loss = 12.10463 Val Loss = 13.02622
2025-06-05 15:51:59.288072 Epoch 26  	Train Loss = 12.08668 Val Loss = 13.03801
2025-06-05 16:01:45.029426 Epoch 27  	Train Loss = 12.07665 Val Loss = 13.03776
2025-06-05 16:11:30.816540 Epoch 28  	Train Loss = 12.06501 Val Loss = 13.02959
2025-06-05 16:21:16.716497 Epoch 29  	Train Loss = 12.05099 Val Loss = 13.08076
2025-06-05 16:31:02.536128 Epoch 30  	Train Loss = 12.04068 Val Loss = 13.03081
2025-06-05 16:40:48.116395 Epoch 31  	Train Loss = 11.95934 Val Loss = 13.01921
2025-06-05 16:50:33.133754 Epoch 32  	Train Loss = 11.95187 Val Loss = 13.02771
2025-06-05 17:00:18.060462 Epoch 33  	Train Loss = 11.94858 Val Loss = 13.00802
2025-06-05 17:10:03.629347 Epoch 34  	Train Loss = 11.94745 Val Loss = 13.02810
2025-06-05 17:19:49.067549 Epoch 35  	Train Loss = 11.94460 Val Loss = 13.01597
2025-06-05 17:29:34.132381 Epoch 36  	Train Loss = 11.94359 Val Loss = 13.00842
2025-06-05 17:39:19.781116 Epoch 37  	Train Loss = 11.94064 Val Loss = 13.03949
2025-06-05 17:49:05.371939 Epoch 38  	Train Loss = 11.93913 Val Loss = 13.05002
2025-06-05 17:58:51.190954 Epoch 39  	Train Loss = 11.93671 Val Loss = 13.00462
2025-06-05 18:08:36.851002 Epoch 40  	Train Loss = 11.93533 Val Loss = 13.05495
2025-06-05 18:18:22.633716 Epoch 41  	Train Loss = 11.92531 Val Loss = 13.02313
2025-06-05 18:28:08.467581 Epoch 42  	Train Loss = 11.92395 Val Loss = 13.01677
2025-06-05 18:37:54.214279 Epoch 43  	Train Loss = 11.92492 Val Loss = 13.02100
2025-06-05 18:47:39.861336 Epoch 44  	Train Loss = 11.92367 Val Loss = 13.01070
2025-06-05 18:57:25.817051 Epoch 45  	Train Loss = 11.92323 Val Loss = 13.01919
2025-06-05 19:07:11.650438 Epoch 46  	Train Loss = 11.92287 Val Loss = 13.02952
2025-06-05 19:16:57.333162 Epoch 47  	Train Loss = 11.92390 Val Loss = 13.02415
2025-06-05 19:26:42.952084 Epoch 48  	Train Loss = 11.92403 Val Loss = 13.02346
2025-06-05 19:36:28.302918 Epoch 49  	Train Loss = 11.92269 Val Loss = 13.02003
2025-06-05 19:46:12.914811 Epoch 50  	Train Loss = 11.92313 Val Loss = 13.01239
2025-06-05 19:55:57.886343 Epoch 51  	Train Loss = 11.92326 Val Loss = 13.01606
2025-06-05 20:04:09.222383 Epoch 52  	Train Loss = 11.92221 Val Loss = 13.02442
2025-06-05 20:10:39.576777 Epoch 53  	Train Loss = 11.92117 Val Loss = 13.01968
2025-06-05 20:17:08.820022 Epoch 54  	Train Loss = 11.92214 Val Loss = 13.02680
2025-06-05 20:23:38.267744 Epoch 55  	Train Loss = 11.92253 Val Loss = 13.02329
2025-06-05 20:30:07.653052 Epoch 56  	Train Loss = 11.92197 Val Loss = 13.02097
2025-06-05 20:36:36.892444 Epoch 57  	Train Loss = 11.92063 Val Loss = 13.01976
2025-06-05 20:43:06.079315 Epoch 58  	Train Loss = 11.92067 Val Loss = 13.02526
2025-06-05 20:49:34.889740 Epoch 59  	Train Loss = 11.91990 Val Loss = 13.02035
Early stopping at epoch: 59
Best at epoch 39:
Train Loss = 11.93671
Train RMSE = 20.34881, MAE = 12.26631, MAPE = 11.23607
Val Loss = 13.00462
Val RMSE = 21.96974, MAE = 13.51505, MAPE = 12.63021
Saved Model: ../saved_models/STHDformer-PEMS03-2025-06-05-11-38-10.pt
--------- Test ---------
All Steps RMSE = 27.25188, MAE = 15.40664, MAPE = 15.99094
Step 1 RMSE = 21.29578, MAE = 12.67496, MAPE = 13.72364
Step 2 RMSE = 23.02747, MAE = 13.39288, MAPE = 14.28976
Step 3 RMSE = 24.46150, MAE = 14.03509, MAPE = 14.87191
Step 4 RMSE = 25.63213, MAE = 14.57525, MAPE = 15.29898
Step 5 RMSE = 26.54482, MAE = 15.02669, MAPE = 15.65928
Step 6 RMSE = 27.34638, MAE = 15.42490, MAPE = 15.97931
Step 7 RMSE = 28.09342, MAE = 15.81896, MAPE = 16.26550
Step 8 RMSE = 28.70974, MAE = 16.15845, MAPE = 16.56259
Step 9 RMSE = 29.28830, MAE = 16.48748, MAPE = 16.85873
Step 10 RMSE = 29.80740, MAE = 16.79001, MAPE = 17.18434
Step 11 RMSE = 30.22738, MAE = 17.07330, MAPE = 17.46739
Step 12 RMSE = 30.77192, MAE = 17.42164, MAPE = 17.72987
Inference time: 39.49 s
